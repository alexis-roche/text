Les pistes de travail actuelles:

1. Segmentation via FCNN. La labélisation des pixels par blocs centrés de taille fixe est-elle suffisante? Faut-il prendre en compte l'échelle? Comment? Quel lien avec les approches type R-CNN ou SSD? 

2. Apprentissage classique. Faire un inventaire des métodes actuelles. Comment diagnostiquer la convergence? Ca coûte cher de faire des itérations... Pire, est-ce que les performances ne se dégradent pas au fil des itérations par effet de sur-apprentissage? Si oui, comment remédier à ce problème? Le "weight decay" est-il vraiment la solution?

3. Apprentissage "bayésien". Bénéfices des approches type EP ou SEP? Est-ce que ça converge plus vite? Est-ce que ça permet de tester la convergence? Diagnostiquer le sur-apprentissage? Dimensionner les données d'apprentissage?

4. Exploitation des hypothèses de régularités dans l'apprentissage (hypothèses sur la distribution jointe inputs/outputs). Echangeabilité, de Finetti et tout le bastringue? 

5. Réflexions générales sur l'optimisation. Au-delà de la descente de gradient: mises à jour non-linéaires (par ex: compositionnelles genre les "démons"). Impact en AI?

6. TensorFlow vs Keras. Which one is more convenient for covii projects?

---------------------------------------------

2. Apprentissage classique

Est-ce qu'on peut prendre le checkpoint qui donne les meilleures perfs
en validation croisée? Ca ressemble à une faute de débutant, qui
consisterait à confondre les données d'apprentissage et de validation,
mais ce n'est pas tout-à-fait la même chose car l'optimisation reste
confinée aux données d'apprentissage. 

5. Optimisation

Dans l'algorithme des "démons", qui représente décidemment une espèce
de fil rouge de ma carrière scientifique, il y a une idée nouvelle qui
pourrait s'avérer utile à un niveau plus général en
optimisation. C'est l'idée de mettre à jour la solution au problème
d'optimisation selon une règle non-linéaire contrairement au cadre
classique de la descente de gradient. Cette approche permet d'obtenir
des propriétés de "régularité" qui ne sont pas explicitement imposées
à l'espace de recherche et qui ne seraient pas non plus garanties par
la descente de gradient.

Dans le cas du recalage, je force chaque mise à jour de mon algo à
composer des petits déplacements fluides avec la solution
courante. C'est moi qui contrôle l'amplitude et la fluidité des
déplacements en question. Je le fais parce que j'obtiendrai ainsi un
difféomorphisme suffisamment fluide et que c'est ce qui
m'intéresse. Néanmoins, l'espace des difféomorphismes est sans doute
dense dans celui des déformations libres... en d'autres termes, il y a
des tas de difféomorphismes très moches qui ressemblent autant qu'on
veut à des déformations discontinues. Il serait naïf de travailler
dans l'espace entier des diffémorphismes.

Le hic, c'est que je ne sais pas définir précisément l'espace qui
m'intéresse. Comment traduire mathématiquement la notion de
difféomorphisme suffisamment fluide? L'idée des démons est d'imposer
"à la volée" une contrainte de régularité à la solution, en
restreignant non pas l'espace de recherche en tant que tel mais la
façon dont on se déplace dedans.

C'est l'idée que l'espace des solutions admissibles est complexe,
alors on se place dans un espace plus grand, bien défini
mathématiquement, et dans lequel on sait définir localement des
sous-espaces de solutions admissibles. Ainsi, de proche en proche, on
obtiendra une solution admissible.

Encore faut-il éviter d'itérer indéfiniment car on sent bien qu'il y a
un lièvre caché dans cette affaire. Chaque itération nous attire vers
une solution potentiellement inadmissible puisque il s'agit d'un
optimum local dans l'espace élargi. Pour faire le barbot, si notre
espace d'admission n'est pas compact au sens topologique du terme, hé
bien on pourrait finir par en sortir ou, tout du moins, se retrouver à
la limite. 

On le voit aisément dans la condition d'arrêt des démons: l'algorithme
continue à évoluer tant que les images ne sont pas d'intensité égales
partout! Et c'est un aspect qui m'a toujours perturbé dans cet
algorithme, qu'il faille, en pratique, limiter le nombre d'itérations
sous peine de partir en torche.

Une idée simple qui me vient pour remédier à ce problème est
d'appliquer un "taux d'apprentissage" aux mises à jour des démons,
exactement comme dans l'approximation stochastique... On peut
justifier ça par le fait que la SSD n'est qu'une version empirique de
la "vraie" SSD, celle qu'on observerait si les images n'étaient pas
bruitées. Et c'est ce bruit qui pourrait faire converger l'algorithme
des démons vers une solution absurde.

L'idée sous-jacente est de se ramener au cas où le critère atteint son
minimum absolu (ou au moins local) sur l'espace d'admission.


3. Apprentissage bayésien

La brique de base de cette approche, c'est l'ajustement gaussien d'une
distribution. On a une approximation courante q(x) de l'a posteriori
sur les poids du réseau, on considère un batch qui donne une
distribution b(x), et on veut améliorer q(x) en tenant compte de
b(x). Je suppose ici par convenance que b(x) est mis "à l'échelle",
ie, élevé à la puissance k=N/n où n est la taille de batch et N la
taille totale de l'ensemble d'apprentissage.

L'ajustement procède comme suit:

1) Déterminer un contexte c(x) en fonction de l'approximation courante

2) Déterminer un facteur f(x) en fonction du batch

3) Calculer une approximation gaussienne de la cible p(x)=c(x)f(x) et
mettre à jour q(x) selon

Par exemple, dans un apprentissage avec batch aléatoire, le contexte
peut être c(x)=q(x)^1-w, où w est un poids qui décroit vers zéro au
fil des itérations et qui représente l'importance relative du batch
par rapport à l'approximation courante. Le facteur est f(x)=b(x)^kw où
k est une constante permettant de mettre le facteur "à l'échelle", ie,
k=N/n où n est la taille de batch et N la taille totale de l'ensemble
d'apprentissage.

Quelles sont les méthodes dont nous disposons pour faire une
approximation gaussienne? Toutes celles que j'ai référencées dans mes
papiers sur le VS: outre VS, "Laplace", IS, MCMC... que l'on peut voir
comme diverses mises en oeuvre pratiques de la méthode "idéale", à
savoir la minimisation de la divergence KL, mais qui n'est pas
calculable.

Pour "Laplace" (en fait, juste un vieux DL...), il faut choisir un
point pour le DL, le choix le plus naturel étant le centre du
contexte. La vraie approximation de Laplace correspond à l'argmax de
c(x)f(x), ce qui est évidemment plus coûteux en calculs sans être plus
précis dans les faits. Notons que dans le premier cas, l'approximation
gaussienne ne dépend que du mode mais pas de la variance du contexte.

Pour VS (et IS) se pose la question du noyau d'échantillonnage
(aléatoire ou pas). Le contexte c(x) est le choix le plus naturel, ce
qui nous ramène à la présentation de mon article de 2016.

Plus le contexte est resseré, comme on s'y attend au fil des
itérations, plus Laplace se rapproche de min-KL. Par contre, en début
d'algorithme, lorsque le contexte est encore large, Laplace pourrait
fournir de mauvaises approximations des facteurs et faire partir
l'algorithme dans une mauvaise direction... 

On peut aisément imaginer des cas pathologiques où l'algorithme
d'apprentissage partirait en torche dès la première itération à cause
d'une seule approximation de Laplace effectuée "au mauvais
endroit". Mais tout cela reste un peu théorique, il faut bien le
reconnaitre... Il reste à prouver en pratique que VS permet un
apprentissage plus robuste ou plus rapide, et ce dans le contexte du
deep learning. Mais l'idée de tenir compte de l'incertitude dans
l'apprentissage fait sens.

Pour accélérer VS, on peut considérer une version one-step qui
consisterait à faire une seule itération de Newton. Si on cherche q(x)
sous la forme c(x)g(x), où g=exp[a phi], on peut initialiser a via g =
q/c (ce qui suppose que tous les facteurs "se ressemblent"). Dans
l'algo stochastique, ca revient à initialiser l'approximation du
facteur par g(x)=q(x)^w.

Et là, une question me vient: n'y a-t-il pas moyen d'utiliser une
relaxation à la con genre EM pour approcher VS?

L(a) = -a <phi> + int c exp[a phi]

Il faudrait majorer L(a)-L(a0) <= Q(a, a0)

1 + a phi + (a phi)^2 / 2



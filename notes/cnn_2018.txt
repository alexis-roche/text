Issues: 

* How should we choose the patch size? 

* Odd/even patch size? Should the center match an actual pixel?

* How do we make sure that examples are comprehensive? For instance,
  in the first running version, I observe that strong reflections on
  foil are often classified as food, presumably this is a consequence
  of not having many examples of that in the training dataset. 

* Should we have ambiguous examples?

* Should we balance classes in trainging (class_weight='auto' vs None)

* How to run the model in C/C++?

* Spatial regularization (Markov)

-----------------------------------------------------------------------

Efficiency of the patch-based approach. It seems that the community
has replaced patch-based CNNs with "fully convolutional nets" (FCN).

One thing that is worthwile noticing about CNNs is that they only
depend on the input image size IF they have fully connected layers.
However, the fully convolutional layers can be replaced with
convolutional layers of appropriate kernel sizes (depending on the
original input image size). 

This  seems to  be the  idea behind  FCN. There  is a  trivial way  to
transform a CNN into a FCN, the  advantage being that the FCN can take
up any  image size.  There seems  to be non-trivial  stuff in  the FCN
paper, but  let's first draw  some basic conclusions from  the trivial
part.

Et, donc, plutôt que d'appliquer un CNN indépendamment à toutes les
patchs d'une image indépendamment, il est beaucoup plus efficace
d'appliquer le FCN équivalent à l'image entière car les calculs sont
ainsi partagés à travers les patchs. C'est une remarque d'ordre
purement calculatoire, mais importante!

Si j'arrive à reverse-engineerer l'application d'un modèle par Keras,
ce qui ne devrait pas être bien difficile, je peux coder tout ça en C
indépendamment de toute librairie... et ça devrait être méga-efficace!

-----------------------------------------------------------------------
Feb 9.

Check padding 'same' or 'valid' in Keras. By default, Keras applies
'valid', which shrinks the input size. 'same' seems to be padding with
zeros. If 'valid', don't need to apply coordinate checks when applying
kernel mask, so may be substantially faster.

What is the concept behind Tensorflow sessions? Why converting a TF
tensor to a numpy array via the eval method requires to start a
fucking session, and WHY THE FUCK is the result fucking random?????
For now, use keras TF backend as a workaround.

OK, so it seems I have a running C implementation of multiple
convolution, just need to speed it up in the default case of 'valid'
padding. The ReLU/max pooling is still to be tested, but it's
difficult to screw up.

The main left difficulty will be to understand how the dense layers
are implemented and how they can be "convolutionalized"... Basically,
the convolutional layer ends up with a tensor of shape 4x4x64 in the
case of our CNN working on 50x50 image patches. So we have this
elongated 3D block of features fully connected to 64 units in this
instance, which amounts to 64 filters with kernel sizes (4, 4).

-----------------------------------------------------------------------
Feb 19.

Retour de vacances à Morillon, et comme souvent après une prise de
recul, le champ de vision s'élargit.

Ce qu'il faut implémenter, c'est une "convolution dilatée".

Il faut entrainer le CNN 'patch-based' avec padding 'valid' pour
éviter les effets de bord quand on implémente le fully convolutional
CNN équivalent. On a alors un effet de shift des coordonnées de bord
de patchs (+1 à chaque couche convolutionnelle) mais on s'en fout
peut-être un peu si on s'intéresse aux centres.

ATTENTION: le fully convolutional CNN doit, lui, maintenir les
dimensions spatiales constantes, donc le padding doit être de type
'same'... peu importe car les pixels concernés ne sont pas "valides"
au sens où ils ne sont pas au centre d'un patch pouvant être
classifié.

-----------------------------------------------------------------------
Feb 23.

Les premiers tests indiquent que le FCNN fait le job attendu avec un
temps de calcul environ 10 fois inférieur au code python basique (ne
partageant pas les calculs entre les patchs).

Il semble y avoir des effets de bord... Cela vient vraisemblablement
du fait que le zero-padding n'est pas approprié (s'il y a du signal au
bord de l'image, il va "baver" à l'extérieur par convolution mais cet
effet sera ignoré par les convolutions ultérieures). Une solution
simple serait d'étirer l'image de départ en la complétant
"physiquement" par des bords nuls suffisamment larges. Non, ce n'est
pas la même chose que faire du zero-padding à la volée (sauf à la
première convolution).

Ce sont clairement les convolutions qui prennent le plus de temps (par
rapport au max pooling). Il faut avoir à l'esprit que le coût
calculatoire d'une convolution dépend de:

- la taille du noyau: (sx, sy)
- le nombre de canaux en entrée: n0
- le nombre de canaux en sortie (nombre de filtres): n1

Grosso merdo, le temps de calcul est proportionnel à: sx*sy*n0*n1.

Ici, 

- pour la première convolution, on a: sx=sy=3, n0=3, n1=32.

- pour la convolution correspondant à la première couche "dense", on
  a: sx=sy=4, n0=64, n1=64.

Le rapport de temps de calcul sera donc de l'ordre de 76! Sur mon PC,
avec une compilation en O3, la première convolution prend environ 3/4
de seconde pour une image de taille 640x480, et la dernière prend
logiquement de l'ordre d'une minute.


-----------------------------------------------------------------------
DOUBLE vs FLOAT 

The code can work in float or double (depending on a typedef in
run_utils.h).

To probe the difference in encoding, I ran a convolution with kernel
size (4, 4, 64, 64) on a 640x480 image. The computation time was as
follows:

Double: 62.765 s
Float: 46.036 s

So, by using C-type float rather than double, we roughly reduce
computation time by 27%, in addition to reducing memory load by a
factor 2.

-----------------------------------------------------------------------
SHIFT

I observed that the FCN output was shifted by several pixels wrt the
result using the brute-force implementation of patched-based CNN
segmentation... why is it so?

This is normal. It can be seen that, for a CNN of the type considered
and with pool size 2, the top left corner of any patch is shifted by:

sum_{i=0}^{#conv_filters} 2^i = 2^{#conv_filters + 1} - 1
 
For instance, with 3 convolutional layers, the shift is of 2^4 - 1 =
15 pixels in both directions.

This means that the output corresponding to a patch top left corner
with coordinates x, y is found at (x+15, y+15) in the image produced
by the "equivalent" FCNN.

If now (x, y) are the patch center coordinates, the corresponding
output coordinates are (x-h+15, y-h+15) where h is half the patch
size. If we trained the CNN on patches of size 50x50, h=25, therefore
the shift is 15-25=-10, negative. This means that the pixel with true
coordinates (0, 0) is not found in the FCNN output as it would
correspond to coordinates (-10, -10).

To correct for that, we just need to translate the input image by 10
towards the bottom right before applying the FCNN.

If we do that using the same image size (hence truncating the input
image to the bottom right), we get the correct output up to some
clutter in the bottom right...  due to the truncation but also the
fact that the input should be padded with zeros. We can further
correct for this using a sufficiently large image size but this seems
too little a problem to deserve headaches.


-----------------------------------------------------------------------
DIMENSION FLOW

Assume kernel size = 3, pool size = 2

Convolutional layer: dim -> dim - 2
Pooling: dim -> dim / 2
Shift is sum_{i=0,...,#pool_steps} 2^i = 2^{#pool_steps + 1} - 1 

How does this generalize?

Kernel size. The image FOV is reduced to ensure that the kernel always
fully overlaps the image (with the 'valid' padding option).

* If the kernel size is odd, the dim is reduced by 2*((size-1)/2) =
size-1.

* If the kernel size is even, the kernel mask is asymetric, taking
  hl=(size-1)/2 pixels and hr=size/2 pixels on the right, hence we
  loose hl+hr = (size-1+size)/2 = (2*size-1)/2 = size-1, again!

--> the dimension reduction due to a convolution is always the kernel
    size minus one.

Pooling. We will barely deal with pooling sizes other than 2, but
let's do the excercise.

First, we have the same issue as for the kernel: the mask needs to
overlap the image completely, so the max filter (or any filter
associated with pooling) is applied to an image with dimensions
reduced by (pool_size-1).

Next, a subsampling operation is performed, which approximately
divides the dim by the pool size. We need to solve:

f * (d-1) < D => d < (D/f) + 1
f * d >= D => (D/f) <= d

=> d = ceil(D/f)

To sum up:

dim -> dim - pool_size + 1 -> ceil((dim - pool_size + 1) / pool_size)
= ceil((dim + 1) / pool_size) - 1

So the effect of convolution + pooling is to do:

dim -> ceil((dim + 1 - kernel_size + 1) / pool_size) - 1

= ceil((dim + 2 - kernel_size) / pool_size) - 1

Shift.

A la 1ere convolution, le coin se déplace de (kernel_size-1)/2
Au 1er pooling, il se déplace de (pool_size-1)/2
Notons s = (kernel_size-1)/2 + (kernel_size-1)/2

Puis on sous-échantillonne d'un facteur f.

A la 2e convolution, le facteur de dilatation vaut f, c-à-d qu'on vit
désormais dans un monde où la distance entre deux pixels voisins est
f. Donc, tout se passe comme avant sauf qu'on multiplie tout par
f... c'est simple.

Le shift vaudra donc: s + s * f

etc...

En notant n le nombre de couches convolutionnelles (suivies de
pooling), le shift total est:

s + sf + sf^2 + ...
= s (f^(n+1) - 1) / (f-1)

Dans notre CNN de base, nous avons s=1, f=2 et n=3.

-----------------------------------------------------------------------
March 2.

TODO:

-> figure out the resolution to use to achieve best performance/cost
   tradeoff. For instance, if we work at half the resolution, we can
   use patches of size 25x25 to train the CNN, and we will essentially
   divide the computation time by 4.

REFLEXIONS...

Je me pose toujours la question de savoir s'il faut prendre des patchs
de taille paire ou impaire.

Si c'est paire, le centre du patch ne coincide pas avec un
pixel. Est-ce que c'est emmerdant? Un peu, si le but est de classifier
chaque pixel en fonction du patch dont il est le centre. Mais c'est un
peu une question d'ordre philosophique car on pourrait dire qu'on
classifie chaque pixel en fonction du patch dont il est le pixel le
plus proche du centre - par valeurs inférieures ou supérieures, comme
on veut... Mais, justement, cet arbitraire n'est pas très souhaitable.

Cette question est à rapprocher à celle de l'entrainement utilisant ou
non des patchs ambigus. 

-----------------------------------------------------------------------
April 9.

Un autre effet de bords auquel on est confronté du fait de la
convolution 'valide' est que l'image de probabilité finale est
entourée d'une bande de pixels de valeur constante sans
signification. La bande est plus large à droite et en bas parce que la
troncature affecte préférentiellement les valeurs supérieures dans
notre implémentation (de façon cohérente avec Keras).

Par exemple, un pooling de 2 ne produit pas de troncature à gauche,
mais fait perdre une bande d'un pixel à droite et en bas.

Ceci s'ajoute au phénomène de déplacement discuté plus haut, lui aussi
dû à l'interpolation 'valide'.

Quand on applique un noyau de taille s avec une dilation de 1, on perd:
* à gauche: (s - 1) // 2
* à droite: s // 2

Avec un facteur de dilatation d, on perd:
* à gauche: d * ((s - 1) // 2)
* à droite: d * (s // 2)

Dans notre réseau favori, on perd:
* à gauche: 1 (conv), 0 (max pool), 2, 0, 4, 0, 8 (dense layer)
* à droite: 1, 1, 2, 2, 4, 4, 16

Ces pertes s'ajoutent à chaque itération, de sorte qu'on se fait
manger 15 pixels à gauche et 30 à droite. Ce sont des pixels pour
lesquels le FCNN n'est pas cohérent avec le CNN associé à cause d'un
champ de vue restreint. 

La perte liée aux convolutions (excepté la 1ère couche dense) est de
la forme:

p_conv = q + 2*q + 4*q + ... = q (1 + 2 + 4 + ... ) = q (2**n - 1)

avec q = (kernel_size-1)//2 à gauche et q=kernel_size//2 à droite, où
n est le nombre de couches convolutionnelles.

De même, pour les relouds Max/Poules, on perd:

p_remax = q (2**n - 1)

avec q = (pool_size-1)//2 à gauche et q=pool_size//2 à droite.

La formule générale est, en notant ks la kernel size, ps la pool size
et fs la kernel size de la convolution finale équivalente à la 1ere
couche dense:

* A gauche: p_left = [(ks-1)//2 + (ps-1)//2] (2**n - 1) + 2**n (fs-1)//2
* A droite: p_right = [ks//2 + ps//2] (2**n - 1) + 2**n fs//2

------------------------------------------

Apr 27.

Looks like, the more training converged, the more binary the CNN
output... Meaning that the network is more confident in its
predictions as training improves. Makes sense, doesn't it?

But could also suggest some kind of overfitting is happening. How can
we evaluate overfitting?

The answer, my friend, is running the EP. The answer is running the
EP.

Overfitting has to be reflected by high posterior variances on the
parameters. Low variances mean near certainty on parameter values. I
realize that not everyone might agree with that, but it's obvious to
me... Low variances mean the data at hand is enough for training, so
it can only mean underfitting. 

But what does EP provide us with? Parameter estimates and
variances. From there, we can evaluate the uncertainty in network
responses using some Monte Carlo simulation for a bunch of examples,
and pick those examples for which the uncertainty is largest. Then,
generate more of this type of examples and see if we improve things.

-------------------------------

July 9.

Reading notes from pyimagesearch

https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/
https://www.pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/

Depthwise convolution? Purely spatial, channel-specific convolution,
resulting in one feature map per channel. Instead of the standard conv
+ max pool building block, we may use depthwise conv + max pool
followed by "1x1 convolution" + max pool. Advantage: for the same
number of scalar feature maps (same number of "filters"), we save a
lot of parameters. This trick is used for resource constrained
devices, hence the name "MobileNet". Drawback: sacrifice accuracy...

I need to understand the different types of object detection
approaches using deep learning. Object detection differs from image
classification in that we want to recognize any object present in an
image and locate it somehow (e.g., using a bounding box). R-CNN, Fast
R-CNN, Faster R-CNN, SSD...

Nice summary on object detection using CNN at:
https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9

"Most successful approaches to object detection are currently
extensions of image classification models" (Joyce Xu)

Old approach: R-CNN (Region-based CNN). Scan input image for possible
objects using a pre-processing method called "selective search", next
run a CNN on top of each region proposal (resizing image patches to a
standard size). Easy. As a cool add-on, use a linear regressor to
tighten the bounding box from the CNN features. R-CNN very intuitive
but very slow.

Selective search essentially relies on segmentation (looks like a good
old k-means clustering) to find "interesting" blobs... typically
resulting in about 2000 region proposals for an image. But at least we
won't run the CNN for each and every window location and scale.

Fast R-CNN. Girschick, 2015. Keep the selective search but apply a CNN
to the ENTIRE image to produce a feature map, from which a
"fixed-length feature vector" is extracted for each proposal region
(before the fully connected layer as far as I understand). So there is
an "RoI pooling layer" that acts as standard max pooling but can work
with any input shape (always producing the same output shape). To turn
a pre-trained CNN into a R-CNN, we replace the last max pooling layer
with a "compatible" RoI pooling layer.

In summary, Fast R-CNN takes as input a set of candidate regions in
addition to the input image, and outputs a probability mass over (K+1)
classes (including the "no object" class) for each candidate RoI (plus
additional bbox regressor parameters, but let's forget about that for
now). The main advantage over R-CNN is that computations are shared
across overlapping candidate RoIs.

Fast R-CNN still uses a selective search to find candidate RoIs, which
is slow, hence the motivation for a Faster R-CNN. Ren et al, 2015. In
short, Faster R-CNN = RPN + Fast R-CNN. RPN is some kind of CNN that
substitutes the selective search. So, what is a RPN?

RPN = Region proposal network. We have an initial CNN of which we
consider the last layer. We move a 3x3 sliding window across the
output of the last convolution layer and maps it to a lower dimension
vector (e.g., 256-d), which is used to predict whether various
pre-defined "anchor boxes" centered at the sliding window location
contain an object or not (it's really a binary decision: object vs
non-object as the goal is only to output candidate regions). In
practice, they use 9 anchor boxes corresponding to three different
aspect ratios (1:1, 2:1, 1:2) and three different scales.

At this point, we note that all these methods (R-CNN, Fast R-CNN,
Faster R-CNN) work in a two-step fashion: first select candidate
regions (using either selective search or RPN), second classify
them. The Single Shot Detector (SSD) method by Liu et al, 2016,
collapse both tasks into one, hence the term "single shot".

"The SSD approach is based on a feed-forward convolutional network
that produces a fixed-size collection of bounding boxes and scores for
the presence of object class instances in those boxes, followed by a
non-maximum suppression step to produce the final detections."

SSD expands on a "base network" suitable for image classification
"(truncated before any classification layers)". Convolutional feature
layers are added to the end of the truncated base network, which
decrease in size, producing multi-scale feature maps for detection.
Each such feature map is convolved by small 3x3 kernels to evaluate 3
default bounding boxes, producing a score for each category and 4
shape offset parameters. This bit is similar to Faster R-CNN.

At each location, for each box out of k, we compute c class scores and
the 4 offsets relative to the original default box shape, yielding
(c+4)k outputs per location in a m×n feature map.

At training, all the default boxes that have a significant overlap
with the ground truth (as measured by the Jacard index, or
"Intersection over Union") are considered as
"positives". Specifically, each ground truth box is matched with the
default box with largest IoU, and with all other default boxes with an
IoU > 0.5. The training loss is a combination of a "confidence" loss
(on the predicted class) and localization loss. 

------

Deep metric learning.

Again from this nice block, Adrian Rosebrock:

https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/

Common deep learning methods are classification methods. They take
some type of data as input and output a label. Deep metric learning is
concerned with methods that output a real-valued feature vector
instead.

Adrian mentions a deep metric learning approach for face recognition,
one that takes a face image patch as input and outputs a 128-d
vector. The idea behing the training of such a system is to use
triplets as data items - in this case, a triplet consists of two image
patches of the same guy and another patch from someone else. Each
triplet should result in tweaking the network weights so that the
vector outputs corresponding to the same guy are closer to each other
and farther from the output corresponding to the other guy.

Deep metric learning extracts a meaningful representation of a face
image, in this case 128-d, which can be fed into a standard
classification technique, e.g., a k-NN classifier for actual face
recognition.

A general remark is that a standard feed-forward network for
classification also extracts real-valued feature vectors before
performing a classification task in the last layer. Can this feature
extraction be considered as a case of "deep metric learning"? Or, in
other words, are these features useful to other tasks than the
classification at hand?  This is kind of the question underlying
transfer learning.

One advantage of metric learning is that it seems to be a more direct
approach: try to represent a complex object by a lower-dimensional
"signature" in such a way that classification is made easy.

On the other hand, transfer learning relies on much smaller training
datasets, making it much more affordable in practice.

Another approach is to design and train a network to decide whether
two input images represent the same person or not. This is not metric
learning per se since the network output is a binary variable (as
opposed to a dense representation) but the network similarly learns
a representation that needs to be person-specific. 


-----------

One thing that comes to my mind is that SSD looks like a brute-force
approach to solve the object recognition problem.

What about a multi-scale version of FCNN? Whatever it means... I need
to think into this.

------------

July 24/25.

What is "softmax cross entropy with logits" (tensorflow expression)?

Let us recall that the cross-entropy between two distributions p and q
is defined by:

H(p, q) = - sum_z p(z) log q(z) = - E_p[log q(Z)]

Now consider the log-likelihood:

L = sum_i log q(xi, yi) = sum_i [log q(yi|xi) + log q(xi)]

where xi is i-th example input and yi the corresponding output and
q(x, y) is the joint distribution implemented by the network. In
practice, the marginal q(x) is not modeled so it can be anything we
want but we simply don't care about this term.

L is fundamentally the same thing as the cross-entropy:

L = -n H(p, q)

where p(z) represents the empirical distribution of z=(x,y) in the
training dataset and q is the model distribution. 

If we drop the marginal term q(x), we see that L is just a negated and
normalized version of the CONDITIONAL cross-entropy, say:

L = -n H_q(Y|X)

where the conditional cross-entropy reads:

H_q(Y|X) = - sum_x,y p(x,y) log q(y|x)

In the case p=q, the conditional cross-entropy is simply the
well-known conditional entropy.

In conclusion, when using anything called cross-entropy as a training
score, one simply aims to maximize likelihood.

Note that the cross-entropy decomposes as the sum of the entropy and
the Kullback-Leibler divergence:

H(p, q) = H(p) + D(p||q)

This is also true in the conditional case.

In practice, `p` is only an empirical estimate of the `true` joint
input x output distribution, but let us assume that the training
dataset is so big that the approximation is accurate.

H(p) would vanish (in any task where the output takes on discrete
values, such as classification) if there exists a way to complete the
task with absolute certainty, i.e. if the label is a deterministic
function of the input data. For instance, if we tackle a vision task
that can be accomplished faultlessly by the human eye, we can assume
that H(p)=0. In this case, the MINIMUM cross-entropy simply reflects
the lack of modeling power of the network, which is a purely EXTRINSIC
source of uncertainty.

More generally, if the task cannot be made 100% reliable by any
device, then cross-entropy reflects a combination of intrinsic and
extrinsic sources of uncertainty. The entropy can be thought of as
some sort of baseline uncertainty measure (you can't get lower than
that unless you measure the label).

From a pragmatic viewpoint, it does not matter where the uncertainty
comes from. The minimum cross-entropy tells us "how unreliable" our
predictive model is - and, don't forget, it's only an empirical
estimate.

The entropy H(p) is a lower bound on the cross-entropy achieved by the
model. It might not be easy to estimate in practice with a dense input
variable, even using big data. However, as already pointed out, we
might assume that H(p) is zero or close to it.

What about the cross-entropy of a random classifier? A random
classifier is one for which q(y|x)=q(y) is independent from x. It is
then obvious that the conditional cross-entropy boils down to the
marginal cross-entropy:

H(p, q) = - sum_y p(y) log q(y) >= H(p)

Hence, the "best" random classifier is the one that matches the
marginal distribution of classes. If that distribution is uniform
(which should be roughly the case if examples are balanced using
appropriate weights), then the entropy is log(K), where K is the
number of classes. In classification, this limit is about 0.69 (nats).

Seems that TensorFlow and Keras use natural or base-2 log for entropy
computation, meaning that they return entropy values in nats as
opposed to bits.

What does it mean? It means that I expect my binary classifier to
achieve a cross-entropy somehwere between 0 and 0.69 nats at
training. Larger than 0.69 is possible but totally useless as it would
mean that the classifier is worse than a trivial classifier, namely
the "best" random classifier.

Obviously, the closer to zero, the better. How close to zero we need
to get to be confident on our classifier is a tricky question, but
this simple reasoning on cross-entropy gives a rule of thumb to make
sense of training logs.

We shall keep in mind that cross-entropy is estimated on
mini-batches. However, if values are fairly consistent across
mini-batches, they should give a reasonable guess of the actual
cross-entropy.

Now, a more tricky question: can we have a perfect classification rate
with a large cross-entropy? In theory, this is possible... yes... But
if the model is trained to maximize the model likelihood, one should
not be concerned about the classification rate. The fundamental goal
is to approximate the `true' conditional distribution, that is, try
and capture the intrinsic uncertainty on predictions as much as
possible.

---------------------------------

July 31.

Assume we have an algorithm that, given two input images representing
a single human being, outputs the probability that these images
represent the same person.

Assume now we run this algorithm for a bunch of image pairs (im, im1),
(im, im2), ... (im, imN), where im is a single test image and im1, im2,
... is a set of known person images.

We get a sequence of probability values p1, p2, ..., pN.

How should we use these values so as to decide whether the person
represented on `im` has a match in the pre-defined image set?

One way I can think about is... the generalized composite likelihood
idea! That is: we consider each algorithm run as an expert's
opinion. The events are atomic and possibly disjoint: `P==P_j`, where
P is the person on im and P_j is the j-th in the register. There might
be multiple images of the same person in the register, so let's
consider the set of distinct persons {1, ..., M} and the person index
vector `y` corresponding to the image index set {1, ..., N}, which
defines a mapping. Given a person index j, y^{-1}(j) denotes all
images representing that person, and let ni denote # y^{-1}(j).

The externally Bayesian opinion pooling rule yields (assuming equal
weights for all experts in the same pool):

p(P==Pj) \propto \prod_{i\in y^{-1}(j)} [pi/(1-pi)]^[1/ni]

In this story, the random variable is P. Each expert confronts the
hypothesis that P is a certain person Pj with the null hypothesis that
P is an unknown person. Given that each expert only knows one person,
his job boils down to testing whether P=Pj or not.

--------------------------

Aug 14

The difference between cross-entropy and accuracy...

While training the ReID network, I notice that at some point the
accuracy slightly decreases over iterations. How can this be
explained?

It could be because the training algorithm is stochastic as opposed to
greedy, so does not guarantee better performance on each iteration,
only in the long run.

It could also be because the test dataset is also random in our ReID
setup (because we don't generate all possible pairs for obvious
computation time reasons).

Finally, it could also be because the training objective, cross-entropy, is
different from accuracy, hence we could be facing a situation where
both cross-entropy and accuracy decrease. Is that really possible and
what does it mean in practical terms?

To summarize, the decrease in accuracy can occur because of either 1)
stochastic behavior of the training, or 2) stochastic behavior of the
testing, or 3) discrepancy between cross-entropy and accuracy.

Let us focus on 3.

Minimizing cross-entropy loosely means that the model is trained to
output large probabilities on the correct classes. Maximizing accuracy
means that the model is trained to output a probabilities above .5 on
the correct classes (in the binary case).

Therefore, with accuracy as the objective, we don't really care about
the class probabilities. Whether the model is "confident" or not about
its prediction is not relevant as long as the prediction is correct.

Consider a bad classification case for which, at some point in traing,
the network assigns a probability p close to zero to the true
class. This single example will considerably inflate cross-entropy as
it contributes -log(p), which is a very large number. The same example
has a strikingly more limited impact on accuracy as it just penalizes
it by the same amount as any other bad classification case.

Here we get the fundamental insight into the difference between
accuracy and cross-entropy: cross-entropy forces the network to be
"humble" in the sense that it cannot be very confident whent it's
wrong. This may come at the price of lower accuracy because training
won't be so focused on "bordeline" cases for which the network gets
the prediction wrong but with low confidence. The good thing is that
the probability output by the network reflects the confidence in
classification, so borderline cases could be detected by even
probabilities, while no such property is guaranteed when maximizing
accuracy.

In brief, the key difference between accuracy and cross-entropy is
that accuracy penalizes all classification errors in the same way
while cross-entropy penalizes classification errors depending on the
confidence level (errors made with high confidence are considered much
worse than errors made with low confidence).

---------------

Sep 6

Quelques réflexions sur l'apprentissage...

L'évolution du taux d'apprentissage est critique pour la convergence
de l'algorithme d'optimisation. Une décroissance trop rapide conduira
au "syndrôme de l'escargot", c-à-d une convergence très lente. Une
décroissance trop lente entrainera l'effet inverse, à savoir des
fluctuations intempestives autour de l'optimum: le "syndrôme de la
grenouille". Dans les deux cas, la convergence sera lente.

Le taux d'apprentissage joue un rôle analogue à la température dans le
recuit simulé. Son évolution est déterminée par quelques paramètres,
typiquement un taux d'apprentissage initial et un paramètre de
"decay", dont le choix semble relever de l'empirisime le plus
total...

L'optimum dont on parle est celui du coût moyenné sur la totalité de
l'ensemble d'apprentissage, avec éventuellement des poids différents
selon les classes. Son calcul à paramètres du réseau contants revient
à moyenner les coûts asssociés à des mini-batches aléatoires sur un
grand nombre de mini-batches (c'est l'idée du bootstrap).

La possibilité de pondérer les classes m'inspire une réflexion: on
modifie ainsi la distribution empirique marginale des classes de
manière à la remplacer par un "a priori" uniforme! C'est bien
évidemment ce qu'il faut faire si on veut que le classifieur soit
aussi performant pour toutes les classes. Dans le cas contraire, les
classes rares auraient peu de poids dans l'apprentissage et pourraient
donner lieu à des inférences très mauvaises, bien que se produisant
rarement par essence.

N'est-ce pas là l'essence-même de la conception bayésienne de la
probabilité par opposition à sa conception fréquentiste? Substituer au
monde réel un monde des possibles défini "a priori". On repense à la
phrase de de Finetti: "la probabilité n'existe pas".

Pourquoi, tant qu'on y est, ne pas remettre en cause la distribution
générative sur le même principe? C'est une question fondamentale. Cela
irait à l'encontre du principe de vraisemblance.

Cette parenthèse refermée, revenons à nos moutons. Il semble exister
un programme de recuit optimal en terme de vitesse de
convergence. Mais comment comparer deux programmes distincts?

Une idée naturelle est de regarder l'évolution du coût (c-à-d
l'entropie croisée) au fil des itérations. Celle-ci étant, en
pratique, calculée sur des mini-batches, on n'a qu'une estimée bruitée
de l'entropie globale. Si les paramètres à optimiser n'évoluaient pas,
l'entropie oscillerait autour d'une valeur fixe. En calculant une
moyenne fenêtrée, on pourrait réduire ces oscillations.

Mais les fluctuations des paramètres à optimiser contribuent elles
aussi à faire varier l'entropie. 


---------------------------------------------

Sep 12.


L'idée selon laquelle le "deep learning" serait la solution ultime à
tous les problèmes d'IA est grossièrement fausse. Quand on fait du
deep learning en pratique, on se rend compte qu'on a rarement un
modèle permettant d'accomplir directement la tâche souhaitée. C'est
souvent faute de données d'entrainement, mais pas nécessairement par
manque de temps ou de moyens: il y a des problèmes pour lesquelles
l'acquisition massives de données d'entrainement n'est tout simplement
pas possible (pensons à des problèmes de reconnaissance de personnes).

Dans les faits, on se retrouve la plupart du temps à faire de
l'apprentissage par transfert, c-à-d utiliser un modèle entrainé à une
tâche "générale" sur des données collectées et mises en ligne par tel
ou tel institut sympa et recycler ledit modèle pour la tâche qui nous
intéresse. Ce recyclage peut prendre plusieurs formes mais toutes
reviennent peu ou prou à récupérer la représentation apprise par le
réseau "généraliste" pour faire du machine learning classique.

Oui, du machine learning classique, car tout l'objet du deep learning
est l'apprentissage de représentation. Quand on n'apprend pas la
représentation, on fait du machine learning classique - par
définition. Et quand les données sont comptées, mieux travailler avec
des modèles génératifs. "Apprentissage par transfert" est donc une
expression codée pour dire qu'on revient à nos bons vieux modèles
génératifs du 20ème siècle.

Le panorama de l'IA se divise ainsi en deux méthodologies bien
distinctes et complémentaires:

- l'apprentissage de représentation: deep learning, modèles
  discriminants (quoique on laisse pour l'instant de côté
  l'apprentissage non-supervisé);

- l'inférence statistique: shallow learning, modèles génératifs de
  préférence.

Il existe une raison fondamentale pour laquelle l'apprentissage de
représentation ne nécessite pas de modèle génératif complet. Un tel
apprentissage est assimilable à la recherche d'une statistique
exhaustive pour l'inférence d'une certaine variable d'intérêt. Or, on
l'a déjà vu à maintes reprises, il suffit pour cela de modéliser la
distribution conditionnelle de la variable cachée à représentation
connue. Plus généralement, on peut travailler avec un modèle de
distribution jointe de la variable cachée et de la représentation, au
prix d'une complexité accrue (type théorie des jeux) et sans bénéfice
notable si on est en régime "big data". Mais dans tous les cas, il est
inutile d'exhiber un modèle génératif des données complètes.

La résolution de nombreux problèmes d'IA procède en deux étapes
différentes en nature:

1. Identifier le type de problème: choisir une représentation
pertinente par analogie avec une tâche similaire déjà apprise;

2. Comprendre le problème: modéliser la distribution de la
représentation sous les différentes hypothèses à envisager.

C'est une erreur de penser qu'il faut nécessairement réaliser ces deux
étapes par deep learning. Ce n'est vrai que pour un petit nombre de
problèmes "généralistes" qui sont en quelque sorte des savoir-faires
élémentaires sur lesquels on peut capitaliser pour développer des
aptitudes plus spécifiques.

Ce serait aussi une erreur de penser que le deep learning, ou une
autre forme d'apprentissage, est nécessaire pour réaliser la première
étape. L'exemple du recalage montre qu'on peut travailler avec des
représentations de base "par défaut" pour lesquelles existent des
modèles génératifs identifiables découlant des régularités de la
représentation. Par exemple, dans le cas du recalage, la
représentation de base est l'ensemble des couples de pixels (l'ordre
des couples étant sans importance), dont la distribution est
assimilable à iid en vertu du théorème d''echangeabilité de de
Finetti.

Ainsi, restreindre volontrairement l'information permet dans certains
cas d'éviter la phase d'apprentissage de représentation.

Une petite question au passage: quelles contraintes faut-il appliquer
à un CNN pour que les attributs qu'il apprend soient échangeables?  Si
tel est le cas, leur distribution est facile à modéliser...

Mais, de façon plus générale, est-il toujours faisable de modéliser la
distribution d'une représentation apprise? Quid de l'estimation des
paramètres? N'a-t-on pas intérêt parfois à sacrifier l'information
(ou, plus exactement, mettre les paramètres inconnus dans l'équation
informationnelle)? La vraisemblance composite peut apporter des
réponses à ce niveau.

En résumé: apprendre à faire une inférence, c'est d'abord choisir une
représentation (laquelle passe éventuellement par un apprentissage
profond), ensuite comprendre comment cette représentation varie en
fonction de la variable d'intérêt, enfin évaluer la qualité de
l'inférence qui en découle et recommencer le procès au besoin.

L'apprentissage profond fournit une représentation hautement
informative dans la mesure où le réseau choisi est suffisamment
flexible et où la tâche considérée est suffisamment proche de celle
qui nous intéresse.

Question comme ça: est-ce qu'un CNN peut me pondre un descripteur des
yeux indépendant de la pose du sujet? Idéalement, le 56e neurone de
l'avant-dernière couche dense code la forme des yeux. Comprendre les
CNN, c'est comprendre s'ils sont capables de faire ça.

Mis à part la position du descripteur en question qui est évidemment
arbitraire, la question revient à celle de l'invariance par
translation et rotation. Si j'en crois Goodfellow, chapitre 9, les CNN
sont plus ou moins invariants par translation, du fait de
l'équivariance par translation de la convolution et des opérations de
pooling. Pour ce qui est des rotations, une invariance approximative
peut être obtenue en utilisant plusieurs versions tournées de chaque
filtre... Si on ne le fait pas explicitement, l'apprentissage devrait
le faire automatiquement pour la gamme de rotations observées dans
l'ensemble d'apprentissage.

On peut penser aux convolutions comme à des "match filters". Une
certaine forme va "activer" telle couche à tel endroit et le réseau
"transmet" cette activation aux couches ultérieures en perdant
progressivement l'information de localisation spatiale.

Cette parenthèse refermée, il reste à tirer une conclusion de la
discussion sur l'apprentissage de représentation vs inférence
"classique".

Pendant longtemps, je crois, j'ai cru dans la possibilité de
s'affranchir de toute représentation en inférence statistique - ou
plutôt, j'ai cru qu'il était préférable en toute circonstance de faire
de l'inférence avec des représentations universelles.

Par exemple, en vision par ordinateur, une image est une fonction
scalaire d'une grille spatiale, c'est vrai en toute circonstance et
c'est donc une représentation maximale. On peut exhiber des modèles
génératifs d'images qui permettent de faire de la segmentation, par
exemple. Cette conception est-elle contradictoire avec l'idée-même de
rechercher des représentations?

La segmentation d'images doit-elle faire appel au deep learnign? En
revenant de Berlin fin août, j'ai imaginé un algorithme de
segmentation des plus classiques basé sur un clustering en intensité
(sans faire intervenir l'espace comme dans le cas des "super voxels")
via un bon vieux k-moyen ou, si on veut éviter des régions trop
petites, via un VEM régularisant comme à la grande époque.

Ensuite, il convient de séparer les clusters en composantes connexes
et éventuellement faire un peu de morphologie math pour nettoyer tout
ça... 

Les régions homogènes ainsi produites sont de tailles arbitraires. Je
ne crois pas à l'approche "super pixel": si on l'applique à une image
uniforme, elle va produire plein de super pixels qui ne servent à
rien. L'absence de toute contrainte spatiale dans le clustering permet
de produire des représentations plus parcimonieuses.

A ce stade, il n'y a plus qu'à assembler les régions pour reconnaitre
des objets. On peut voir ça comme un problème d'optimisation
combinatoire. Pour chaque combinaison possible de régions, avec
éventuellement des contraintes morphologiques, identifier l'objet
éventuel que produit cette combinaison.

Revenons à la segmentation. La segmentation par intensités n'est pas
très robuste pour les images naturelles, du fait des effets de
réflection, ombres, textures... Je veux dire par là qu'un objet peut
se retrouver éparpillé sur plusieurs clusters. Pour remédier à ce
souci, le clustering pourrait utiliser des critères locaux de
"texture" en plus de la couleur. Bref, une représentation plus
pertinente.

On pourrait se dire que c'est juste rendre l'information redondante
puisque l'image est une représentation maximale d'elle-même, si j'ose
dire, et là on reboucle sur la question posée plus haut. Une
représentation n'est utilisable que si on dispose d'un modèle
génératif, or ce n'est pas pour la couleur dans une image naturelle!
Le clustering par k-means (ou sa version régularisée VEM) reposent sur
des modèles grossiers et ne peuvent que fournir des résultats
grossiers.

Je me suis dit, à l'aéroport de Berlin, qu'on pourrait peut-être
identifier une bonne représentation avec un CNN. L'idée serait alors
de remplacer la couleur par cette représentation plus complexe et plus
adaptée aux images naturelles. Il faudrait juste exhiber un modèle
(semi)génératif pour cette représentation... ce qui permettrait
d'ajuster certains paramètres à la volée, comme l'intensité, la
couleur, etc.

Reste juste à construire un CNN qui permettrait d'identifier des
"textures" dans les images naturelles. Comment faire ça? 

--------------------------------------

Sepember 18...

On the stochastic EP algorithm...

At the end of the day, we do not need to rely on a factorization of
the target distribution.

Consider the situation where we are iteratively approximating some
target distribution. At some iteration, we have:
- a current Gaussian approximation q
- a batch approximation f

We want to update the Gaussian approximation based on these two
things. A natural thing to do is to start with some geometric average
of both, hence introducing the target:

p = q^1-lda f^lda

where lda is some "learning rate" parameter in (0,1). Using a Taylor
expansion of log(p) at the location parameter of q (or, equivalently,
q^1-lda), the new Gaussian has hessian and mean:

h = (1-lda) h + lda hi

m = m - lda gi / h


Proof: 

log p = (1-lda) log q + lda log f

==> g = (1-lda) g + lda gi ==> g = lda gi at minimizer of log q. 

==> h = (1-lda) h + lda hi

The soucy with this update rule is that there is no warranty that H be
positive definite - and that is all the more likely to happen as the
iteration is far from convergence. We need to stabilize the updating
process for small H. One way to go is to do:

m = m - lda gi / k

k = max(h, lda / ss)

where ss is a positive step size and lda is the learning rate. This
prevents large variations of m whenever the hessian estimate
vanishes. In the early iterations, this safeguard may apply frequently
but as iterations progress, the learning rate gets smaller and the
hessian tends to take over.

Let's compare this with the momentum method, with the convention that
lda starts at 1, we have:

vi = gamma * vi + gi
m = m - lda * eps * vi

Note the case gamma=0 corresponds to the plain steepest descent. With
nonzero gamma, the "speed" vi is a stabilized version of the gradient
up to a constant factor:

v <--> g / (1-gamma)

Therefore, the momentum method is a stable shooting version of the
steepest descent with initial step size ss = eps/(1-gamma):

m = m - lda * (eps / (1-gamma)) * gi

eps is often referred to as the "learning rate" although it's
misleading IMO because it collapses the notion of "learning rate" with
that of step size. I prefer to pick a step size ss and call
tensorflow's function with an initial "learning rate" equal to ss *
(1-gamma).

What should be called learning rate is the importance given to the
batch under consideration relative to the current parameter
estimates. It should decrease from 1 to 0 throughout the learning
process, always starting from one since the initial parameter guesses
are just random.


--------------------------------------

Alternative SEP update:

What we have been doing so far is:

h = (1-lda) h + lda hi
k = max(h, lda / ss)
m = m - lda gi / k

This implies that k >= lda / ss, hence k can be seen as a precision
estimate.

Instead, we could consider:

h = max((1-lda)h + lda hi, lda / ss)
m = m - lda gi / h

The only difference is that the Hessian is constrained to be positive
definite at each and every iteration, reflecting the prior assumption
that the posterior distribution on the net parameters is well-behaved
(has a finite variance). That way, the algorithm does not record any
negative hessian. 

We need to make sure that this iteration makes h converge to the
actual hessian. Well, not being too formal, if h converges almost
surely to some value hinf, it means that h becomes as close as we want
to deterministic in the long run, so for small lda we have (roughly):

E(max((1-lda)h + lda hi, lda / ss)) = max(E(...))

==> E(h) = max((1-lda)E(h) + lda E(hi), lda / ss)

If E(hi) > 0, the above equation is satisfied with E(h) = E(hi).
If E(hi) <= 0, it is satisfied with E(h) = 0.

So, loosely speaking, h should converge to the "relu" of the
Hessian. But what if the Hessian has negative elements? We won't see
them. The method makes a fairly strong assumption about the posterior
distribution of the parameters... How can we rule out that the
distribution is improper?  This could happen if the network is badly
designed, or if there is simply not enough training data...

In conclusion, the alternative does not make much sense.

--------------------------------------

One key observation is that the "annealing schedule" of the learning
rate has a great impact on the training performance. I use a rule of
the form:

lr = 1 / [1 + i * a]^b,

where i is the iteration number, a is the annealing 'speed' and b is a
'power' parameter. We should choose b according to the Robbins-Monro
convergence theorem, i.e. 0.5 < b <= 1.

The speed parameter, on the other hand, can be chosen as any strictly
positive value. In practice, no constant value seems
satisfactory. With a large value, the learning phase may need a
ridiculous number of iterations to track the optimum. With a small
value, it may oscillate for way too much time. It looks as a good
practice to decrease the speed periodically (e.g., multiply the speed
by 10 every 10000 iterations).


--------------------------------------

Oct 1.

Back to transfer learning.

We have a general approach for learning that combines representation
learning with standard statistical inference. Representation learning
can be achieved using deep neural models trained on big data. But they
generally do not solve the particular task at hand. To achieve that,
we can perform statistical inference based on learned representations.

Let's try to apply this brilliant idea to person recognition. Assume
that our goal is to identify one person in an image among a set of
known people (with the possibility to declare the person as
unknown). What kind of deep learning models can directly perform this
task? The answer is: none, because we would have to train such a model
on the very set of people to be recognized in practice... which is
meaningless.

What we can hope from deep learning in this case is a "generic model"
that can compute the probability that two images represent the same
person or not. This does not solve our problem but provides a key step
towards the solution.

How can we use the "generic model" from that point on?

One obvious approach in this particular case is to use the deep model
to compare an input image with a bunch of images from known persons
and somewhow combine the probabilities output by the deep model to
make a decision about the image under consideration.

Another natural approach is to train some generative classifier
(e.g. naive Bayes) on the features learned by the "generic model" (the
output of the second-to-last layer). In this typical transfer learning
scenario, the generic model is only used as a feature extractor to
pre-process the data for our task. The drawback is that we might need
a fairly bigger training dataset than in the previous approach because
we have to estimate some parameters unlike in the first approach.

Are we done? No... Another idea we can have is to use the "metric"
learned in the last layer of the generic model to implement some sort
of KNN classifier. How is that different from approach 1?


-----------------------------------------------------------------------

Oct 15.

Some remaining questions after experimenting SEP for training...

One unpredicted outcome is that many hessian elements (and so
precisions) are negative! Does it mean that SEP converged to a saddle
point? Is it possible? Of course, it is. But is it really what is
happening here? That's one riddle to solve. If it is, we certainly
have a regularization issue... But it could be that the hessian is
poorly estimated.

How can we tackle this? 

Oct 16.

Watch Andrew Ng talk: 
https://www.youtube.com/watch?v=F1ka6a13S9I

Engineering features vs end-to-end approach (data -> output).

We need both as we may not have enough data for end-to-end
approach. For instance, to infer age from an x-ray image, we can
implement an end-to-end approach to estimate bone lengths from x-ray
and then some simple method to infer age:

image -> bone lengths -> age

Constant challenge in AI: what to do next? Look at the algorithm
errors:

human-level error
training set error
dev (validation) set error

If large bias between human-level and training, we have a "bias" problem.
If large bias between training and dev, we have a "variance" problem.

Simple flowchart:

Training error high (large bias)?
If yes, bigger model, train longer, new model architecture
Until bias is small.

Then, is dev error high (large variance)?
If yes, more data, regularization, early stopping, new model architecture
Until variance is small. Done.

Data synthesis.
Unified data warehouse is very important.

Now, train and test sets usually come from different distributions... Make
sure dev+test are from the SAME distribution. Because we will optimize
the algorithm on dev and we want to it to work well on test!

Also good to have a train-dev set (used for testing but from same
distribution as training data).

Dev/test set = problem specification

So the generalized recipe. Start with measuring the following 5 numbers

1. Human-level error
2. Training set error
3. Train-dev set error
4. Dev set error
5. Test set error

Gaps: 1-2 = bias, 2-3 = variance, 3-4 = distribution mismatch, 4-5 = overfitting. 

Is 1 high? --> bigger model, train longer, new model archi
Is 2 high? --> more data, regularization, new model archi
Is 3 high? --> more data similar to test, data synthesis, new model archi
Is 4 high? --> get more dev set data (new model archi?)
Done.

Why is it difficult to suprass human performance?

Humans are pretty good, close to optimal error rate (Bayes
rate). Cannot do better than that cause input is noisy. Sometimes
"humans are a proxy for the Bayes error rate".

While worse than humans, we have good ways to make progress: get
labels from humans, error analysis, estimate bias/variance
effects. For example, if the training set error is much worse than
human-level, we can work on reducing bias. Otherwise, maybe work on
reducing the variance.

Idea: segment training set for subsets where AI still does worse than
humans.

But how do you define human-level performance? In a medical app,
typical human? typical doctor? expert doctor? team of expert doctors? 

What can AI/DL do? 

Processes for AI product definition are lacking. 

1) anything a person can do in < 1 sec
Not perfect but quite useful. Basically, perception.

2) predicting outcome of next in sequence of events.
We have big data for "things that happen over and over". Will this guy click on this add?

Pieces of advice:
Read papers + replicate results.
Dirty work (download data, code, hacking...).

AI is the new electricity.

---

Andrew Ng about saddle points...

https://fr.coursera.org/lecture/deep-neural-network/the-problem-of-local-optima-RFANA

"Most points with zero gradient are saddle points"

"In very high-dimensional spaces, you are actually much more likely to run into a
saddle point than a local optimum"

From LeCun, Nature 2015:

"Recent theoretical and empirical results strongly suggest that local
minima are not a serious issue in general. Instead, the landscape is
packed with a combinatorially large number of saddle points where the
gradient is zero, and the surface curves up in most dimensions and
curves down in the remainder. The analysis seems to show that saddle
points with only a few downward curving directions are present in very
large numbers, but almost all of them have very similar values of the
objective function. Hence, it does not much matter which of these
saddle points the algorithm gets stuck at."

He refers in particular to a paper by Dauphin et al, "Identifying and
attacking the saddle point problem in high-dimensional non-convex
optimization", NIPS 2014.

"A typical problem for both local minima and saddle-points is that
they are often surrounded by plateaus of small curvature in the
error. While gradient descent dynamics are repelled away from a saddle
point to lower error by following directions of negative curvature,
this repulsion can occur slowly due to the plateau."

Insight from stastitical physics: strong correlation between the error
and the index of a critical point (fraction of negative Hessian
eigenvalues). "The larger the error, the larger the index" and
vice-versa. So we want an optimizer that can escape saddle
points with large index values.

Interesting finding: any saddle point is an attractor for the Newton
method (not for a gradient descent!). 

Oct 17.

Back to LeCun's Nature paper. 

The selectivity-invariance dilemna. Representations need to be
"selective to the aspects of the image that are important for
discrimination" but "invariant to irrelevant aspects such as the
pose".

About ReLU: "In past decades, neural nets used smoother non-linearities, such as
tanh(z) or 1/(1 + exp(-z)), but the ReLU typically learns much faster
in networks with many layers, allowing training of a deep supervised
network without unsupervised pre-training". 

"The hidden layers can be seen as distorting the input in a non-linear
way so that categories become linearly separable by the last layer".

In the 90's, "it was commonly thought that simple gradient descent
would get trapped in poor local minima". The reality is that there are
many saddle points (not local minima) corresponding to different
weight configurations, and most of these yield satisfactory
performance at the task.

Neural networks were revived in 2006 by Hinton and colleagues with an
unsupervised pre-training approach.

"For smaller data sets, unsupervised pre-training helps to prevent
overfitting. [...] Once deep learning had been rehabilitated, it
turned out that the pre-training stage was only needed for small data
sets."

About CNN architecture: "Although the role of the convolutional layer
is to detect local conjunctions of features from the previous layer,
the role of the pooling layer is to merge semantically similar
features into one."

"The convolutional and pooling layers in ConvNets are directly
inspired by the classic notions of simple cells and complex cells in
visual neuroscience 43 , and the overall architecture is reminiscent
of the LGN-V1-V2-V4-IT hierarchy in the visual cortex ventral
pathway."

Face recognition using CNN: Deepface by Taigman et al, CVPR'14.

RNN: recurrent CNN. "For tasks that involve sequential inputs, such as
speech and language, it is often better to use RNNs".

The conclusion is that the future is unsupervised learning. "Human and
animal learning is largely unsupervised: we discover the structure of
the world by observing it, not by being told the name of every
object."

-----

Taigman et al, 2014. Deepface.

"Most current face verification methods use hand-crafted features."

"The proposed face representation is learned from a large collection
of photos from Facebook, referred to as the Social Face Classification
(SFC) dataset."

Alignment --> CNN --> SFC label.

The alignment pipeline relies on a fiducial point detector. The
particular detector used in this paper is an SVM "trained to predict
point configurations from an image descriptor" (in this case, LBP
histograms).

Interesting that the data is processed before entering the CNN, and
the pre-processing happens to be alignment...

-----

Huang et al, Learning to Align from Scratch, NIPS 2012. 

"One of the most challenging aspects of image recognition is the large
amount of intra-class variability, due to factors such as lighting,
background, pose, and perspective transformation."

Congealing = groupwise image registration via intensity-based
similarity measure.

They do deep congealing. Looks like the idea is to replace the raw
images with feature maps output by a CNN (without dense layers). But
they train their CNN in an unsupervised way, considering it as a
convolutional deep belief network (CDBN). This means that the CNN
represents the joint distribution p(input, feature) as opposed to the
conditional distribution p(feature|input). It is thus possible to
apply unsupervised learning, which in this instance just learns "useful"
features from images disregarding the alignment goal.

Not sure I understand everything but what a poorly written paper...

-----

Osadchy et al, 2007, Journal of Machine Learning
Research. "Synergistic Face Detection and Pose Estimation with
Energy-Based Models".

Simultaneous face detection and pose estimation.

The idea is to map a raw image to a low-dimensional feature space via
a CNN. In the feature space is pre-defined a "face manifold" F(Z)
indexed by the pose parameters Z. Given an input image X, the
proximity of the feature G(X) to the manifold tells us whether it is a
face image, and the closest point on the manifold yields an estimate
of the pose.

It looks like a generalization of the notion of classification. In
fact, up to the manifold embedding, which amounts to reparameterizing
the pose, this is just a regression problem.

-----

Chen et al, ECCV'12, "Bayesian Face Revisited: A Joint Formulation".

Straightforward application of Bayesian inference for
classification... The features are given and a generative model is
used.

-----

Restricted Boltzman machines and all that.

A RBM is a bipartite graph with binary variables. In short, two layers
(say, 'visible' and 'hidden'), only between-layer connections (no
intra-layer connections). 

Both p(h|v) and p(v|h) are factorial.

Meaning that: p(h|v) = prod_i p(h_i|v) and similarly for p(v|h). 

A classical perceptron only defines p(h|v). Is it factorial? Yes, it
is.

So, RBM is a generative version of the perceptron.

What is a DBN? Deep belief network. Essentially, it is a stack of
RBMs. Does it imply that the visible units are iid conditionally on
the first (or any) hidden layer? It does not. The only thing we can
claim is that they are iid conditionally on the LAST hidden layer.

Convolutional RBMs are special types of RBMs. Likewise, there are
convolutional DBNs.

The factor graph of RBM is pretty simple: each unit has an own factor,
and there are factors connecting each input/output pair.

-----

Go back to image registration. This is a case where a default
representation can do (basically, pixel pairs - if we allow for
"contextual" representations). Maybe "better" representations could be
obtained using deep learning but the default one does not require any
training, which is crucial if no training data is available.

Une question comme ça. Sous quelles conditions un réseau
convolutionnel produit-il des attributs échangeables? 


\documentclass{article}

\usepackage{fullpage}
\usepackage{amsmath,amssymb}
\usepackage{graphicx,subfigure}


\def\x{{\mathbf{x}}}
\def\a{{\mathbf{a}}}
\def\s{{\mathbf{s}}}
\def\r{{\mathbf{r}}}
\def\I{{\mathbf{I}}}
\def\eps{{\boldsymbol{\varepsilon}}}
\def\m{{\boldsymbol{\mu}}}
\def\S{\mathbf{S}}
\def\H{\mathbf{H}}
\def\g{\mathbf{g}}
\def\A{\mathbf{A}}
\def\B{\mathbf{B}}
\def\param{{\boldsymbol{\theta}}}

\title{Powell's method revisited using variational sampling?} 
\author{Alexis Roche} 
%\date{} 

\begin{document}

\maketitle      

\section{Introduction}

Some thoughts I had during the May 1st break... I came across the concept of variational sampling more than 8 years ago. I was always convinced that it was an interesting idea, but the reality is that I haven't been able to find any clearly useful application of it so far. 

My latest attempt to use VS as a building block for the Expectation Propagation algorithm ended up showing in practice that VS was not really better than the good old Laplace method. Before that, I had considered a brute-force implementation of VS to solve the problem of evaluating a KL-optimal approximation at a very general level, and the conclusion was that VS was not appropriate in large dimension.

One path I haven't followed up to now is the natural continuation of the one that took me to the VS idea. I was initially trying to perform gradient-free quadratic approximations to a cost function in the context of running Powell's optimization method. The standard implementation of Powell's method performs successive line minimizations using Brent's method, a fairly simple scheme that relies on one-dimensional quadratic approximations using three points. Beside the problem that Brent's method requires a fair number of function evaluations, it doesn't provide a global quadratic approximation to the objective function, unlike Newton's method -- and such an approximation would be nice to evaluate things like the posterior variance (if the cost is some kind of log-likelihood function).

\section{Factorial fit}

So here is the problem to solve. Consider an unnormalized distribution $p(\x)$. We want to approximate it by a factorized form $q(\x) = \prod_i g_i(\x_i)$, where $(\x_1,\x_2,\ldots)$ are coordinate blocks and each factor is parameterized independently, 
$$
g_i(\x_i) = \exp [\param_i^\top \phi(\x_i)]
.
$$

This is not even a restriction of the general fitting problem if the coordinate blocks are allowed to overlap. In fact, for now, the ``big idea'' is to perform an alternate minimization in the parameter space as opposed to a full minimization. Does it make our life easier? Of course, it does, because each $\param_i$ lives in a smaller dimensional space than all parameters altogether, so we will be performing many simple minimizations instead of a single very difficult one. VS can be used for each such minimization, obviously yielding a sequence of convex problems...

I cannot believe that I haven't thought about that earlier! This is actually not true, but it is true that I got lost on that  track, for reasons that I don't really know... I probably was blinded by two things: one was the EP algorithm, the other was some sort of belief that alternate minimization is irrelevant in convex problems. This conception is clearly wrong when considering the benefit of dimension reduction. On the other hand, the key difference with EP is that we don't assume here that the target distribution has a factorial form -- only the fitting distribution is factorial. So we are not trying to solve the same problem as EP, in fact our problem is (much) more general.

Each optimization step boils down to fitting $p(\x)$ with a form $c(\x)g_i(\x_i)$, where $c(\x)=\prod_{j\not= i}g_j(\x_j)$ is the very same thing as the context in EP, by minimizing the KL divergence $D(p\|cg_i)$ wrt $g_i$.


\section{Variational sampling idea}

The generalized KL divergence reads:
$$
D(p\|q) = \int  p \log \frac{p}{q} - p + q .
$$

If $p$ is our target distribution, the objective is minimized wrt $q$ in some approximation space. This is obviously equivalent to maximizing the ``log-likelihood'':
$$
L(q) = \int p \log q - q,
$$
or, more generally, if $q$ decomposes as $q=c g$ with $c$ fixed:
$$
L_c(f) = \int p \log g - c g
$$

The key idea of VS is to approximate the generally intractable objective via a sampling average. For this, we need to choose some sampling distribution $s(\x)$ and apply an associated quadrature rule:
$$
\int s(\x) f(\x) d\x \approx \sum_k w_k f(\x_k),
$$
leading to the surrogate objective:
$$
\tilde{L}_{c,s}(g)
= 
\sum_k 
w_k
\left[
a(\x_k) \log g(\x_k)
- b(\x_k) g(\x_k)
\right],
\qquad
a = \frac{p}{s},
\quad
b = \frac{c}{s}
.
$$

When the approximation space is an exponential family, $f(\x) = \exp[\param^\top \phi(\x)]$, we have:
$$
\tilde{L}_{c,s}(\param)
= 
\sum_k 
w_k \left[
a(\x_k) \param^\top \phi(\x_k)
- b(\x_k) e^{\param^\top \phi(\x_k)}
\right]
$$


$$
\nabla \tilde{L}_{c,s}(\param)
= 
\sum_k 
w_k \left[
a(\x_k) 
- b(\x_k) e^{\param^\top \phi(\x_k)}
\right]
\phi(\x_k)
.
$$

$$
\nabla \nabla^\top \tilde{L}_{c,s}(\param)
= 
- \sum_k 
w_k 
b(\x_k) e^{\param^\top \phi(\x_k)}
\phi(\x_k) \phi(\x_k)^\top 
.
$$

There are two arbitrary distributions involved in VS:
\begin{itemize}
\item the reference or cavity or context distribution $c$, which can be thought of as some window function defining the area of interest;
\item the sampling distribution $s$, which impacts the VS approximation accuracy; it's not obvious how to optimally choose $s$ (see appendix~\ref{app:opt_sampling}), but intuition suggests that sampling from~$c$ should do.
\end{itemize}


\subsection{Importance of context}

In my initial take on VS \cite{ijasp:13}, there was no context, meaning $c\equiv 1$. Choosing an appropriate sampling distribution was then fairly painful, as one just cannot sample from a uniform distribution... As an educated guess, one can try and sample from a distribution that approximates the target, yielding a typical ``chicken and egg" problem.

In my work on combining VS with EP \cite{rr:16}, I was using the context as the sampling distribution, which makes sense if the context is narrow enough compared to the variations of the target distribution -- something we expect to be verified in the late EP iterations. Arguably, a better choice would be the context multiplied by the current approximation to the factor being updated... By construction of EP, the context is also included in the target, i.e. $p=cf$, yielding an alternative interpretation that the factor $f$ is being approximated by another factor $g$. Choosing $s=cg_0$, we have $a=f/g_0$ and $b=1/g_0$.


\subsection{Iterative VS}

To date, I have only studied one-step versions of VS, but we can think of the following iterative scheme in case the sampling distribution is completely off as the context is too broad.

Given a target distribution $p$ and a context $c$, consider $f=p/c$ and do the following:
\begin{enumerate}
\item Pick some initial narrow context $c$ and some initial factor fit $g$ and set $s\leftarrow c g$.
\item Solve the VS problem for $s$, yielding a factor update $g$.
\item Widen context, reset $s\leftarrow c g$, and return to 2 until the context reaches its shape.
\end{enumerate}

If some sort of convergence occurs, we are left with both a parametric approximation to the target and an approximate independence sampler... Sounds a bit naive, though. In low dimension, there is some hope that the above iteration may work, but I wouldn't be too confident in high dimension... Intuition says that we need to take very small steps to update the context and that could be very time consuming. But, anyway, forget about the brute-force version of VS in large dimension... just too heavy. 


\section{Dimension reduction scheme}

So far we haven't made any use of the fact that the factor depends on a subset of coordinates only. The 1000 dollars question is: do we really need to sample the whole $\x$ space if the factor involves a few coordinates? Needless to say, this could be time consuming. 

Let us take a step back and note that, if $f_i$ is a function of $\x_i$ only, the objective (not the surrogate one) amounts to an integral in the relevant subspace:
$$
L_\pi(f) = \int [ p_i(\x_i) \log f(\x_i)  - \pi_i(\x_i) f(\x_i) ] d\x_i,
$$
with:
$$
p_i(\x_i) = \int p(\x) d\x_{-i},
\qquad
\pi_i(\x_i) = \int \pi(\x) d\x_{-i}
,
$$
but this is still cumbersome because we don't have the marginal $p_i(\x_i)$ and would need some extra magic to somehow evaluate it.

What if we approximate $p_i(\x_i)$ proportionally to a coordinate application for some fixed values of the coordinates not included in $\x_i$... This would be verified, for instance, if we replace the actual target with:
$$
\tilde{p}(\x) = \lambda c(\x) p(\a_{-i}, \x_i),
$$
where $\lambda$ is some constant and $\a$ is some current estimate of the target mode and $\a_{-i}$ denoting the ``fixed" coordinates of $\a$. This way, the factor to be adjusted is essentially the coordinate application at the mode estimate.

Doesn't that sound familiar? Yes, it reminds us of the EP trick! The {\em only} difference, or say, addition, is that the factor is independent from any context in EP because the target is assumed to be factorial, while here no such assumption is made. And what I am finding out here, is that there is a very simple way to generalize the EP trick, which is to do this $p(\a_{-i}, \x_i)$.

How shall we choose $\lambda$? Let us assume {\em non-overlapping coordinate blocks}. We are approximating each coordinate application by some parametric form and a particular iteration of our algorithm is concerned with a particular coordinate application. Defining:
$$
g_{-i}(\x)\equiv \prod_{j\not=i} g_j(\x_j),
\qquad
c(\x) \equiv \frac{g_{-i}(\x)}{g_{-i}(\a)},
$$
where it is natural to choose $\a$ as the current approximation maximizer, the coordinates of which respectively maximize each coordinate fit, a natural approximation to the target distribution is:
$$
p(\x)
\approx
\tilde {p}(\x)
= c(\x) p(\a_{-i},\x_i)
\approx c(\x) g_i(\x_i),
$$
which verifies the property $\tilde{p}(\a)=p(\a)$. The quantity $c(\x)$ can be seen as the {\em context} in which the coordinate fit is performed, so here the context is normalized (it's not the mere product of the factors).

In summary, our scheme is similar to EP up to two modifications:
\begin{enumerate}
     \item The context is normalized.
     \item The target factor is contextual (depends on the context). 
\end{enumerate}


\section{Generalized EP}

The goal is to describe some target distribution as a set of {\em factors}. All it takes is a parametric approximation space for each factor and a {\em contextual} approximation scheme. The contextual scheme is a method to approximate the target depending on the said factors. More specifically, there is one scheme for each factor, which determines an approximation to the target given the factor under consideration and all the other factors fitted.

We thus have two approximation levels: one structural (the description stage) and one numerical (the fitting stage). The algorithm is fully determined by the description stage.

In the classical EP, the description is exact in the sense that it recovers exactly the target if all factors are exact.

In the coordinate-wise EP, the description is inexact unless the target is separable, i.e. is the product of its coordinate applications up to a constant. But we could not care less.

The two things that these situations have in common is the factorial nature of the approximating distribution and the fact that each factor may involve a reduced set of coordinates. In fact, if they don't, one may question the benefit of the factor decomposition.


\subsection{Motivation}

The above insight helps understanding the very nature of EP: it is a dimension reduction trick! Minka never gave a clear motivation for the structural approximation underlying EP \cite{Minka-01,Minka-05}. 

When factors correspond to data items, one advantage is that we don't need to load the whole data in memory -- but is it decisive? Our premise is that we want an approximation of the form: 
$$
p(\x) 
\approx
c(\x) g_i(\x_i)
$$

Another potential motivation, which is more or less what Minka seemed to have in mind, is that the fitting problem may enjoy a closed-form solution with the approximation while being intractable without. This is of course not always true, as in the logistic and hinge regression problems considered in my EP/VS paper. Reason why VS is an alternative to EP {\em per se} in moderate dimension.

Dimension reduction appears as a stronger rational: if the factors happen to depend on subsets of variables, then the EP trick leaves us with smaller dimensional fitting problems. I don't remember Minka mentioning this. In the classical online learning setting, this does not happen because each factor is the model response to a single data item and thus basically depends on all the model parameters...

\subsection{Global fit}

As the algorithm hopefully converges, do the different factor approximations $c_{-i}g_i$ converge to the same distribution? If so, it would yield a global fit to the target, which may be needed sometimes. 

Open question: how to design the contextual scheme to guarantee this convergence in general? It is straightfoward in the classical EP. However, in the coordinate-wise EP, it is unlikely to occur because, even if $\a$ converges, all fitted factors may not have exactly the same value at $\a$. 

To address this, we can simply define a global approximation scheme to be used in the end, which does not need to be consistent with the contextual approximation scheme. 

The contextual scheme applies when trying to best approximate the target while able to evaluate each factor independently, and only one exactly. 

The global scheme applies when trying to best approximate the target while able to evaluate each fitted factor independently. 

It is about setting constraints to ourselves in order to gain freedom. As often in computational statistics.


\subsection{Machine learning}

It sounds a bit like I was not using EP at its best for machine learning. Instead of having factors associated with data items, they should be associated with model parameters. But... wait... the target distribution does not factorize across model parameters, so EP is not applicable here. It's not, but we have seen how we can extend EP to satisfy our need.

If we still wish to do online learning, we have the opportunity to keep the factorization across batches, and sub-factorize each batch factor across model parameters. This means that each iteration of the algorithm will update a specific parameter subset based on a specific batch.


\section{Epilogue}

Il n'est pas n\'ecessaire d'imposer la factorisation par coordonn\'ees de la forme approximante. Celle-ci d\'ecoule directement de l'approximation contextuelle ``g\'en\'eralis\'ee'' dans laquelle le facteur \`a ajuster est remplac\'e par le produit de ses applications coordonn\'ees. Avec \c ca et la forme param\'etrique factorielle, choisie {\em a priori}, non seulement l'approximation est factorielle mais, et c'est le point cl\'e, l'ajustement des facteurs peut se faire coordonn\'ee par coordonn\'ee, c-\`a-d, grosso modo, par des ajustements unidimensionnels peu co\^uteux. 

La seule innovation n\'ecessaire, par rapport au FF-EP de base, c'est l'approximation du facteur lui-m\^eme dans l'approximation contextuelle. On a vu en quoi cette id\'ee permet de r\'eduire la dimensionalit\'e du probl\`eme d'ajustement par divergence KL et de faire de l'approximation variationnelle {\em low cost}.

Une subtilit\'e qui m'avait \'echapp\'e lorsque je travaillais sur FF-EP, c'est que la m\'ethode de Laplace (qu'on peut voir comme une alternative \`a l'ajustement via KL) garantit elle aussi la s\'eparabilit\'e de l'ajustement des applications coordonn\'ees (\`a condition de faire le DL au centre de la cavit\'e). En revanche, les m\'ethodes de quadrature n'ont pas cette propri\'et\'e. On le voit ais\'ement avec la quadrature gaussienne: chaque moment est estim\'e par une somme pond\'er\'ee de $2d+1$ points o\`u toutes les dimensions varient \`a tour de r\^ole.  

Pour \'eviter cette lourdeur, il suffit de recourir \`a notre petite astuce: imposer la forme s\'eparable au facteur et remarquer que, d\`es lors, l'ajustement via KL est lui-m\^eme s\'eparable. Le gain en efficacit\'e est proportionnel \`a la dimension, donc \'enorme en grande dimension!

Intuitivement, on ne perd pas grand chose \`a ajuster chaque application coordonn\'ee l'une apr\`es l'autre. C'est aussi ce que fait la m\'ethode de Laplace ``gratuitement'' au sens o\`u elle donne une approximation quadratique {\em non-s\'eparable} dont on d\'eduit naturellement une approximation factorielle en ignorant les termes non-diagonaux du hessien. C'est ``\'evident'' mais je me rends compte que je ne m'\'etais pas fatigu\'e \`a justifier cela dans mon c\'el\`ebre article. 

Une justification, celle que j'avais en t\^ete je crois, c'est de dire qu'on applique la formule de mise \`a jour de FF-EP \`a l'approximation de Laplace du facteur, ce qui a pour seul effet d'annuler les termes non-diagonaux du hessien. Pourquoi? Parce que c'est une propri\'et\'e de la gaussienne: en annulant les moments crois\'es, on ne modifie pas les moments non-crois\'es.

Une autre justification, c'est d'appliquer la m\'ethode de Laplace \`a l'approximation contextuelle, ce qui produit le m\^eme r\'esultat mais est plus coh\'erent avec l'id\'ee que la m\'ethode de Laplace remplace la minimisation de la divergence KL. 

En d'autres termes, la g\'en\'eralisation de l'approximation contextuelle EP \'etait d\'ej\`a plus ou moins sous-jacente \`a mon article de 2016...



\appendix

\section{Optimal sampling distribution}
\label{app:opt_sampling}

For random sampling, it can be shown that the VS solution achieves a KL divergence larger than the actual minimum by a quantity vanishing at asymptotic rate $\nu/N$ with:
$$
\nu = \frac{1}{2}\int \frac{[p(\x)-q_\star(\x)]^2}{s(\x)} \psi(\x)^2 d\x,
\qquad 
\psi(\x)^2 = \sum_j \phi_j(\x)^2,
$$
where $q_\star$ is the optimal factor, i.e. $q_\star=\arg\min_q D(p\|q)$, and $\phi_j$ is an orthornomal basis for the dot product induced by~$q_\star$:
$$
\int q_\star \phi_j \phi_k  = \delta_{jk} 
\qquad \Rightarrow \quad
\int cg_\star \psi^2 = n,
$$
where $n$ is the number of free parameters. 

Minimizing $\nu$ wrt $s$ is akin to optimal design in importance sampling. The optimal sampling distribution is: $s_\star \propto |p-q_\star|\psi$. As can be seen, it is not simply $p$ as could perhaps be expected or hoped. Note that the equivalent IS problem is to compute the integral:
$$
\int (p-q_\star)\psi,
$$
but this is purely theoretical as we don't know $q_\star$ (yet we do know that this integral is zero).

Now it is time to remember that we may have a context, meaning that $p=cf$ and $q=cg$. We can rewrite the same story using the minimizer $g_\star$ of $D(cf\|cg)$ and the same kind of orthonormal basis built from $cg_\star$:
$$
\nu = \frac{1}{2}\int \frac{c^2}{s} (f-g_\star)^2 \psi^2,
$$
and the optimal sampling distribution is $s_\star \propto c|f-g_\star|\psi$. If the functions under consideration vary slowly compared to $c$, then $c$ should be a fair approximation to $s_\star$. Using $c$ as a sampling distribution, we have:
$$
\nu = \frac{1}{2}\int c (f-g_\star)^2 \psi^2
$$

Letting $\omega=cg_\star\psi^2/n$, which is a density, i.e. $\int \omega = 1$, we see that the variance factor is proportional to the chi-square divergence between the factors in the context $\omega$: 
$$
\nu 
= \frac{n}{2} \int \omega \frac{(f-g_\star)^2}{g_\star}
= \frac{n}{2} D_2(\omega f\| \omega g_\star)
.
$$

So we have an interesting outcome here: the variance relates to the contextual divergence between factors, but this is neither the same divergence nor the same context as the initial ones (those considered for minimization): $\chi_2$ instead of KL and $\omega$ instead of $c$... Yet, we can say loosely that $\nu$ should be smaller as the context gets narrower, hence VS should be faster to converge.

If we are not in ``narrow context'' regime, then we are kinda screwed, right? But we can try and generalize the above argument:
$$
\nu = \frac{n}{2}\int \omega \frac{c}{s} \frac{(f-g_\star)^2}{g_\star}
.
$$

So, now, we see that this is the same story as before by redefining the context as $\omega'=\omega c / s$, which may no longer be a density. If we set $s=c g_\star/z$ with the constant $z$ so that $\int s = 1$, then the context simplifies to $\omega'=(z/n) c \psi^2$, hence:
$$
\nu = \frac{z}{2} D_2(c' f\| c' g_\star),
\qquad
c' \equiv c\psi^2
.
$$

It now looks like we have a similar interpretation as above, with the exception that $n$ is replaced by $z$, so the variance does no longer depend on the family dimension! On the other hand, the chi-square divergence involved here may not be as small as one would like since we assume a possibly broad context. 

Those are just some insights on the effect of the sampling distribution. In summary: if the context is narrow, it is a good choice as the sampling distribution; if not, then multiply it by some initial guess of the factor fit... and pray cause there is no warranty.

The conclusion is that VS may be inefficient unless used in a certain {\em context}. A context is a region wider than an infinitesimal neighborhood but can't be too wide. If very narrow, VS will most likely work very well but will ressemble the Laplace method. If broader, VS efficiency will heavily rely on the availability of a good approximation to the target. 


\section{Adding quadratic forms}
\label{sec:quad_add}

Consider a quadratic form parameterized by a tuple $(a,\m,\H)$ as follows:
$$
q(\x) = \alpha + \frac{1}{2} (\x-\m)^\top \H (\x-\m)
$$

Let now two quadratic forms $q_1$ and $q_2$ with respective parameters $(\alpha_1,\m_1,\H_1)$ and $(\alpha_2,\m_2,\H_2)$. What are the parameters $(\alpha,\m,\H)$ of $q_1+q_2$? 

\subsection{Basic formula}

By definition,
$$
(q_1 + q_2)(\x)
= 
\alpha_1 + \frac{1}{2} (x-\m_1)^\top \H_1 (x-\m_1)
+ \alpha_2 + \frac{1}{2} (x-\m_2)^\top \H_2 (x-\m_2)
$$

Clearly, $\H$ is the Hessian of $q_1+q_2$. To find $\m$, we need to solve for $\nabla(q_1 + q_2)(\m)=0$. Finally, $\alpha=(q_1+q_2)(\m)$. We have:
$$
\nabla(q_1 + q_2)(\x)
= 
\H_1 (x-\m_1) + \H_2 (x-\m_2)
$$
and:
$$
\nabla\nabla^\top(q_1 + q_2)(\x)
= 
\H_1 + \H_2
$$

Therefore, we find that:
$$
\H=\H_1+\H_2,
\quad
\m = \H^{-1}(\H_1\m_1 + \H_2\m_2),
\quad
\alpha =
\alpha_1 + \alpha_2 +
\frac{1}{2} 
\left[
\|\m-\m_1\|_{\H_1}^2 + \|\m-\m_2\|_{\H_2}^2
\right]
.
$$

\subsection{Incremental form}

We can derive the following alternative expressions of incremental nature:
\begin{itemize}
\item For the Hessian:
$$
\H = \H_1 + \Delta\H,
\qquad 
\Delta\H \equiv \H_2
.
$$
\item For the center:
$$
\m = \m_1 + \Delta\m,
\qquad 
\Delta\m \equiv \H^{-1}\H_2(\m_2 - \m_1)
.
$$
\item For the offset:
$$
\alpha = \alpha_1 + \Delta \alpha,
\qquad
\Delta \alpha \equiv \alpha_2 + \frac{1}{2}
\left[
\|\Delta\m\|_{\H_1}^2
+ \|\Delta_2\m\|_{\H_2}^2
\right]
,
$$
with $\Delta_2\m = \H^{-1}\H_1(\m_2-\m_1)$.
\end{itemize}

These show the final parameters as updates to the initial parameters of one of the two quadratic forms (it doesn't matter which one, of course). Essentially, the Hessian increment is the other quadratic Hessian, the center increment is based on the center difference, and the offset increment combines both the other offset and the center difference.

Both the Hessian and center update rules are straightfoward from the above. To derive the update rule for the offset, we first note that $\m-\m_1=\Delta\m$ by definition, hence: 
$$
\|\m-\m_1\|_{\H_1}^2 = 
\Delta\m^\top \H_1 \Delta\m,
$$
and the remainder follows by symmetry.



\section{Laplace approximation}
\label{sec:laplace_approx}

Consider now another type of parameterization for a quadratic form (one that naturally follows from a Taylor expansion, for instance):
$$
q(\x) = \beta + \g^\top (\x-\a) + \frac{1}{2} (\x-\a)^\top \H (\x-\a)
$$

What are the $(\alpha,\m,\H)$ parameters of $q$?

Obviously, the Hessian has to be the same in both forms since $\nabla\nabla^\top q = \H$.
Using the gradient,
$$
\nabla q (\x) = \g + \H (\x-\a),
$$
we identify $\m$ via $\nabla q(\m)=0$, yielding $\H(\m-\a)=-\g$:
$$
\m = \a - \H^{-1}\g.
$$

Finally, we find $\alpha$ via $\alpha=q(\m)$, hence:
$$
\alpha = \beta - \frac{1}{2} \g^\top \H^{-1}\g
= \beta + \frac{1}{2} \g^\top(\m-\a)
.
$$

\section{Laplace-EP}

Consider a target: 
$$
p(\x)=\prod_{i=1}^n f_i(\x)
$$

Assume we have some current Gaussian approximations $g_i$ of each factor and want to improve one of them. We form the cavity distribution, or context:
$$
c(\x) = \prod_{j\not=i} g_j(\x)
$$
in order to minimize $D(cf_i \| cg_i)$ over $g_i$, either exactly or approximately, yielding an unnormalized Gaussian with parameters $(\alpha_i,\m_i,\H_i)$. Let us denote the context parameters by $(\alpha_{-i},\m_{-i},\H_{-i})$ and the new global approximation parameters by $(\alpha,\m,\H)$. 

Following Section~\ref{sec:quad_add}, we have:
\begin{enumerate}
\item Hessian update:
$$
\H = \H_{-i} + \H_i,
$$
\item Center update:
$$
\m = \m_{-i} + (\H_{-i} + \H_i)^{-1}\H_i(\m_i-\m_{-i})
$$
\item Offset update:
$$
\alpha = \alpha_{-i} + \alpha_i + \ldots
$$
\end{enumerate}

One may wonder what is the interest of updating the offset... If the ultimate goal of running EP is to estimate the mode and the associated uncertainty, the offset is useless. We would need it, for instance, to evaluate the evidence for comparison with other models (e.g., decide which CNN architecture is best for some specific task and training dataset).
    
If we assume now that the factor approximation has a Taylor-like expression at the cavity center $\m_{-i}$, then according to Section~\ref{sec:laplace_approx} and denoting $\g_i$ the log-factor gradient, $\H_i (\m_i-\m_{-i}) = -\g_i$, we have:
$$
\m = \m_{-i} - (\H_{-i} + \H_i)^{-1}\g_i
$$

Doesn't it remind us of something? Yes, the second-order gradient descent. More precisely, the EP algorithm is comparable with a second-order version of {\em stochastic} gradient descent. Instead of taking a step propotional to the gradient using some (decaying) {\em learning rate} value, the gradient is scaled using anisotropic and adaptive curvature information. Updates tend to be large In the early iterations (assuming $\H\equiv 0$ initially) and get smaller as the Hessian stabilizes to a (hopefully) positive-definite matrix.

As for the offset update,
$$
\alpha = \alpha_{-i} + \frac{1}{2} \beta_i \g_i^\top(\m_i-\m_{-i}) + \ldots
$$
but we said we don't care, didn't we?

\section{Practical adaptations of EP}
\label{sec:practical_ep}

In large dimension, the EP algorithm is intractable for at least two reasons. First, the Hessian computation and inversion is too expansive. This turns out to be easy to work around using a fully factorized parametric form, which simply amounts to replacing the Hessian matrices with their diagonals in the computations. 

Another issue is parameter proliferation as EP keeps track of a complete set of parameters corresponding to each and every factor, hence requesting ridiculous memory load. Memory can be saved by assuming a unique approximation to all factors, yielding an EP variant known as the Averaged~EP (AEP) algorithm. 

On each iteration, we have a current unique factor approximation $g(\x)$ that fully determines the current {\em overall} approximation: $q(\x)=g(\x)^n$. Therefore, the cavity is $c(\x)=g(\x)^{n-1}$ independently from the factor under consideration, implying:
$$
\alpha_{-i} = \lambda \alpha,
\quad
\m_{-i} = \m,
\quad
\H_{-i} = \lambda \H,
\quad 
\lambda = \frac{n-1}{n}
$$

We then proceed to the factor fitting exactly as before, yielding a new overall approximation $q(\x)=c(\x)g_i(\x)$. However, instead of storing $g_i$, we collapse the new factor approximation to the previous ones via: $g(\x)=q(\x)^{1/n}$. This step makes the algorithm keep memory of all past factor approximations. In the standard EP, each new factor approximation simply substitutes the previous one. 

The problem I have with tis is: does AEP converge? From the center updating formula, it is clear that the condition for the algorithm to stop moving is that each factor gradient vanishes at the overall center estimate: this won't happen because the factors have  different centers (be them slightly or massively different)... This lack of convergence seems to arise due to the ``factor collapsing" step. 

We might need to introduce some decay to make convergence happen. One way to go is to say: OK, let's consider some scaling $f_i^\epsilon$, with $0<\epsilon\leq 1$ , as the factor under consideration, meaning that the context is made up of ``$n-\epsilon$ factors'', hence $\lambda=(n-\epsilon)/n$. The gradient and Hessian of the log-factor are respectively $\epsilon \g_i$ and $\epsilon \H_i$, hence the update rules generalize to:
$$
\H \leftarrow (1-\epsilon/n)\H + \epsilon \H_i,
\qquad
\m \leftarrow \m - \epsilon\H^{-1}\g_i
.
$$

Clearly, when the decay parameter $\epsilon\to 0$, these updates are effectless hence the algorithm converges. But does it converge to a reasonable solution? That's the question. More on this later.


\section{EP as Game Theory}

It occured to me that EP can be viewed as a game, where each factor is associated with a player whose aim is to fit a parametric form to the factor. Each player's payoff depends on all other players as the fit needs to be more accurate in the subspace considered most important collectively.

Player interference arises in a mere alternate minimization of the KL divergence. The key insight into EP is to make the ``where'' task much easier than in the alternate KL minimization approach as it essentially provides a relevance window function that is parametric, hence greatly simplifying each fitting problem.

From this standpoint, the question of convergence of EP boils down to the existence of a Nash equilibrium...

How about AEP? The difference with EP is that players do not see what the other players are doing. They only get aware of the overall fit, which is updated after each player's move but is not specific to one player. One way to account for this is to introduce an additional player that we may call the mediator. He's the only player the others speak to and he can speak with all of them. When a player makes a move, the mediator can reflect it. Problem is: the mediator's strategy depends on which factor he speaks to, so it doesn't seem to be standard game theory.


\section{AEP as Stochastic Approximation}

AEP is better understood in terms of an iterative algorithm minimizing factor-dependent costs $D_i(q,q')$ and performing updates $q\to F_i(q)$ where  $F_i(q)\equiv \arg\min_{q'}D(q^\lambda f_i\|q')$ with $q'=q^\lambda g$ or $F_i(q)=q^\lambda g$ with $g$ the Laplace approximation of $f_i$ around the center of $q$.

So it's a weird kind of ``game'' where players always move together. It's not in the framework of Nash equilibrium theory. Convergence in this scenario relies on the existence of a multiple fix point $q=F_1(q)=\ldots=F_n(q)$. This is too much to ask, insn't it? If all factors were the same, all transition functions would be the same and that could work, but there is no way this will happen in practice.

One important property of these transition functions is that they may all be seen as noisy versions of the same ``ideal'' transition function: ${\cal F}(q)=q_\star=\arg\min_{q'}D(p\|q')$ which is actually independent from $q$ and is the Holy Grail distribution this whole stuff is about trying to approximate. So, whenever we update~$q$, the result is slightly or even completely off, and it's only by averaging updates across iterations that we can achieve an accurate result.

Therefore, the iteration will jump from one update to another frantically unless it has some memory. Take a look at the update formula in Section~\ref{sec:practical_ep}. Defining:
$$
F_i(\m, \H) = (\m-\H^{-1}\g_i, \lambda\H+\H_i),
$$
they correspond to a Robbins–Monro algorithm to track the fix point of $E[F_i(\m,\H)]=(\m,\H)$ where $\epsilon$ can be chosen according to the Blum theorem to guarantee convergence. Bascially, $\sum_n\epsilon_n=\infty$ and $\sum_n\epsilon_n^2<\infty$.

Keras stochastic gradient implementation seems to use a time-based decay of the form:
$$
\epsilon_n = \frac{\epsilon_0}{1 + \alpha n},
$$
where $\epsilon_0$ is the initial learning rate and $\alpha$ is the decay value, therefore $\epsilon_n=O(1/n)$ and satisfies the Blum condition. 

Keras also shuffles the training data at the beginning of each epoch. This is not compliant with the native EP algorithm but is natural and easy to do with AEP. So shuffle or not shuffle? 

\section{Short biblio on stochastic gradient descent}

\subsection{First-order SGD}

The plain steepest descent looks like this:
$$
\x \leftarrow \x - \gamma_i \g_i
,
$$
where $\gamma_i$ is an adaptive learning rate parameter and $\g_i$ is the stochastic gradient. 

The so-called RMS propagation implemented in Keras normalizes the gradient using some running average of its magnitude:
$$
a^2\leftarrow \rho a^2 + (1-\rho) \|\g_i\|^2,
\qquad
\x \leftarrow \x - \frac{\gamma_i}{a + \epsilon} \g_i,
$$
where $0\leq\rho\leq 1$ defaults to 0.9 and $\epsilon$, which defaults to $10^{-7}$, is  here to avoid divisions by zero. The learning rate is computed as $\gamma_i = \gamma/(1 + \alpha i)$ with $\gamma=10^{-3}$ (initial learning rate) and $\alpha=0$ (decay) by default.

\subsection{Second-order SGD}

Bottou (2010) defines 2nd order SGD as an iteration of the form:
$$
\x \leftarrow \x - \gamma_i \Gamma_i \g_i,
$$
where $\Gamma_i$ is a ``positive definite matrix approaching the inverse of the Hessian'' and $\gamma_i$ is some decay parameter which, like in 1st order SGD, is needed to achieve convergence under the single-point approximation of the gradient (the ``stochastic'' part). But he notes that ``this modification does not reduce the stochastic noise and therefore does not significantly improve the variance" (of the parameter estimate).

Note sure if everyone agrees with that. It seems that some researchers still believe that 2nd order SGD is potentially faster to converge than its 1st order counterpart. One issue in the context of SGD is to come up with an effective online approximation to the Hessian. Then we might want to use the conjugate gradient (CG) method to solve the linear system associated with the SGD update, rather than inverting the Hessian itself. In practice, CG can be run for a few iterations to reach a good enough approximation -- this is known as the {\em Hessian-free} approach (HF). See James Martens (2010). 

So what kind of online Hessian estimates can be used? Martens simply use the Hessian of a large enough mini-batch based on purely empirical arguments. Argawal et al (2017) in a paper called {\em Second-Order Stochastic Optimization for Machine Learning in Linear Time} revisit the question and propose an algorithm called LiSSA (linear time second-order stochastic algorithm) that estimates the Hessian inverse incrementally.

The trick is to exploit the famous formula:
$$
\A^{-1} 
= \sum^{\infty}_{i=0} (I-\A)^i 
$$

In other words,
$$
\s_0 \equiv \g,
\qquad
\s_j \equiv \sum_{i=0}^j (I-\A)^{i}\g
$$
converges to $\A^{-1}\g$ as $j\to\infty$. The key observation is that $\s_j$ can be computed recursively via $\s_j=\g + (I-\A)\s_{j-1}$, giving rise to a linear solver algorithm that can be seen as an alternative to CG. Now, we would like to replace $\A$ with the Hessian but it's more effective to estimate the Hessian from a mini-batch. More specifically, the paper uses a different mini-batch at each step ``$j$'' of the recursion -- which makes it somewhat more sophisticated than the plain HF mini-batch approach.

To summarize, the fully factorial EP standpoint provides two things: 1) a neat justification to the diagonal approximation to the Hessian; 2) an online Hessian estimate that implicitely depends on all datapoints and likely becomes stable across iterations (unlike the gradient estimate).

\section{Robbins-Monro algorithm}

A brief refresher. The Robbins-Monro algorithm is a stochastic approximation method that uses an iteration of the form:
$$
\x_{n+1} = \x_n - \epsilon_n f_n(\x_n)
,
$$
in order to solve for $f(\x)=E[f_n(\x)]=0$, assuming that $f_n(\x)$ are random functions with same mean $f(\x)$. Equivalently, the iteration aims to find a fix point of $F(\x)=\x-f(\x)$ and also reads:
$$
\x_{n+1} = (1-\epsilon_n)\x_n + \epsilon_n F_n(\x_n),
$$
with $F_n(\x)\equiv \x - f_n(\x)$.

Now, consider the computation of a sample mean:
$$
\x_n = \frac{1}{n}\sum_{i=1}^n \a_{i-1}
.
$$

We note that $(n+1)\x_{n+1} = n\x_n + \a_n$ hence $\x_{n+1}=[n\x_n+a_n]/(n+1)$, so this is equivalent to the following iteration:
$$
\x_{n+1} = \x_n - \frac{1}{n+1} (\x_n-\a_n),
$$
which is of Robbins-Monro type with $f_n(\x)=\x-\a_n$, or $F_n(\x)=\a_n$, where $\a$ is the random variable, and $\epsilon_n=1/(n+1)$. The above general considerations confirm that this iteration is meant to track $\x=E(\a)$. 

But it also shows, by substituting formally $\a_n$ with $F_n(\x_n)$, that the Robins-Monro iteration with $\epsilon_n=1/(n+1)$ computes the average of $F_1(\x_1), F_2(\x_2), \ldots, F_{n-1}(\x_{n-1})$.

If it turns out that $F$ is constant, RM is just a way to compute a sample mean. More generally, the Robbins-Monro may be seen as a procedure to estimate some quantity from repeated noisy measurements.


\section{Some more stuff about learning}

Why using placeholders? Apparently, people like placeholders because it makes it possible to implement a computational graph without using any data. So we can decorrelate the network architecture from the training. I can see a potential advantage in terms of memory management but it looks like Keras does not use it (to fit a model using Keras, the whole training dataset shall be loaded into memory beforehand).

Dynamic learning phase? What is it? No clue.

How does Keras initialize the network weights?





\bibliographystyle{abbrv}
\bibliography{cvis,stat,alexis}


\end{document} 



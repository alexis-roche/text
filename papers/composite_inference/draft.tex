\documentclass[english]{scrartcl}

\usepackage{fullpage}
\usepackage{amstext,amssymb,amsmath,amsthm}
\usepackage{graphicx,subfigure}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{color}

\graphicspath{{.}{pics/}}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{proposition}[theorem]{Proposition}


\title{Composite Bayesian inference}
\date{}
\author{Alexis Roche\thanks{\url{alexis.roche@centraliens.net}}}

%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}
%\newcommand{\matphi}{\boldsymbol{\Phi}} 
%\def\x{{\mathbf{x}}}
%\def\z{{\mathbf{z}}}
%\def\u{{\mathbf{u}}}
%\def\p{{\bar{\mathbf{p}}}}
%\def\q{{\bar{\mathbf{q}}}}



\begin{document}

\maketitle

\begin{abstract}
This note revisits the concept of composite likelihood as a method to make a probabilistic inference by aggregation of multiple Bayesian agents. This view makes it natural to use a machine learning approach to calibrate the weights associated with composite likelihood. For instance, they can be tuned in a typical supervised learning phase so as to minimize cross-entropy on a labeled training dataset, yielding a convex problem. We argue that composite Bayesian inference is a middle way between generative and discriminative modeling approaches that trades off between interpretability and prediction performance, both of which are crucial to some artificial intelligence tasks.
\end{abstract}

% which stems naturally from a connection with the maximum entropy principle: the predictive distribution that maximizes conditional entropy relative to a given prior and subject to multiple mean log-likelihood inequality constraints is, up to a normalizing factor, the prior multiplied by a particular composite likelihood function, hence providing a ``composite'' extension of Bayes rule. {\color{red} Confusing: we simply maximize cross-entropy, which turns out to have a conditional maxent interpretation.} We argue that composite Bayesian inference is a middle way between generative and discriminative approaches to statistical inference, which can be powerful in shallow learning and transfer learning problems. {\color{red} Rephrase this: extends Bayes and keeps introspection ability, which is crucial in some AI problems.


\section{Introduction}
\label{sec:intro}

Frequentist and Bayesian inference paradigms rest upon the existence of a probabilistic data-generating model that is both empirically valid and computationally tractable. Because this is challenging for complex data, other inference models are commonly used in applied science: deliberately misspecified generative models, as in quasi-likelihood methods \cite{White-82,Walker-13} or na\"ive Bayes \cite{Ng-01}; data compression models as in minimum description length \cite{Grunwald-07}; and discriminative models\footnote{A {\em discriminative} model is a parametric family that describes the distribution of a variable of interest conditional on data, in contrast with a {\em generative} model, which describes the conditional distribution of data given the variable of interest.}, which currently dominate the field of artificial intelligence (AI) and typically require supervised learning on large datasets, including ``shallow'' learning techniques (random forests \cite{Ho-95}, maximum entropy classifiers/logistic regression \cite{BergerA-96}, Gaussian processes \cite{Rasmussen-06}, support vector machines \cite{Vapnik-00}) as well as most deep learning techniques \cite{Lecun-15,Goodfellow-16} (with the exception of deep belief networks \cite{Hinton-06}).

%Interpretability needed in life-impacting applications of AI. 
%Whenever possible, proxy features should be avoided as they make models gameable. 
%Local Interpretability for a Single Prediction: Why did the model make a certain prediction for an instance?
%What about sensitivity analysis: study the prediction as a function of a single feature...

Discriminative models are often considered as black boxes because their predictions may be valid  but difficult to interpret. This is problematic in life-impacting applications, where AI is confined to a supportive role in a human-driven decision making process \cite{Molnar-18}. For instance, in medical diagnosis, automated disease predictions should be corroborated by individualized findings to bring clinical value -- in other words, AI systems should be able to justify their opinions just like clinicians. Simple univariate generative models, although expectedly less powerful than discriminative models in terms of prediction performance, can provide decisive clues for diagnosis, {\em e.g.}, by pinpointing biomarkers that deviate from healthy reference ranges.

% simple methods based on range analysis of univariate biomarkers can provide meaningful clues, and are thus more useful in practice although less powerful in terms of prediction performance.

%With generative models you can compare data with expected outcomes to get insight. 

%the information they provide to support them is limited to the boundaries between classes, which are difficult to interpret for high-dimensional data representations. 

%Decision making is not about pooling opinions, it is about pooling empirical observations and find which hypothesis they point to.

%Need to emphasize why we need generative models. Below are some valid reasons: unsupervised learning, potentially better performance on small training datasets... But the \#1 reason is that discriminative models are black boxes: they provide an answer but no explanation to support it. Truth is, they perform better in supervised big data context in the sense of accuracy, cross-entropy, and so on, no question about that -- but they are mere inference recipes. Is that what AI should be? Sometimes, yes. But there are application fields where this clearly cannot work, for instance medical diagnosis. 

%Imagine a doctor telling a patient: ``Sorry, sir, you have Alzheimer's disease because this fancy AI software said so''. Unless the said software is 100\% reliable, which is practically impossible, this would obviously be unacceptable. Acceptable would be to corroborate the diagnosis with some key quantitative empirical observations: low memory test scores, hippocampal atrophy, etc. In other words, inference has to come with some insight. An AI system is but one expert relying on hypotheses which may be invalid, as any expert, and is therefore prone to error. A typical error source is to train a classifier on a subset of classes from the real world. We cannot avoid inference errors but we can safeguard against them by being able to interrogate the system: why do you think what you think? To achieve that, the system should be capable of {\em introspection.

%Vapnik: ``one should solve the classification problem directly and never solve a more general problem as an intermediate step''. True if the goal is classification only. Wrong if introspection is needed because the problem is then more general in nature.

%Human inference does not work in a pre-determined universe of ``causes''. We confront theories with reality to evaluate how likely they are, while knowing that none of the theories considered so far may be ``right''. In fact, we are not looking for the truth, but for a conving enough theory. 

%Composite likelihood seen as a pool of experts comes with a potential introspection mechanism: first sort experts by decreasing influence on the predictive distribution, see how likely the different classes are to the most influential experts.

Another limitation of discriminative models is that they are not suitable for unsupervised learning or on-the-fly parameter estimation because they treat the data and the model parameters as marginally independent, meaning that the data conveys no information about the parameters unless the variable of interest is observed. This is illustrated in Figure~\ref{fig:graph_comparison} by the respective directed graph representations of generative and discriminative models. For the same reason, supervised learning in a discriminative model is less precise than in a generative model spanning the same family of posteriors, hence is less effective in small training sets \cite{Ng-01}. 

%Overall, pure discriminative models are of little use outside the context of big labeled data.

% p(x,y|theta) = p(x|y,theta)p(y|theta)

\begin{figure}[!ht]
\begin{center}
\subfigure[Generative model]{\includegraphics[width=.25\textwidth]{generative.pdf}\label{fig:generative}}
\hspace*{.2\textwidth}
\subfigure[Discriminative model]{\includegraphics[width=.25\textwidth]{discriminative.pdf}\label{fig:discriminative}}
\caption{Bayesian networks representing generative and discriminative models, where $X$, $Y$ and $\theta$ respectively denote the variable of interest, the data, and the model parameters. Note the marginal independence of data and parameters in the discriminative model.}
\label{fig:graph_comparison}
\end{center}
\end{figure}

This note advocates the (old) idea of probabilistic opinion pooling \cite{Genest-86} as a possible compromise between generative and discriminative models. Rather than using a purely discriminative model or attempting at a full generative model, we may combine multiple low-dimensional generative models for different pieces of information extracted from the input data. Each such model acts as an isolated ``agent'' that uses a single feature to express an opinion in the form of a likelihood function of the variable of interest. The agent opinions may then be aggregated into a unique predictive probability distribution analogous to a Bayesian posterior. This simple idea may be understood as a probabilistic version of boosting, or as a ``divide and conquer'' approximation to the intractable Bayesian posterior. Importantly, predictions made in such a way could be readily interpretable owing to the use of low-dimensional generative models.

It turns out that, when the aggregation method is chosen as a log-linear pool, the consensus distribution is (up to a multiplicative prior) a quantity known as composite likelihood (CL); see \cite{Varin-11} and the references therein. CL was developed in computational statistics as an extension of the familiar notion of likelihood to situations where a genuine likelihood is intractable. While maximum CL does not inherit the general property of maximum likelihood to yield an asymptotically minimum-variance estimator, it is proven to be consistent under mild conditions \cite{Xu-11} and may offer an excellent trade-off between computational and statistical efficiency in practice.

%the data distribution is not modeled as a whole but separate feature-based generative models are available

%CL is a semi-generative approach to statistical inference that extends the familiar notion of likelihood without requiring a full generative model. The key idea is to model an arbitrary set of low-dimensional features separately and then combine them, instead of modeling the data distribution as a whole. This may be viewed as a ``divide and conquer'' method to approximate the true but intractable likelihood. While maximum CL does not inherit the general property of maximum likelihood to yield an asymptotically minimum-variance estimator, it is consistent under mild conditions \cite{Xu-11} and may offer an excellent trade-off between computational and statistical efficiency in practice. 

%An open question regarding CL is how to choose the weights associated with the respective features. 

With the opinion pooling interpretation in mind, the feature weights in CL may be optimized for prediction performance in a typical discriminative learning scenario. As we will see, this strategy amounts to training a maximum entropy classifier using the feature-based log-likelihoods as basis functions. These likelihood functions themselves may need to be pre-trained, therefore the proposed model typically comes with a double training scheme: a generative training phase (to learn feature-based likelihood parameters) followed by a discriminative training phase (to learn feature weights in aggregation).

Technical details are given in the remainder.

%The relevance of each feature in the CL is determined by an arbitrary weight used to scale the associated likelihood. For classification performance, the weights can be tuned in a typical supervised learning scenario so as to concurrently maximize discriminative likelihood (or, equivalently, minimize cross-entropy), thereby making the method hybrid between generative and discriminative modeling approaches. We shall note that this particular training strategy amounts to building a maximum entropy classifier \cite{BergerA-96} that uses the feature-based log-likelihoods as basis functions.

%{\color{red} Still, note that the likelihood functions themselves are trained beforehand so this is slightly different from a standard maxent classifier aka logistic regression. Double training phase.}

%{\color{red} Classical composite likelihood not studied with nuisance parameters. In our scenario, nuisance parameters are learned beforehand.}

%{\color{red} Interpretability of composite likelihood. Obvious from the construction. Can achieve prediction performance comparable with fully discriminative models while maintaining the interpretability of generative models.}

%We further argue that a particular log-linear opinion pool yields the best possible predictive distribution in the sense of maximum relative conditional entropy. This perspective entails a method to optimize the weights of the different agents from training data as in a typical discriminative learning scenario. 

%Something for true statisticians. Maybe we could clarify that we use the term ``Bayesian'' in the sense of ``empirical Bayesian'', implying that the parameters $\theta$ are to be estimated somehow contrary to a ``full Bayesian'' approach where they would be integrated out at inference time.



\section{Composite likelihood as opinion pooling}
\label{sec:log_pool}

Let $Y$ an observable multivariate random variable with sampling distribution $p(y|x)$ conditional on some unobserved variable of interest, $X\in{\cal X}$, where ${\cal X}$ is a known set. Given an experimental outcome $y$, the likelihood is the sampling distribution evaluated at $y$, seen as a function of $x$:
$$
L(x) = p(y|x)
.
$$

This requires a plausible generative model which, for complex data, may be out of reach or involve too many parameters to be estimated. A natural workaround known as data reduction is to extract some lower-dimensional representation $z=f(y)$, where $f$ is a many-to-one mapping, and consider the potentially more convenient likelihood function:
$$
\ell(x) = p(z|x)
.
$$

Substituting $L(x)$ with $\ell(x)$ boils down to restricting the sample space, thereby  ``delegating'' statistical inference to an ``agent'' provided with partial information. While it is valid for such an agent observing~$z$ only to consider $\ell(x)$ as the likelihood function of the problem, the drawback is that $\ell(x)$ might yield too vague a prediction of~$X$ due to the information loss incurred by data reduction. To make the trick statistically more efficient, we may extract several features, $z_i=f_i(y)$ for $i=1,2,\ldots,n$, and try to combine the likelihood functions $\ell_i(x) = p(z_i|x)$ that they elicit.

If we see the likelihoods as posterior distributions corresponding to uniform priors, this is a classical problem of probabilistic opinion aggregation from possibly redundant sources, for which several methods exist in the literature \cite{Tarantola-82,Genest-86,Garg-04,Allard-12}. One that is appealing as it tends to produce single peaked distributions is the {\em generalized logarithmic opinion pool}:
\begin{equation}
\label{eq:log_pool}
p_\lambda(x|y) = \frac{1}{m_\lambda(y)} \pi(x) \prod_{i=1}^n p(z_i|x)^{\lambda_i},
\end{equation} 
where $\pi(x)$ is some positive-value function which can be interpreted as a global prior, and $\lambda=(\lambda_1,\ldots,\lambda_n)$ is a vector of weights chosen so that the normalizing factor $m_\lambda(y)$ is finite. This condition holds whenever all weights are positive ($\lambda\succeq 0$), assuming that the agent opinions are always upper bounded. Negative weights should be ruled out to warrant a natural monotonicity property: if all agents agree that an outcome $x_1$ is less likely than an outcome $x_2$, then the consensus probabilities should always satisfy $p_\lambda(x_1|y)\leq p_\lambda(x_2|y)$. Note that it is possible to have the weights depend on the data~$y$, but this would make the method considerably more complex and will therefore not be considered here.


%Strictly positive weights guarantee the so-called 0/1 forcing property, that is, if an hypothesis~$x$ has zero likelihood according to at least one agent, then its consensus probability vanishes too.

% Not clear yet as to what a negative weight could mean!


\section{Composite Bayes rule}
\label{sec:bayes_rule}

The log-linear pool~(\ref{eq:log_pool}) bears a striking similarity to Bayes rule, yielding  the form: 
$$
p_\lambda(x|y)\propto \pi(x) L^c_\lambda(x),
$$
where the function:
\begin{equation}
\label{eq:comp_lik}
L^c_\lambda(x) \equiv \prod_{i=1}^n \ell_i (x)^{\lambda_i}
\end{equation} 
plays the same role as a traditional likelihood function. This expression happens to be known as a {\em marginal composite likelihood} \cite{Varin-11}. The slightly more general {\em conditional composite likelihood} form can be derived in the same way as above by conditioning all probabilities on confounding variables, see Appendix~\ref{sec:conditional}. The computational advantage of CL over a genuine likelihood function stems from the fact that it is more efficient to evaluate the marginal distributions of each feature  than the joint distribution of all features.

CL in equation~(\ref{eq:comp_lik}) shares a convenient factorized form with the likelihood derived under the assumption of mutual feature independence, usually referred to as {\em na\"ive Bayes} or {\em simple Bayes} in the machine learning literature, which corresponds to the special case of unitary weights, $\lambda_1=\ldots=\lambda_n= 1$. While our derivation does not assume feature independence, it yields an alternative interpretation of na\"ive Bayes as a general feature aggregation method. However, unitary CL weights do not guarantee optimal prediction performance.


\section{Composite weight tuning}

\subsection{A priori tuning}

%When the features can be considered exchangeable, it is natural to choose constant CL weights. 

In the absence of knowledge, a default rule is to choose constant CL weights. The CL function is then a scaled version of the na\"ive Bayes likelihood, the common weight value being irrelevant to the maximum CL estimator (MCLE). For the purpose of computing reasonable credibility sets, it may be tuned empirically so as to best adjust the pseudo posterior variance matrix to the asymptotic variance matrix of the MCLE \cite{Pauli-11}, or via a close-in-spirit curvature adjustment \cite{Ribatet-12}, in attempts to match frequentist and composite Bayesian notions of uncertainty.

Ignoring such a goal, another sensible prior recommendation for log-linear pooling weights is to sum up to one. This is motivated in \cite{Genest-86b} by the fact that the only pooling operator that preserves external Bayesianity\footnote{External Bayesianity essentially means that it should not matter whether the prior is incorporated before or after pooling, provided that all agents agree on the same prior.} is the log-linear opinion pool with unit sum weights. A further justification given in \cite{Garg-04} is that the log-linear pool with unit sum weights minimizes the average Kullback-Leibler (KL) divergence to the agent opinions. See also \cite{Wang-14}, Theorem~3, for another information theoretic argument.

Unit sum weights, however, implicitly assume strong redundancy between features and may therefore be ineffective for moderately correlated features. An ideal method to tune weights is one that can capture and adjust for existing statistical dependences in the data. This calls for a training approach, which, as we will see, is unlikely to pick constant or unit sum weights.

%automatically adjust to the level of statistical dependence beween features.

%are over-conservative for , which require weights close to one for the CL to closely approximate the true likelihood. Using weights smaller than one, the CL will be flatter than it should and lead to severly overestimated credibility sets.

%correspond to the extreme situation where the features are assumed to be maximally redundant, but is

%{\color{red} Why is it important to fine-tune the weights?}


\subsection{Training}

Brute-force approach: optimize fit for optimal prediction. Discriminative model defined by (\ref{eq:log_pool}). Training objective, weight likelihood (which, in this context, is negated cross-entropy) better than accuracy:
$$
\max_{\lambda\succeq 0} {\cal L}(\lambda),
\qquad {\rm with} \quad
{\cal L}(\lambda)\equiv\sum_k \log p_\lambda(x_k|y_k)
$$

We recognize the dual function $\psi(\lambda)$ of a conditional maxent problem, see Appendix.
$$
{\cal L}(\lambda)
= 
\sum_k \log \pi(x_k)
+ n \psi(\lambda)
$$

Maxent defined by constraints given by mean log-likelihood values. Weaker than assuming feature distributed according to agent models. Can be seen in terms of detection threshold: all we care about is that our feature-based log-likelihoods reach guaranteed levels (on the average) for the true class. Hence inequality constraints are more natural than equality ones. Model is admissible if agent compression rates are achieved or surpassed.

We know more about the model, so this is relaxation. Doesn't make optimization easier but, I speculate, the solution is more stable than if we were imposing feature-based marginals. Both search spaces are determined by the empirically estimated likelihood parameters, intuitively I would say that the bigger space is less sensitive to estimation errors. We can show an example where weights are much more stable by choosing inequality vs equality constraints.


%From a machine learning perspective, it is natural to tune the weights so as to minimize some prediction error, for example conditional cross-entropy. This brings the question: why then do it with ``pre-learned'' likelihood functions rather than with basis functions spanning the same predictive distribution family? Because of potential overfitting. The whole point of this note is that it may be better to run two learning phases than a single fully blown discriminative learning due to overfitting. It is a kind of tradeoff between pure generative and pure discriminative learning. 

%The ``hybrid'' strategy necessarily achieves a larger cross-entropy than the pure discriminative one on the training dataset, but might generalize better. Should show examples of this. 

%The rest of the discussion, i.e. the link with maximum entropy, is less important beside showing that the composite likelihood form is somehow optimal given some knowledge. 

Intuition behind weights: Lagrange multipliers to enforce a constraint... 

Alternative: equality constraints (negative weights), less interpretable.

%\section{Extensions}

\section{Super composite likelihood}
\label{sec:super}

Extension: use mean-values constraints conditional on $x$ instead of jointly averaged over $x$ and $y$? This boils down to picking functions of the form $\chi(x-a)\rho_i(x,y)$ and equate their expectations. For this to work, we need the constraints to involve log-likelihood ratios vs a reference distribution chosen as one of the class distributions. Log-likelihood ratios make even more sense than log-likelihood in the constraints... 

The maxent solution has the form:
$$
p_\lambda(x|y) = \pi(x) \prod_i \left[\frac{p(z_i|x)}{\pi(z_i)}\right]^{\lambda_i(x)}
$$

In other words, the weights become dependent on the variable of interest. This is the super composite likelihood idea in a nutshell.

In 2016, I wrote:
\begin{quote}
When chosen for computational simplicity, clues may not only convey limited information at individual level: their informativeness may also be very much hypothesis-dependent. Consider, for instance, diagnosing a disease from a routine medical checkup. Body temperature may point to a bacterial infection by comparison with normality, but would not help detecting a non-infectious cardiovascular disease -- and conversely for, say, blood pressure.
This motivates a more general setting where clues can be weighted differently depending on hypotheses.
\end{quote}

I was thinking about the unit sum weight version... The diagnosis example could be one in which clues are independent, in which case $\lambda_i(x)\equiv 1$ would be optimal, making any super composite trick irrelevant. Also see that with hypothesis-dependent weights, we may lose the estimation consistency. At the end of the day, the interest of the super composite idea is questionable.



\section{Discussion}
\label{sec:discussion}

{\color{red} Semi-generative method. Good for shallow learning or transfer learning. Still need representation learning methods.}

Number of discriminative parameters in logistic regression: $K(F+1)$. In standard composite inference: $F$. In super-composite inference: $FK$. Number of generative parameters in composite inference using univariate Gaussian models: $2K$ with heteroscedasticity, $K+1$ with homoscedasticity. 

Intuition behind weights: notion of global feature relevance. But does $\lambda_1>\lambda_2$ imply that $z_1$ is more relevant than $z_2$ for a particular instance? No. But we can assess the influence of each scaled likelihood in order to rank features according to their influence on the decision.

CL is a concept from computational statistics that has mainly been developed so far in a frequentist perspective as a surrogate for the maximum likelihood method. We have shown a deep connection between CL and probabilistic inference, thereby establishing CL as a class of discriminative models. Because CL is built from a set of marginal feature-specific generative distributions, it is in essence a two-step semi-generative, semi-discriminative learning approach. In the first, ``generative'' phase, the feature distributions are learned; in the second, ``discriminative'' phase, the feature weights are learned. This strategy can be thought of as a form of non-adaptive boosting.

%The first phase corresponds to the training of ``weak learners''. The second phase amounts to a form of boosting. 

A purely discriminative learning could be used instead but...
Why potentially more efficient than pure discriminative training: because generative training is always more efficient than discriminative training if models are comparable. So the key point is that ``we have less parameters in the discriminant phase". Good in small datasets. But also in asymptotic regime if the features are weakly or highly correlated (???).

Logistic regression / naive Bayes example. Under homoscedasticity (assuming that each feature has class-independent variance), CBI is equivalent to logistic regression -- because the predictive distribution family is the same. This is a case where BCI brings nothing. But consider heteroscedasticity, then BCI yields a quadratic classifier. Compared to a fully discriminative model, the number of parameters to learned in the discriminative phase is reduced by half.

The first training phase is easier if supervised but could be unsupervised too (using mixture models). How do we then deal with label switching issues? Can we safely assume conditional feature independence {\em in the generative training phase}? I believe so provided that the marginal distribution parameters are disjoint. It's obvious in the supervised learning scenario.

If unsupervised, the first learning phase could be compared with contrastive pre-training of RBMs \cite{Hinton-06,Fischer-14}, which also optimize parameters for generation of observable features. BCI is comparable with an RBM with a single output unit (which is just a generative model assuming conditionial feature independence). The key difference is that the RBM is a full generative model while BCI only deals with marginal models, hence relying on weaker assumptions. This won't change anything in the pre-training phase but RBMs have to deal with more parameters in the generative learning phase. In fully supervised context, RBM pre-training is pointless since all parameters learned in the first phase will be overwritten. In BCI, pre-training is crucial even in supervised context because the parameters learned in the discriminative phase (the feature weighs) describe a sub-manifold of the predictive distribution family.

Needs features. Not a representation learning method, but could be coupled why not.


%{\color{red} Moreover, while RBM parameters are typically refined in a supervised discriminative learning step, disjoint set of parameters for SCL. Dunno how to say that.} 

%In such context, SCL competes with classical discriminative models (logistic regression, Gaussian processes \cite{Rasmussen-06}, maximum entropy models \cite{BergerA-96}, etc.), and may compare more or less favorably in practice depending on the amount of training data. For relatively small training datasets, we may hope for more accurate inference using~SCL than using traditional discriminative models, extrapolating from the results of \cite{Ng-01} regarding the comparison between logistic regression and na\"ive Bayes classifiers.
%{\color{red}Ici, il manque la comparaison avec les RBMs qui ne sont pas (forcement) des modeles discriminatifs.}

%%, hence alleviating the need for heavily supervised model training

%In summary, CL has the potential to yield weakly supervised or unsupervised Bayesian-like inference procedures depending on the particular task at hand. This property reflects the encoding of statistical relationships between the data and {\em all} unknown parameters. CL thus appears as a trade-off between generative models, which are optimal for unsupervised learning but possibly intractable, and traditional discriminative models (logistic regression, Gaussian processes \cite{Rasmussen-06}, maximum entropy models \cite{BergerA-96}, etc.), which are inherently supervised. CL models are discriminative models assembled from atomic generative models and, from this point of view, may be considered as {\em semi-generative} models.

%CL may be considered as a {\em semi-generative} model: a discriminative model assembled from partial generative models.

%As a note of caution, we shall stress that the pre-determined weights assigned to the different associations between observed and unobserved values represent prior knowledge regarding the informativeness of clues. A poor choice of weights will inevitably result in a poor approximation to the ``true'' Bayesian posterior -- the posterior that would be obtained from a realistic generative model if it was tractable. In future work, we will investigate feature selection strategies to mitigate this problem.

% Improve the discussion on following aspects:
% ** Why is it compatible with unsupervised learning? Give more insight.
% ** Stress the contribution: class-specific weighting.
% * Pivotality argument.
% * Bayes is a special case of composite Bayes.

{\color{red}Product of fucking experts \cite{Hinton-02}. Log-linear pool to build a generative model (assuming in fact independence between experts). Better seen as a ``log-mixture model''. Sounds like each expert is associated with a class, so it's the same as a RBM. Contrastive learning approximates ML parameter estimation. The experts are learning jointly. It's just a generative model.}

{\color{red}Two-step training: generative phase then discriminative phase. Why is it cool?}

{\color{red}CBI is just a way to reweight Naive Bayes. What's the big deal? Can we really expect massively superior performance? Are we just talking about realistic credibility sets?}


\appendix

\section{Conditional composite likelihood}
\label{sec:conditional}

As a straightforward  extension of marginal CL, each feature-based likelihood may be conditioned by an additional ``independent'' feature $z^c_i = f^c_i(y)$ considered as a predictor of the ``dependent'' feature, $z_i=f_i(y)$, yielding the more general form:
\begin{equation}
\label{eq:cond_feat_lik}
\ell_i(x) = p(z_i|x,z^c_i).
\end{equation}

Conditioning may be useful if it is believed that $z^c_i$ alone is little or not informative about $x$, but can provide relevant information when considered jointly with $x$, as in the case of regression covariates, for instance. Equation~(\ref{eq:comp_lik}) then amounts to conditional CL \cite{Varin-11}, a more general form of CL also including Besag's historical {\em pseudo-likelihood} \cite{Besag-74} developed for image segmentation.

\section{Maxent}

\subsection{Training}
\label{sec:maxent}

{\color{red}This stuff should move to the appendix. It is not critical to understand the Maxent connection of trained composite likelihood, althought it gives some insight. Essentially, agents report compressing rates using feature-based models and we either trust them (equality constraints) or assume that they are underestimated (inequality constraints). Why underestimated? Well, it's just more general. Overestimated leads nowhere, we have to assume something.}

We can derive composite likelihood from the conditional maximum entropy principle \cite{BergerA-96}. It comes with a method of tuning weights, hence a particular composite likelihood function that we call maxent composite likelihood (MCL).

A cheap MaxEnt argument was already given by \cite{Wang-14}  (standard, non-conditional MaxEnt at fixed $y$, hence just an existence result but no feasible weight tuning).

What we do here is maximum {\em conditional} entropy. It's slightly more complex than a simple $I$-projection. The search space is a set of predictive distributions compatible with mean log-likelihood contraints. If I know the generative distribution, I know the expected log-likelihood.

We could work with inequality constraints to force $\lambda\geq 0$:
$$
E[\log p(z_i|x)] \geq \bar{\ell}_i \equiv \int h(x)p(z_i|x) \log p(z_i|x) dx dy
$$
where $h(x)$ is the marginal distribution of labels.

What does the inequality constraint mean? It means that a model is admissible if it jointly allows for all agents to achieve the same compression rate {\em or better} than achieved individually. Somewhow, we forget that there should exist a true distribution that is consistent with the agent constraints both jointly and individually. We forget that not because it is not true but because we deem it irrelevant. Connection to MDL? We are reminded of these words by Rissanen:
\begin{quote}
No statistical model is ``true" or ``false", ``right" or ``wrong"; the models just have varying performance, which can be assessed.  
\end{quote}

Importantly, no agent will be proven wrong since individual performance is at least as good as announced. But if it is strictly better, it means that the agent is simply discarded (getting a zero weight rather than a difficult-to-intepret negative weight).  

We could also consider inequality constraints as a relaxation: explicitly making it easier for Nature to screw us up, but for a good cause. Not to make optimization easier, to make it easier to intepret. Why? Because we know that a posteriori from the fact that we get $\lambda_i\geq 0$. But a priori?

%Okay, but then it's not ``nature'' playing, is it? The game is between a ``generator'' G and a ``predictor'' P. G tries to make it as difficult as possible to predict outcomes using an admissible model. We could argue that true models are at the boundaries of the admissible space, but G is not restricted to true models as long as it maintains the agents performance level.

%In fact, P is the agent coordinator and G could be seen as some sort of test controller. Inequality constraints add some further robustness to the model. 

Why not using the feature-based posteriors or Bayes factors rather than the likelihoods? It's a possibility (perhaps a connection here with Gr\"unwald's luckiness). A justification for not doing it is that the coordinator should discard the priors used by the agents. The constraints are not purely evidence-based since the moments depend on the coordinator's prior but it is arguable a mess of priors wouldn't be good.


The derivation assumed the marginal distribution of the data $h(y)$ to be known. In practice, it is estimated by the empirical distribution just like the moments are estimated.

Game theoretic interpretation \cite{Grunwald-04}.

The dual function boils down to the familiar notions of cross-entropy, log-likelihood, KL divergence...
$$
\psi(\lambda)
= \int h(x,y) \log \frac{p_\lambda(x|y)}{\pi(x)} dx dy
$$

If we introduce $p_\lambda(x,y)=p_\lambda(x|y)h(y)$, it is even clearer that this is a most classical distribution fitting problem:
$$
\psi(\lambda)
= \int h(x,y) \log \frac{p_\lambda(x,y)}{\pi(x)h(y)} dx dy
$$
(we can obviously drop the reference distribution here). So the Maxent justifies the composite likelihood concept as a somewhat optimal parametric form but the remainder of the story is most classical.

Single feature case ($y=z$): the maxent solution has the form $p(x|y)\propto\pi(x)p(y|x)^\lambda$. We expect to have $\lambda=1$ but it's not necessarily the case. It is if $h(y)=\int\pi(x)p(y|x)dx$ because then $h(y)p(x|y)=\pi(x)p(y|x)$ so the constraint is verified (and active). This also tells us that the data marginal should be consistent with the prior on labels for this much expected consistency property to hold. So, either weight datapoints so as to match a desired prior (e.g., uniform) or take the empirical distribution of labels as the ``prior''. 

Which brings another question: in this case, the (unique) weight is insensitive to the prior -- is it true in general? The answer is no. The weights generally depend on the prior. Hence the maxent composite likelihood is prior-dependent. This is an important conceptual difference with the classical notion of likelihood.




\subsection{Data weighting}

The distribution $h(x)$ is the marginal of the labels, which is natural to approximate via their empirical distribution. We may then have a discrepancy between $h(x)$ and $\pi(x)$, which makes little sense if it is believed that $h(x)$ is a ``good encoder''. 

To correct for this, we may adjust the empirical distribution of labels and data so as to match the target prior, $\pi(x)$, leading to weight the data points as a preprocessing step:
$$
h(y) = \sum_i w_i \delta(y-y_i),
\qquad
w_i = \frac{\pi(x_i)}{h(x_i)}
$$

Are the two methods equivalent? No. The feature-based parameter estimates are obviously the same in both cases, but the search space for the predictive distribution is different, hence we can't get the same answer. Which one of the two methods is best? The one that allows you to choose your own prior makes more sense to me because the distribution of classes in your training dataset may be completely different from the one that you will deal with in practice. For instance, it be weird to say that someone with all the symptoms of a very rare disease does not have it because it is very rare. 


\section{Maximum entropie conditionnelle}

$$
D(p) = \int h(y)p(x|y) \log \frac{p(x|y)}{\pi(x)} dxdy
$$

Lagrangian:
$$
{\cal L}(p,\lambda) = D(p) -\lambda^\top (\int h(y)p(x|y)f(x,y)dxdy - F)
$$

Optimal $p$ at fixed $\lambda$:
$$
\frac{\partial{\cal L}}{\partial p}
=
h(y) \log \frac{p(x|y)}{\pi(x)} - h(y) \lambda^\top f(x,y)
= h(y) c(y)
\quad
\Rightarrow
\quad
p_{\lambda}(x|y) = \frac{\pi(x)}{z(\lambda,y)} e^{\lambda^\top f(x,y)}
$$
with:
$$
z(\lambda,y) = \int \pi(x) e^{\lambda^\top f(x,y)} dx
$$

Dual:
$$
\psi(\lambda) = \lambda^\top F - \int h(y) \log z(\lambda, y) dy 
$$

$$
\nabla z
=
\int \pi(x) f(x,y)  e^{\lambda^\top f(x,y)} dx
$$

$$
\nabla \nabla^\top z 
=
\int \pi(x) f(x,y) f(x,y)^\top e^{\lambda^\top f(x,y)} dx
$$

$$
\nabla\psi
= F - \int h(y) \frac{\nabla z}{z} dy
$$

$$
\nabla\nabla^\top\psi
= - \int h(y) \left(
\frac{\nabla \nabla^\top z}{z} 
-
\frac{\nabla z \nabla z^\top}{z^2}
\right)
dy
$$

Note the equivalent expression of the dual function as:
$$
\psi(\lambda)
=
\int h(x,y) \log \frac{p_\lambda(x|y)}{\pi(x)} dx dy
= D(h\|\pi) - D(h\|p_\lambda)
$$


\section{Maxent via the generalized KL}

It's a bit different as the unit integral constraint is made implicit.

Assume $\int\pi(x)dx=1$ and $\int h(y) dy = 1$. 
$$
D(p) 
= \int h(y)p(x|y) \log \frac{p(x|y)}{\pi(x)} dxdy
- \int h(y)p(x|y)dx dy
+ 1
$$

Goal is to minimize $D(p)$ s.t. mean-value constraints on $p$. 

Lagrangian:
$$
{\cal L}(p,\lambda) = D(p) -\lambda^\top (\int h(y)p(x|y)f(x,y)dxdy - F)
$$

Optimal $p$ at fixed $\lambda$:
$$
\frac{\partial{\cal L}}{\partial p}
=
h(y) \log \frac{p(x|y)}{\pi(x)} - h(y) \lambda^\top f(x,y)
= 0
\quad
\Rightarrow
\quad
p_{\lambda}(x|y) = \pi(x) e^{\lambda^\top f(x,y)}
$$

Dual:
$$
\psi(\lambda) = \lambda^\top F - \int h(y)z(\lambda, y) dy + 1
$$
with:
$$
z(\lambda,y) = \int \pi(x) e^{\lambda^\top f(x,y)} dx
$$

$$
\nabla\psi(\lambda)
= F - \int \pi(x) h(y) f(x,y) e^{\lambda^\top f(x,y)} dx dy
$$

$$
\nabla\nabla^\top\psi(\lambda)
= - \int \pi(x) h(y) f(x,y)f(x,y)^\top e^{\lambda^\top f(x,y)} dx dy
$$


\section{Minimally discriminative model}

Let $\pi(x)$ some reference distribution that represents full
uncertainty about $X$. We wish to select the joint distribution
$p(x,y)$ that minimizes:
$$
I(p) = \int p(x,y) \log \frac{p(x|y)}{\pi(x)} dy,
$$ subject to feature mean-value constraints of the form:
$$
\int p(x,y) f(x,y) dx dy = \mu.
$$

There are two special cases of this problem in the literature.  On the
one hand, the {\em maximum entropy classifier} \cite{BergerA-96}
incorporates the constraint that $p(y)$ be known, however we will see
that this is not necessary. The {\em minimally informative likelihood}
method \cite{Yuan-99b,Yuan-99}, on the other hand, imposes that
$p(x)=\pi(x)$, hence assuming the form $p(x,y)=\pi(x)p(y|x)$. We won't
make such assumptions here and will consider the case where $\mu$ is
completely unknown.

The above problem is seen to be equivalent to minimizing the auxiliary
objective function:
$$
I(p,m) 
= \int p(x,y) \log \frac{p(x,y)}{\pi(x)m(y)} dxdy,
$$ over $p$ and $m$, subject to the same constraints on $p$. We note
that $I(p,m)=I(p)+D(p_y\|m)$, showing that the auxiliary function
essentially adds a penalty term to the actual objective in order to
force $p(y)$ close to $m(y)$.

Minimizing $I(p,m)$ along both $p$ and $m$ is basically a minimum KL
divergence problem between two convex distribution spaces, so there
must be a unique solution {\bf -- to be checked in
  \cite{Cover-91}}. It is similar to the rate-distortion problem in
information theory, which may be solved by alternate minimization,
yielding a Blahut-Arimoto algorithm.
\begin{itemize}
\item {\em Let's call it A-step}. Optimize $p(x,y)$ at fixed $m$
  s.t. constraint:
$$
\exists \lambda, \qquad
p_{\lambda,m}(x,y) = \frac{1}{Z(\lambda, m)} \pi(x) m(y) e^{-\lambda^\top t(x,y)}
$$
The actual $\lambda$ is found my maximizing the dual function
$\psi(\lambda,m)$, see below.
\item {\em Let's call it B-step}. Optimize $m(y)$ at fixed $p(x,y)$:
$$
m(y) = \int p(x,y) dx
$$
\end{itemize}

Upon convergence, the algorithm outputs both joint and marginal
distributions $p_{\star}(x,y)$ and $m_{\star}(y)$ that both depend on
$\mu$. By construction, we have that $p_{\star}(y) = m_{\star}(y)$,
therefore $p_\star(x|y)=p_{\star}(x,y)/m_{\star}(y)$.


\section{Comparison with maximum entropy classifier}

Lagrangian...
$$
{\cal L}(p,m,\lambda)
= 
\int p(x,y) \log \frac{p(x,y)}{\pi(x)m(y)} dxdy
+
\lambda^\top \left( 
\int p(x,y) f(x,y) dydy - \mu 
\right)
$$ where $\lambda$ represents a vector-valued Lagrange multiplier. At
fixed $m$, the solution has the form:
$$
p_{\lambda,m}(x,y) = \frac{1}{Z(\lambda,m)}
\pi(x) m(y) e^{-\lambda^\top f(x,y)} 
$$

Note that this implies that the conditional distribution is
independent of $m$ once $\lambda$ is determined,
$$
p_\lambda(x|y) = \frac{1}{z(\lambda,y)} \pi(x) e^{-\lambda^\top f(x,y)} 
$$

Also, the marginal is a modulation of $m(y)$:
$$
p_{\lambda,m}(y) = \frac{z(\lambda,y)}{Z(\lambda,m)} m(y)
$$

Dual function at fixed $m$:
$$
\psi(\lambda,m) 
\equiv \min_p {\cal L}(p,m,\lambda)
= 
- \log Z(\lambda,m) - \lambda^\top \mu
.
$$ 

An alternative expression is:
$$
\psi(\lambda, m)
= 
\int h(x,y) 
\log \frac{p_{\lambda,m}(x,y)}{\pi(x)m(y)} dxdy,
$$ where $h(x,y)$ is any distribution statisfying the
constraints. This shows that maximizing the dual function at fixed $m$
is essentially the same as minimizing the KL divergence
$D(h\|p_{\lambda,m})$ over $\lambda$, in other words fitting $h(x,y)$
by some distribution of the form $p_{\lambda,m}$. The fact that the
result does not depend on the particular $h(x,y)$ that is chosen, as
long as it satifies the constraint, is a general property of
exponential families.

We also have that the dual function associated with $I(p)$ reads:
\begin{eqnarray*}
\psi(\lambda) 
 & = & \min_m \psi(\lambda, m)\\
 & = & \int h(x,y) \log \frac{p_{\lambda}(x|y)}{\pi(x)} dxdy\\
 & = & -\int h(y) \log z(\lambda,y) dy - \lambda^\top \mu
\end{eqnarray*}

But, wait, that's exactly what we get in the maximum entropy
classifier! So, at the end of the day, we simply got an alternative
method to learn $\lambda$ in the maximum entropy classifier, i.e. the
Blahut-Arimoto algorithm as opposed to a brute-force maximization of
$\psi(\lambda)$. Both methods will converge to the same $\lambda$...

What it essentially means is that the optimal $\lambda$ is insensitive
to $m(y)$ as long as $m(y)$ is compliant in the sense that:
$$
m(y) = \int h(x,y) dx,
$$ for some distribution $h(x,y)$ statisfying the constraint. This is
true for the optimal $m_\star(y)$ output by the BA algorithm, but
also, for instance, for the empirical distribution of observations in
a training dataset used to estimate $\mu$, as proposed in
\cite{BergerA-96}. The only special property of $m_\star(y)$ is to
yield the full Bayesian model $p_\star(x,y)=p_\star(x|y)m_\star(y)$
that minimizes the discrimination information. We may not care too
much about that in practice since we will only use $p_\star(x|y)$. 

\section{Comparison with minimally informative likelihood}

Yuan \cite{Yuan-99} considered the situation where we add the
constraint that $p(x)=\pi(x)$ to the minimum discrimination inference
problem. Unknown is then the conditional distribution
$p(y|x)$. Lagrangian...
\begin{eqnarray*}
{\cal L}(p,m,\lambda)
 & = & 
\int \pi(x)p(y|x) \log \frac{p(y|x)}{m(y)} dxdy
+
\lambda^\top \left( 
\int \pi(x)p(y|x) f(x,y) dydy - \mu 
\right) \\
 & = & 
\int \pi(x)
\left( 
\int
p(y|x) \log \frac{p(y|x)}{m(y)} dy
+
\lambda^\top 
\int p(y|x) f(x,y) dy
- \mu 
\right)
dx 
\end{eqnarray*}

The derivative is given by,
$$
\frac{\partial\cal L}{\partial p(y|x)}
= 
\pi(x)\left[ 
1 + \log \frac{p(y|x)}{m(y)} 
+ \lambda^\top f(x,y)
\right],
$$
hence the optimal model at fixed $m$ has the form:
$$
p_{\lambda,m}(y|x) = \frac{1}{Z(\lambda,m,x)} m(y) e^{-\lambda^\top f(x,y)} 
$$

Note that the induced posterior distribution $p_{\lambda,m}(x|y)$ has
a different form from above unless the normalizing factor
$Z(\lambda,m,x)$ turns out independent from $x$. If this is not the
case, we no longer have the property that $p_{\lambda,m}(x|y)$ is
independent from $m$ given $\lambda$.

Dual function...
$$
\psi(\lambda,m) 
=
- \int \pi(x) \log Z(\lambda, m, x) dx
- \lambda^\top \mu
$$

Equivalently, for any distribution under the form $\pi(x)h(y|x)$ that
satisfies the moment constraint, we have:
$$
\psi(\lambda,m) 
=
\int \pi(x) h(y|x) \log \frac{p_{\lambda,m}(y|x)}{m(y)} dxdy
$$

Now, the real question is why should we constrain $p(x)$, which boils
down to a Bayesian prior in this context, to be the same as our
reference $\pi(x)$? It only makes sense if we want our inference to
stick to a generative modeling paradigm... but haven't we already
given up on that? Therefore, unless we find a good reason not to, we
won't impose the $p(x)=\pi(x)$ constraint, thereby allowing for a
discrepancy between the reference and the prior.


\section{Discriminative vs. semi-discriminative}

Let's go back to the equivalence we found between our approach and the
maximum entropy classifier (MCE) \cite{BergerA-96}. We have said that
the former is essentially a re-formulation of MCE.

But the re-formulation also conveys a generalization of MCE if,
instead of letting $m$ being an arbitrary distribution, we restrict
its search space to some set of acceptable reference
distributions. Would such a strategy be useful? 

Recall that the method selects the joint distribution that minimizes
discriminative information, as defined by $I(p)$:
$$
I(p) 
= \int p(x,y)\log\frac{p(x|y)}{\pi(x)} dxdy
= E_Y[D(p_{x|y}\|\pi)]
$$

The latter characterization reminds us that discriminative information
is defined in an average sense. The corresponding posterior $p(x|y)$
may not be conservative for some $y$, in particular those that are
unlikely under $p(y)$. It would be a problem if that's the case for
the particular data at hand. For that to happen rarely, the ideal
$p(y)$ would be the ``true'' marginal distribution of $Y$.

However, for vague mean value constraints, nothing may prevent the
optimal $m_\star(y)$ from departing significantly from that ideal
distribution. In fact, we can show that $m_\star(y)$ only has mass at
$y$ values that maximize $z(\lambda_\star,y)$, and is therefore likely
sparse. This is because $m_\star$ minimizes $\psi(\lambda_\star,m)$,
which is equivalent to maximizing $Z(\lambda_\star,m)$ and we have:
$$
Z(\lambda, m) = \int m(y) z(\lambda, y) dy.
$$

In other words, the solution to our problem may be singular! It does
not hurt since, as discussed above, one may alternatively fix $m(y)$
to some pre-defined distribution... but this, in practice, restricts
the method to supervised learning situations.

We may not face this issue with the MIL method, which further imposes
the prior on $X$, so all the information from the constraints goes
into specifying a possibly reasonable generative model.


\section{Data reduction inequality}
\label{sec:reduction_inequality}

A fundamental property of the Kullback-Leibler divergence is that it decreases under feature extraction, {\em i.e.}, application of a deterministic transformation. This comes as a consequence of the logarithmic sum inequality \cite{Cover-06} (and also follows as a straightforward corollary of the PDF~projection theorem \cite{Minka-04,Baggenstoss-15}). 

The proof is elementary and can be sketched as follows. Let $Z=f(Y)$ some feature extracted from a variable~$Y\sim p(y)$. Given an arbitrary distribution~$\pi(y)$, let  $\tilde{p}(z)$ and $\tilde{\pi}(z)$ the distributions induced on~$Z$ by $p(y)$ and $\pi(y)$, respectively. For any potential value~$z$ of~$Z$, consider the level set: $\Gamma(z)=\{y, f(y)=z\}$. Under conditions of existence, we can partition the integral involved in the KL~divergence $D(p\|\pi)$ using these level sets: 
$$
\int p(y) \log \frac{p(y)}{\pi(y)} dy
=
\int \left( \int_{\Gamma(z)} p(y) \log \frac{p(y)}{\pi(y)} dy \right) dz\\
.
$$

Applying the logarithmic sum inequality \cite{Cover-06} to each integral over~$\Gamma(z)$, we then readily get:
$$
\int_{\Gamma(z)} p(y) \log \frac{p(y)}{\pi(y)} dy
\geq 
\tilde{p}(z) \log \frac{\tilde{p}(z)}{\tilde{\pi}(z)}
,
$$
owing to the fact that $\displaystyle \int_{\Gamma(z)} p(y) dy = \tilde{p}(z)$ by definition. Therefore,
$$
\int \tilde{p}(z) \log \frac{\tilde{p}(z)}{\tilde{\pi}(z)} dz
\leq 
\int p(y) \log \frac{p(y)}{\pi(y)} dy
,
$$
or, in a more compact way, $D(\tilde{p}\|\tilde{\pi})\leq D(p\|\pi)$. The case of equality occurs if, and only if, $p(y)/\pi(y)=\tilde{p}(z)/\tilde{\pi}(z)$, in other words, if~$z$ is a sufficient statistic for the ``alternative'' hypothesis~$H_1:y\sim p(y)$ versus the ``null'' hypothesis~$H_0:y\sim\pi(y)$. 

Since it is a well-known fact that $D(\tilde{p}\|\tilde{\pi})\geq 0$ as any KL~divergence, we conclude that:
\begin{equation}
\label{eq:reduction_inequality}
0 
\leq D(\tilde{p}\|\tilde{\pi}) 
\leq D(p\|\pi)
.
\end{equation}


\section{Basic asymptotic properties of composite likelihood}
\label{sec:asymptotic}

It follows from the double inequality~(\ref{eq:reduction_inequality}) that, for any extracted feature~$z_i$ and for any hypothesis~$\theta$,
$$
0 \leq
E\left[
\log \frac{p(z_i|\theta_\star)}{p(z_i|\theta)}
\right]
\leq
E\left[
\log \frac{p(y|\theta_\star)}{p(y|\theta)}
\right],
$$
where the expectation is taken with respect to the true distribution $p(y|\theta_\star)$. Using a weighted sum of such inequalities, we can bracket the expected variations of the logarithm of any standard composite likelihood function (assuming unit sum positive weights):
\begin{equation}
\label{eq:variation_bound}
0 \leq
E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta, \mathbf{w})} \right]
\leq 
E\left[ \log \frac{L(\theta_\star)}{L(\theta)} \right]
.
\end{equation}

This implies two asymptotic properties of standard composite likelihood: 
\begin{itemize}
\item {\em Consistency.} The expected log-composite likelihood is maximized by~$\theta_\star$, the true value of~$\theta$.
\item {\em Conservativeness.} The expected log-composite likelihood function is ``flatter'' than the true expected log-likelihood. In other words, composite likelihood ratios $L_c(\theta, \mathbf{w})/L_c(\theta_\star, \mathbf{w})$ relative to~$\theta_\star$ tend to be larger than the corresponding true likelihood ratios. 
\end{itemize}

These properties do not necessarily extend to the more general case of SCL. We may state a weaker consistency property by considering the upper envelope of expected log-composite likelihood ratios:
$$
M(\theta) = \max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right],
$$
where ${\cal W}$ is the $n$-dimensional simplex. Clearly, the expected logarithm of the SCL is upper bounded by~$M(\theta)$:
$$
E[\log {\cal L}_c(\theta, \mathbf{W})] \leq M(\theta)
.
$$

Moreover, $M(\theta)$ is maximized by $\theta_\star$ since (\ref{eq:variation_bound}) implies that, for any $(\theta,\theta_0)$,
$$
E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
\leq
E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right],
$$
which remains true when taking the maximum over the weights in both sides: 
$$
M(\theta) = 
\max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
\leq
\max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
= M(\theta_\star)
.
$$

Therefore, maximizing SCL corresponds to maximizing a lower bound on~$M(\theta)$, which can be considered as an objective function since it is maximized by~$\theta_\star$ (like the expected log-likelihood). Lower bound maximization guarantees a certain objective value, however not the maximum, hence asymptotic consistency may not hold.


\bibliographystyle{abbrv}
\bibliography{cvis,stat,alexis}

%%\input{draft.biblio}

\end{document}

\documentclass[english]{scrartcl}

\usepackage{fullpage}
\usepackage{amstext,amssymb,amsmath,amsthm}
\usepackage{graphicx,subfigure}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{color}

\graphicspath{{.}{pics/}}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{proposition}[theorem]{Proposition}


\title{Composite Bayesian inference}
\date{}
\author{Alexis Roche\thanks{\url{alexis.roche@centraliens.net}}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\lda}{{\boldsymbol{\lambda}}}
%\newcommand{\matphi}{\boldsymbol{\Phi}} 
%\def\x{{\mathbf{x}}}
%\def\u{{\mathbf{u}}}
%\def\p{{\bar{\mathbf{p}}}}
%\def\q{{\bar{\mathbf{q}}}}



\begin{document}

\maketitle

\begin{abstract}
We revisit the concept of composite likelihood as a method to make a probabilistic inference by aggregation of multiple Bayesian agents. This view makes it natural to use a machine learning approach to calibrate the weights associated with composite likelihood. For instance, they can be tuned in a typical supervised learning stage so as to minimize cross-entropy on a labeled training dataset, yielding a convex problem. We argue that composite Bayesian inference is a middle way between generative and discriminative modeling approaches that trades off between interpretability and prediction performance, both of which are crucial to many artificial intelligence tasks.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Textbook statistical inference (frequentist or Bayesian) rests upon the existence of a probabilistic data-generating model that is both empirically valid and computationally tractable. Because this double requirement may be very challenging for multi-dimensional data, other inference models have been developed in applied science: deliberately misspecified generative models, as in quasi-likelihood \cite{White-82,Walker-13} or na\"ive Bayes \cite{Ng-01} methods; data compression models as in the minimum description length paradigm \cite{Grunwald-07}; and discriminative models\footnote{A {\em discriminative} model is a parametric family that describes the distribution of a target variable conditional on the data, in contrast with a {\em generative} model, which describes the conditional distribution of the data given the target variable.}, which currently dominate the field of artificial intelligence (AI) and typically require supervised learning on large datasets -- these include ``shallow'' learning \cite{Ho-95,BergerA-96,Vapnik-00,Rasmussen-06} and deep learning \cite{Lecun-15,Goodfellow-16} techniques (with the exception of deep belief networks \cite{Hinton-06}).

Discriminative models, however, lack the ability to establish causality links between data instances and predictions, and therefore cannnot ``support'' their own predictions in a built-in manner. Consider as a straightforward example the problem of deciding whether a pan is a sauce pan or a frying pan based on its depth. A discriminative model can learn an optimal threshold that correctly classifies most pans in practice, but it cannot gain an internal representation that sauce pans tend to be deeper than frying pans, which is {\em why} the thresholding works. To interpret a discriminative model means to ``reverse engineer'' it using external knowledge, which quickly becomes an impossible task as the data dimension increases.

There is growing awareness that AI should be interpretable in life-impacting applications~\cite{Molnar-18}, where it is (and perhaps should remain) confined to a supportive role in human-driven decision making processes. For instance, in medical diagnosis, automated disease predictions should always be corroborated by individualized findings to be communicable and thus be taken into consideration by clinicians -- in other words, there is no interest for ``black boxes'' but for AI systems that can justify their opinions just like human experts. Clinicians have long used reference range analysis for diagnosis, which stems from simple univariate generative models.

%, {\em e.g.}, by pinpointing biomarkers that deviate from healthy reference ranges.

% simple methods based on range analysis of univariate biomarkers can provide meaningful clues, and are thus more useful in practice although less powerful in terms of prediction performance.

%With generative models you can compare data with expected outcomes to get insight. 

%the information they provide to support them is limited to the boundaries between classes, which are difficult to interpret for high-dimensional data representations. 

%Decision making is not about pooling opinions, it is about pooling empirical observations and find which hypothesis they point to.

%Need to emphasize why we need generative models. Below are some valid reasons: unsupervised learning, potentially better performance on small training datasets... But the \#1 reason is that discriminative models are black boxes: they provide an answer but no explanation to support it. Truth is, they perform better in supervised big data context in the sense of accuracy, cross-entropy, and so on, no question about that -- but they are mere inference recipes. Is that what AI should be? Sometimes, yes. But there are application fields where this clearly cannot work, for instance medical diagnosis. 

%Imagine a doctor telling a patient: ``Sorry, sir, you have Alzheimer's disease because this fancy AI software said so''. Unless the said software is 100\% reliable, which is practically impossible, this would obviously be unacceptable. Acceptable would be to corroborate the diagnosis with some key quantitative empirical observations: low memory test scores, hippocampal atrophy, etc. In other words, inference has to come with some insight. An AI system is but one expert relying on hypotheses which may be invalid, as any expert, and is therefore prone to error. A typical error source is to train a classifier on a subset of classes from the real world. We cannot avoid inference errors but we can safeguard against them by being able to interrogate the system: why do you think what you think? To achieve that, the system should be capable of {\em introspection.

%Vapnik: ``one should solve the classification problem directly and never solve a more general problem as an intermediate step''. True if the goal is classification only. Wrong if introspection is needed because the problem is then more general in nature.

%Human inference does not work in a pre-determined universe of ``causes''. We confront theories with reality to evaluate how likely they are, while knowing that none of the theories considered so far may be ``right''. In fact, we are not looking for the truth, but for a conving enough theory. 

%Composite likelihood seen as a pool of experts comes with a potential introspection mechanism: first sort experts by decreasing influence on the predictive distribution, see how likely the different classes are to the most influential experts.

Another limitation of discriminative models is that they are not suitable for unsupervised learning or on-the-fly parameter estimation because they treat the data and the model parameters as {\em marginally independent} variables, meaning that the data conveys no information about the parameters unless the target variable is observed. This is illustrated in Figure~\ref{fig:graph_comparison} by the respective directed graphs representing generative and discriminative models. For the same basic reason, supervised learning in a discriminative model is statistically less efficient than in a generative model spanning the same family of posteriors, hence requires a larger training set to achieve optimal performance \cite{Ng-01}. 

%Overall, pure discriminative models are of little use outside the context of big labeled data.

% p(x,y|theta) = p(x|y,theta)p(y|theta)

\begin{figure}[!ht]
\begin{center}
\subfigure[Generative model]{\includegraphics[width=.25\textwidth]{generative.pdf}\label{fig:generative}}
\hspace*{.2\textwidth}
\subfigure[Discriminative model]{\includegraphics[width=.25\textwidth]{discriminative.pdf}\label{fig:discriminative}}
\caption{Directed graphs representing generative and discriminative models, where $X$, $Y$ and $\theta$ respectively denote the target variable, the data, and the model parameters. Note the marginal independence of data and parameters in the discriminative model.}
\label{fig:graph_comparison}
\end{center}
\end{figure}

This note advocates the (old) idea of probabilistic opinion pooling \cite{Genest-86} as a means to incorporate generative information into a discriminative model. Rather than attempting at a full generative model, one may combine multiple low-dimensional generative models for different pieces of information extracted from the input data. Each ``small'' model acts as an isolated ``agent'' that uses a single feature to express an opinion in the form of a likelihood function of the target variable. The agent opinions may then be aggregated into a unique predictive probability distribution analogous to a Bayesian posterior. This idea may be understood as a probabilistic version of boosting, or as a ``divide and conquer'' approximation to the intractable Bayesian posterior. Importantly, predictions made in such a way are interpretable owing to their underlying generative nature.

When choosing the aggregation method as a log-linear pool, the predictive distribution turns out to be proportional to a quantity known as composite likelihood (CL) in computational statistics. As detailed in \cite{Varin-11} and the references therein, CL was developed as a surrogate of traditional likelihood for parameter estimation. While maximum CL does not inherit the general property of maximum likelihood to achieve asymptotical minimum variance, it is asymptotically consistent under mild conditions \cite{Xu-11} and generally offers an excellent trade-off between computational and statistical efficiency.

With the opinion pooling interpretation in mind, the feature weights in CL may be optimized for prediction performance in a typical discriminative learning scenario. As we will see, this strategy amounts to training a maximum entropy classifier using the feature-based log-likelihoods as basis functions. These likelihood functions themselves may need to be pre-trained, therefore the composite inference approach typically invovles a double training scheme: a generative training stage (to learn feature-based likelihood parameters) followed by a discriminative training stage (to learn feature weights in aggregation).

Technical details are given in the remainder.



\section{Composite likelihood as opinion pooling}
\label{sec:log_pool}

Let $Y$ an observable multivariate random variable with sampling distribution $p(y|x)$ conditional on some unobserved variable of interest, $X\in{\cal X}$, where ${\cal X}$ is assumed to be a finite set for simplicity. Given an experimental outcome $y$, the likelihood is the sampling distribution evaluated at $y$, seen as a function of $x$:
$$
L(x) = p(y|x)
.
$$

This requires a plausible generative model which, for complex data, may be out of reach or involve too many parameters to be estimated. A natural workaround known as data reduction is to extract some lower-dimensional representation $z=f(y)$, where $f$ is a many-to-one mapping, and consider the potentially more convenient likelihood function:
$$
\ell(x) = p(z|x)
.
$$

Substituting $L(x)$ with $\ell(x)$ boils down to restricting the feature space, thereby  ``delegating'' statistical inference to an ``agent'' provided with partial information. While it is valid for such an agent observing~$z$ only to consider $\ell(x)$ as the likelihood function of the problem, the drawback is that $\ell(x)$ might yield too vague a prediction of~$X$ due to the information loss incurred by data reduction. To make the trick statistically more efficient, we may extract several features, $z_i=f_i(y)$ for $i=1,2,\ldots,n$, and try to combine the likelihood functions $\ell_i(x) = p(z_i|x)$ that they elicit.

If we see the likelihoods as posterior distributions corresponding to uniform priors, this is a classical problem of probabilistic opinion aggregation from possibly redundant sources, for which several methods exist in the literature \cite{Tarantola-82,Genest-86,Garg-04,Allard-12}. One that is appealing as it tends to produce single peaked distributions is the {\em generalized logarithmic opinion pool}:
\begin{equation}
\label{eq:log_pool}
p_\lda(x|y) \propto \pi(x) \prod_{i=1}^n p(z_i|x)^{\lambda_i},
\end{equation} 
where $\pi(x)$ is a prior distribution, and $\lda=(\lambda_1,\ldots,\lambda_n)$ is a vector of weights. Negative weights should be ruled out to warrant a natural monotonicity property: if all agents agree that an outcome $x_1$ is less likely than an outcome $x_2$, then the consensus probabilities should always satisfy $p_\lda(x_1|y)\leq p_\lda(x_2|y)$. Also note that it is possible to have the weights depend on the data~$y$, but this would make the method considerably more complex and less interpretable. 


%This condition holds whenever all weights are positive ($\lambda\succeq 0$), assuming that the agent opinions are upper bounded. Negative weights should be ruled out to warrant a natural monotonicity property: if all agents agree that an outcome $x_1$ is less likely than an outcome $x_2$, then the consensus probabilities should always satisfy $p_\lambda(x_1|y)\leq p_\lambda(x_2|y)$. Note that it is possible to have the weights depend on the data~$y$, but this would make the method considerably more complex and less interpretable. 


%Strictly positive weights guarantee the so-called 0/1 forcing property, that is, if an hypothesis~$x$ has zero likelihood according to at least one agent, then its consensus probability vanishes too.

% Not clear yet as to what a negative weight could mean!


\section{Composite Bayes rule}
\label{sec:bayes_rule}

The log-linear pool~(\ref{eq:log_pool}) bears a striking similarity to Bayes rule, yielding  the form: 
$$
p_\lda(x|y)\propto \pi(x) L^c_\lda(x),
$$
where the distribution $\pi(x)$ plays the role of a prior, and the function:
\begin{equation}
\label{eq:comp_lik}
L^c_\lda(x) \equiv \prod_{i=1}^n \ell_i (x)^{\lambda_i}
\end{equation} 
plays that of a likelihood function. The expression~(\ref{eq:comp_lik}) happens to be known in computational statistics as a {\em marginal composite likelihood} \cite{Varin-11}. The slightly more general {\em conditional composite likelihood} form can be derived in the same way as above by conditioning all probabilities on confounding variables, see Appendix~\ref{appendix:conditional}. The computational advantage of CL over a genuine likelihood function stems from the fact that it is more efficient to evaluate the marginal distributions of each feature than the joint distribution of all features.

CL shares a convenient factorized form with the likelihood derived under the assumption of mutual feature independence, usually referred to as {\em na\"ive Bayes} or {\em simple Bayes} in the machine learning literature, which corresponds to the special case of unitary weights, $\lda\equiv 1$. While the above derivation does not assume feature independence, it yields an alternative interpretation of na\"ive Bayes as a general feature aggregation method. However, unitary CL weights do not guarantee optimal prediction performance.


\section{Prediction interpretation}

A compelling advantage of the composite approach is to make it easy to interpret a prediction for any instance~$y$. Given two putative target values $x_0$ and $x_1$, the log-probability ratio of $x_1$ vs~$x_0$ is computed by simply adding up single-feature contributions: 
$$
\log \frac{p_\lda(x_1|y)}{p_\lda(x_0|y)}
= 
\underbrace{\log \frac{\pi(x_1)}{\pi(x_0)}}_{\text{offset}}
+ \sum_i \underbrace{\lambda_i \log \frac{p(z_i|x_1)}{p(z_i|x_0)}}_{\text{feature contribution}},
$$
therefore it is straightforward to assess which features contribute most to a particular hypothesis comparison (either positively or negatively), see possible disagreements between features, and corroborate predictive probabilities with feature-level testable information. 

Each feature contributes to an hypothesis comparison proportionally to its associated log-likelihood ratio test statistic. Contrary to statistical hypothesis testing, likelihood ratios are not converted into $p$-values but are directly combined into a predictive probability ratio. Non-unitary composite weights reinforce or inhibit features compared to na\"ive Bayes prediction ($\lda\equiv 1$), and may therefore account for mutual feature dependence if tuned appropriately. Keeping the analogy with hypothesis testing, composite weights may be thought of as the counterpart of multiple testing correction \cite{Benjamini-10}.

To interpret the predictive distribution globally, the user(s) may go through successive pairwise hypothesis comparisons in a dynamic way, starting by comparing the automated prediction, {\em i.e.} the most probable target, to some educated guess or reference class (for instance, normality in medical diagnosis). Next, depending on the feature contributions highlighted by the comparison, they may submit another hypothesis for comparison to the automated prediction. This process may be repeated until the composite prediction is subjectively understood by the user(s), yielding a form of human machine dialog.



\section{Composite weight tuning}
\label{sec:weight_tuning}

%So far, we have assumed that the composite weights~$\lda=(\lambda_1,\ldots,\lambda_n)$ in~(\ref{eq:log_pool}) are known. 

Let us now turn to the problem of tuning the composite weights~$\lda=(\lambda_1,\ldots,\lambda_n)$ in~(\ref{eq:log_pool}) as part of the method calibration. It is assumed at this stage that the feature generative models are known, which may require a pre-training stage that should not cause technical difficulties as it may use standard parameter estimation techniques. 


\subsection{Agnostic calibration}

In the absence of knowledge, a default rule is to choose constant weights. The CL function is then a scaled version of the na\"ive Bayes likelihood, the common weight value being irrelevant to the maximum CL estimator (MCLE). To get meaningful predictive probabilities, it may be tuned empirically so as to best adjust the pseudo posterior variance matrix to the asymptotic MCLE variance matrix \cite{Pauli-11}, or via a close-in-spirit curvature adjustment \cite{Ribatet-12}, in attempts to match frequentist and composite Bayesian notions of uncertainty.

Another simpler recommendation for log-linear pooling weights is to sum up to one. This may be motivated by the fact that the only pooling operator that preserves {\em external Bayesianity} is the log-linear opinion pool with unit sum weights \cite{Genest-86b}. External Bayesianity essentially means that it should not matter whether the prior is incorporated before or after pooling agent opinions, provided that all agents agree on the same prior. A further justification given in \cite{Garg-04} is that the log-linear pool with unit sum weights minimizes the sum of Kullback-Leibler (KL) divergences to the agent opinions (see also \cite{Wang-14}, Theorem~3, for a related optimality argument).

Unit sum weights, however, implicitly assume strong redundancy between features and thus tend to produce over-conservative predictions when features are weakly correlated, as is generally the case in practice since features are supposed to be chosen to be complementary. An ideal method to tune weights is one that can capture and adjust for the pattern of statistical dependences between features. This calls for a training approach, which, as we will see, is unlikely to pick constant or unit sum weights.


\subsection{Training}

Given a labeled training dataset ${\cal D}=\{(x_k, y_k), k=1,\ldots,N\}$, the most direct approach to tune the composite weights is to maximize their likelihood (or, equivalently, minimize their ``cross-entropy'') under the composite predictive distribution:
\begin{equation}
\label{eq:train_likelihood}
\max_{\lda\succeq 0} U(\lda)
\equiv\sum_{k=1}^N \log p_\lda(x_k|y_k).
\end{equation}

From~(\ref{eq:log_pool}), we see that the set of conditional distributions $p_\lda(x|y)$ spanned by the weights~$\lda$ is the exponential family with natural parameter~$\lda$ and basis functions given by the feature-based log-likelihoods:
$$
p_\lda(x|y) = \pi(x) \exp[\lda^\top \mathbf{L}(x,y) - A(\lda,y)],
$$
with:
$$
L_i(x,y) \equiv \log p(z_i|x),
\qquad
A(\lda, y) \equiv \log \sum_{x\in{\cal X}} \pi(x) e^{\lda^\top \mathbf{F}(x,y)}.
$$

It follows that the utility function $U(\lda)$ is concave as a general property of likelihood in exponential families, hence the training can be implemented using a standard convex optimization scheme such as the limited-memory BFGS algorithm \cite{Byrd-95}, see Appendix~\ref{appendix:maxent} for details. It also implies that maximizing $U(\lda)$ is dual to a minimum KL~divergence problem \cite{Csiszar-84}:
$$
\min_{p\in{\cal P}} D(p\|\pi \cdot h) = \sum_{x\in{\cal X}}\sum_{y\in{\cal Y}} p(x,y) \log \frac{p(x,y)}{\pi(x)h(y)},
$$
subject to the constraints $p(y)=h(y)$ and $\E_p[\mathbf{L}] \succeq \E_h[\mathbf{L}]$, where 
${\cal Y}=\{y_1,\ldots,y_N\}$ is the set of training data values, ${\cal P}$ is the set of joint probability distributions on ${\cal X}\times {\cal Y}$, $h(x,y)$ is the joint empirical distribution of targets and data, and $h(y)$ is the corresponding marginal distribution. 

Therefore, the optimal composite model may be interpreted as the $I$-projection of the independence distribution $\pi(x)h(y)$ onto a set of admissible joint distributions defined by two types of contraints. The marginal constraint $p(y)=h(y)$, on the one hand,  ensures that only the conditional distribution $p(x|y)$ is ``updated'', making the $I$-projection a special case of maximum conditional entropy \cite{BergerA-96}. The mean log-likelihood value constraints $\E_p[\mathbf{L}] \succeq \E_h[\mathbf{L}]$, on the other hand, encapsulate dependences between the target and the data, and consider a model admissible if it guarantees the same average log-likelihood levels as observed separately for each feature. This is weaker a constraint than assuming that the features are distributed according to the generative models $p(z_i|x)$: it only assumes that the ``description length'' \cite{Grunwald-07} achieved by each agent is truthworthy.

A particularly insightful analysis given in \cite{Grunwald-04} shows that the $I$-projection is under broad conditions a minimax strategy for prediction according to the log loss. Hence, any $I$-projection is in some sense a best possible predictive model under particular constraints. Being more specifically a maximum conditional entropy method, trained composite inference belongs to the same family as multinomial logistic regression, only differing by the choice of constraints, which, in the latter case, are not related to feature generative models.

In the $I$-projection framework, the composite weights are the Lagrange multipliers associated with the mean log-likelihood inequality constraints. Weights corresponding to inactive constraints vanish, hence the training has a natural tendency to produce sparse weights. As illustrated in Figure~\ref{fig:disc_weight_plot} on the breast cancer UCI dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)}}, there is no clear relation between optimal weights and feature-level discrimination power, showing that the training differs significantly from univariate feature selection as it takes feature redundancy into account.

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=.6\textwidth]{disc_weight_plot.pdf}
  \end{center}
\caption{Distribution of optimal composite weights vs individual feature discrimination power in a binary classification task (breast cancer UCI dataset). Discrimination is measured by the maximum KL divergence between class-conditional generative distributions.}
\label{fig:disc_weight_plot}
\end{figure}


As is customary and is generally good practice, the training examples may be weighted in~(\ref{eq:train_likelihood}) so that the empirical distribution of classes matches the prior~$\pi(x)$ (obtaining balanced classes if the prior is uniform). This does not affect the above maths provided that the data weights are reflected in the empirical distribution~$h(x,y)$. 

Other variants of the training scheme that we mention here but do not particularly recommend for interpretability include adding basis functions to the exponential family. For instance, class-dependent offsets may be learned concurrently with the composite weights, which amounts to learning the ``prior'' in the training phase. It is also possible to replace the mean log-likelihood inequality constraints with equality constraints: the maximization of $U(\lda)$ is then unconstrained, therefore some trained composite weights may be negative. Finally, the mean log-likelihood constraints may be expressed for each class rather than being averaged wrt the prior: this leads to an extension that we call ``super composite likelihood'', in which weights become class-dependent (see Appendix~\ref{appendix:super}).


%used to predict whether breast lumps are benign or malignant based on a set of features extracted from fine-needle aspirate images.

%The most discriminative features tend to get large weight, however some quite discriminative features may get low weight if strongly correlated with a more discriminative feature

%Non-linear means that features with average discrimination power may get low weight as they are superseded by the most discriminative features. But some features with low discrimination power may also get large weight, which does not mean that they are very influential in practice (see above). This is illustrated in Figure~\ref{fig:disc_weight_plot} with the UCI breast cancer dataset. 

%{\color{red} Equality constraints (negative weights) are less interpretable.}

%{\color{red} Offsets.}

%{\color{red} Data weights.}

%Need to weight the data to adjust to prior as a general measure when minimizing cross-entropy, which should yield a conditional model that approximates the ``true'' conditional distribution, $q(x|y)\approx p(x|y)$. So the model will absorb the ``true'' marginal of $x$, which should thus be replaced by the prior if we want the model to mimic ideal Bayesian inference. In our case, it means that the composite likelihood approximates the true likelihood.

%We can further enforce exact prior matching by including offsets ($K-1$ free parameters), in which case the training phase learns $\pi(x)$ so that it matches the prior $\pi(x)$ in the sense $\int h(y)p_\lambda(x|y)dy = \pi(x)$. This ``deforms'' the prior to improve the training score. But why is it important to enforce this constraint? Again, it all makes things just less interpretable. 

%When including offsets, using homoscedastic generative variance estimation and equality constraints, composite likelihood is equivalent to binomial logistic regression. However, equality constraints are not easily interpretable.

%In some sense, composite weights replace the multiple testing correction required in the frequentist inference paradigm for proper control of false positives. Here, we don't control false positives, we control prediction accuracy.

%Why not using the feature-based posteriors or Bayes factors rather than the likelihoods? It's a possibility (perhaps a connection here with Gr\"unwald's luckiness). A justification for not doing it is that the coordinator should discard the priors used by the agents. The constraints are not purely evidence-based since the moments depend on the coordinator's prior but it is arguable a mess of priors wouldn't be good.

%Single feature case ($y=z$): the maxent solution has the form $p(x|y)\propto\pi(x)p(y|x)^\lambda$. We expect to have $\lambda=1$ but it's not necessarily the case. It is if $h(y)=\int\pi(x)p(y|x)dx$ because then $h(y)p(x|y)=\pi(x)p(y|x)$ so the constraint is verified (and active). This also tells us that the data marginal should be consistent with the prior on labels for this much expected consistency property to hold. So, either weight datapoints so as to match a desired prior (e.g., uniform) or take the empirical distribution of labels as the ``prior''. 

%Which brings another question: in this case, the (unique) weight is insensitive to the prior -- is it true in general? The answer is no. The weights generally depend on the prior. Hence the maxent composite likelihood is prior-dependent. This is an important conceptual difference with the classical notion of likelihood.



\section{Discussion}
\label{sec:discussion}

{\color{red} Why prediction performance is still important? Because it validates (or not) interpretation. Interpretation is the goal but prediction performance is a crucial intermediate step.}

{\color{red} We cannot rank features according to their weights. A feature might incidentally get a large weight because its likelihood is flat. Feature relevance also depends on conditional likelihood distribution. Consider mutually independent features: each one recieves a unit weight, but some may be independent from target.}

{\color{red} Semi-generative method. Good for shallow learning or transfer learning. Still need representation learning methods.}

{\color{red} It is easy to build a good classifier. You just need to pick a parametric family that is big enough to produce a good fit yet small enough to avoid overfitting. Less easy is to interpret what the classifier does. Deep learning is cool but it is relevant to AI only if the representations it learns are generic enough to be used in other problems, i.e. it should ultimately be validated via transfer learning.}

{\color{red} Composite Bayes is invariant by affine transformation of the data (unlike multinomial LR, which is problematic when damping is applied).}

{\color{red} Is damping important? (it guarantees training cost is stricly convex).}

{\color{red} Can we use the same dataset to learn both the feature generative distributions and the composite weights? In principle, nothing speaks about that.} 


Some experimental observations. In the iris dataset (historical data used by Fisher to demonstrate linear discriminant analysis), composite Bayes finds that petal length is the most relevant feature followed by petal width, while sepal length and width get zero weight. In the wine dataset, the flavanoids is the most relevant feature for standard composite Bayes, but it's color intensity closely followed by flavanoids for the super composite version. In the breast cancer, both approaches are equivalent since it is a binary problem, weights are very sparse: only 13/30 are non-zero with worst area the most relevant. In the digits prolem, the most relevant features tend to be in the image center; composite Bayes can achieve similar performance to multinomial logistic regression with about 10 times less parameters.

Number of independent discriminative parameters in logistic regression: $(K-1)(F+1)$. In standard composite inference: $K-1+F$. Difference $=(K-2)F$. In super-composite inference: $(K-1)(F+1)$ (if reference is a class). Number of generative parameters in composite inference using univariate Gaussian models: $2K$ with heteroscedasticity, $K+1$ with homoscedasticity. 

Intuition behind weights: notion of global feature relevance. But does $\lambda_1>\lambda_2$ imply that $z_1$ is more relevant than $z_2$ for a particular instance? No. But we can assess the influence of each scaled likelihood in order to rank features according to their influence on the decision.

CL is a concept from computational statistics that has mainly been developed so far in a frequentist perspective as a surrogate for the maximum likelihood method. We have shown a deep connection between CL and probabilistic inference, thereby establishing CL as a class of discriminative models. Because CL is built from a set of marginal feature-specific generative distributions, it is in essence a two-step semi-generative, semi-discriminative learning approach. In the first, ``generative'' phase, the feature distributions are learned; in the second, ``discriminative'' phase, the feature weights are learned. This strategy can be thought of as a form of non-adaptive boosting.

%The first phase corresponds to the training of ``weak learners''. The second phase amounts to a form of boosting. 

A purely discriminative learning could be used instead but...
Why potentially more efficient than pure discriminative training: because generative training is always more efficient than discriminative training if models are comparable. So the key point is that ``we have less parameters in the discriminant phase". Good in small datasets. But also in asymptotic regime if the features are weakly or highly correlated (???).

Logistic regression / naive Bayes example. Under homoscedasticity (assuming that each feature has class-independent variance), CBI is equivalent to logistic regression -- because the predictive distribution family is the same. This is a case where BCI brings nothing. But consider heteroscedasticity, then BCI yields a quadratic classifier. Compared to a fully discriminative model, the number of parameters to learned in the discriminative phase is reduced by half.

The first training phase is easier if supervised but could be unsupervised too (using mixture models). How do we then deal with label switching issues? Can we safely assume conditional feature independence {\em in the generative training phase}? I believe so provided that the marginal distribution parameters are disjoint. It's obvious in the supervised learning scenario.

If unsupervised, the first learning phase could be compared with contrastive pre-training of RBMs \cite{Hinton-06,Fischer-14}, which also optimize parameters for generation of observable features. BCI is comparable with an RBM with a single output unit (which is just a generative model assuming conditionial feature independence). The key difference is that the RBM is a full generative model while BCI only deals with marginal models, hence relying on weaker assumptions. This won't change anything in the pre-training phase but RBMs have to deal with more parameters in the generative learning phase. In fully supervised context, RBM pre-training is pointless since all parameters learned in the first phase will be overwritten. In BCI, pre-training is crucial even in supervised context because the parameters learned in the discriminative phase (the feature weighs) describe a sub-manifold of the predictive distribution family.

Needs features. Not a representation learning method, but could be coupled why not.


%{\color{red} Moreover, while RBM parameters are typically refined in a supervised discriminative learning step, disjoint set of parameters for SCL. Dunno how to say that.} 

%In such context, SCL competes with classical discriminative models (logistic regression, Gaussian processes \cite{Rasmussen-06}, maximum entropy models \cite{BergerA-96}, etc.), and may compare more or less favorably in practice depending on the amount of training data. For relatively small training datasets, we may hope for more accurate inference using~SCL than using traditional discriminative models, extrapolating from the results of \cite{Ng-01} regarding the comparison between logistic regression and na\"ive Bayes classifiers.
%{\color{red}Ici, il manque la comparaison avec les RBMs qui ne sont pas (forcement) des modeles discriminatifs.}

%%, hence alleviating the need for heavily supervised model training

%In summary, CL has the potential to yield weakly supervised or unsupervised Bayesian-like inference procedures depending on the particular task at hand. This property reflects the encoding of statistical relationships between the data and {\em all} unknown parameters. CL thus appears as a trade-off between generative models, which are optimal for unsupervised learning but possibly intractable, and traditional discriminative models (logistic regression, Gaussian processes \cite{Rasmussen-06}, maximum entropy models \cite{BergerA-96}, etc.), which are inherently supervised. CL models are discriminative models assembled from atomic generative models and, from this point of view, may be considered as {\em semi-generative} models.

%CL may be considered as a {\em semi-generative} model: a discriminative model assembled from partial generative models.

%As a note of caution, we shall stress that the pre-determined weights assigned to the different associations between observed and unobserved values represent prior knowledge regarding the informativeness of clues. A poor choice of weights will inevitably result in a poor approximation to the ``true'' Bayesian posterior -- the posterior that would be obtained from a realistic generative model if it was tractable. In future work, we will investigate feature selection strategies to mitigate this problem.

% Improve the discussion on following aspects:
% ** Why is it compatible with unsupervised learning? Give more insight.
% ** Stress the contribution: class-specific weighting.
% * Pivotality argument.
% * Bayes is a special case of composite Bayes.

{\color{red}Product of fucking experts \cite{Hinton-02}. Log-linear pool to build a generative model (assuming in fact independence between experts). Better seen as a ``log-mixture model''. Sounds like each expert is associated with a class, so it's the same as a RBM. Contrastive learning approximates ML parameter estimation. The experts are learning jointly. It's just a generative model.}

{\color{red}Two-step training: generative phase then discriminative phase. Why is it cool?}

{\color{red}CBI is just a way to reweight Naive Bayes. What's the big deal? Can we really expect massively superior performance? Are we just talking about realistic credibility sets?}

Future work: can we collapse generative and discriminative training into a single step? Maybe using the concept of {\em minimally informative likelihood} \cite{Yuan-99b}.


\appendix

\section{Conditional composite likelihood}
\label{appendix:conditional}

As a straightforward  extension of marginal CL, each feature-based likelihood may be conditioned by an additional ``independent'' feature $z^c_i = f^c_i(y)$ considered as a predictor of the ``dependent'' feature, $z_i=f_i(y)$, yielding the more general form:
\begin{equation}
\label{eq:cond_feat_lik}
\ell_i(x) = p(z_i|x,z^c_i).
\end{equation}

Conditioning may be useful if it is believed that $z^c_i$ alone is little or not informative about $x$, but can provide relevant information when considered jointly with $x$, as in the case of regression covariates, for instance. Equation~(\ref{eq:comp_lik}) then amounts to conditional CL \cite{Varin-11}, a more general form of CL also including Besag's historical {\em pseudo-likelihood} \cite{Besag-74} developed for image segmentation.


\section{Optimal predictive model}
\label{appendix:maxent}

{\color{red} Need cleaning up. Note offsets imply we minimize mutual information. But let's not discuss MIL here.}

Let $\pi(x)$ some prior distribution on~$X$. Optimal prediction aims to select the joint distribution $p(x,y)$ that minimizes the KL divergence $D(p(x,y)\|\pi(x)p(y))$:
$$
D(p) = -\int p(x,y) \log \frac{p(x|y)}{\pi(x)} dx dy,
$$ subject to feature mean-value constraints of the form:
$$
\int p(x,y) f(x,y) dx dy = \mu.
$$

Note that with some constraints of this type, we can enforce $p(x)=\pi(x)$. In this case, the best predictive model minimizes the mutual information between the data and the target.

One important special case of this problem arises when imposing the additional constraint that $p(y)=h(y)$ is known. This is the {\em maximum conditional entropy} method \cite{BergerA-96} which we review hereafter. 



%We will also treat the case where the marginal constraint is not assumed, which corresponds to the {\em minimally informative likelihood} method \cite{Yuan-99b,Yuan-99}.

%We could say that MIL enforces the prior constraint explicitly while Maxent does it implicitly by pre-weighting the data. So it's a soft constraint in the Maxent case (which ends up being loosely satisfied if the model fits the empirical distribution well enough), and it's a hard constraint in the MIL case. On the other hand, the MIL formulation does not exploit the  empirical marginal at all (it only exploits the moments). Which one do we prefer?


\subsection{Maximum conditional entropy}

$$
D(p) = -\int h(y)p(x|y) \log \frac{p(x|y)}{\pi(x)} dxdy
$$

Lagrangian:
$$
{\cal L}(p,\lambda) = D(p) + \lambda^\top (\int h(y)p(x|y)f(x,y)dxdy - \mu)
$$

Optimal $p$ at fixed $\lambda$:
$$
\frac{\partial{\cal L}}{\partial p}
=
- h(y) \log \frac{p(x|y)}{\pi(x)} + h(y) \lambda^\top f(x,y) + (\kappa(y)-1)h(y)
$$
$$
\Rightarrow
\quad
p_{\lambda}(x|y) = \frac{\pi(x)}{z(\lambda,y)} e^{\lambda^\top f(x,y)}
$$
with:
$$
z(\lambda,y) = \int \pi(x) e^{\lambda^\top f(x,y)} dx
$$

Dual:
$$
\psi(\lambda) = \lambda^\top \mu - \int h(y) \log z(\lambda, y) dy 
$$

$$
\nabla z
=
\int \pi(x) f(x,y)  e^{\lambda^\top f(x,y)} dx
$$

$$
\nabla \nabla^\top z 
=
\int \pi(x) f(x,y) f(x,y)^\top e^{\lambda^\top f(x,y)} dx
$$

$$
\nabla\psi
= \mu - \int h(y) \frac{\nabla z}{z} dy
$$

$$
\nabla\nabla^\top\psi
= - \int h(y) \left(
\frac{\nabla \nabla^\top z}{z} 
- \frac{\nabla z \nabla z^\top}{z^2}
\right)
dy
$$

Note the equivalent expression of the dual function as:
$$
\psi(\lambda)
=
\int h(x,y) \log \frac{p_\lambda(x|y)}{\pi(x)} dx dy
= D(h\|\pi(x)h(y)) - D(h\|p_\lambda)
$$


\paragraph{Convexity}

The convexity of any function of the form $A(\lambda)=\log z(\lambda)$ with $z(\lambda)=\int\pi(x)e^{\lambda^\top f(x)}dx$ can be proved using H\"older inequality:
$$
\int f_1 f_2 \leq 
\left(\int f_1^{\frac{1}{\lambda}} \right)^\lambda  
\left(\int f_2^{\frac{1}{1-\lambda}} \right)^{1-\lambda}  
$$
for $f\succeq 0$, $g\succeq 0$, $0\leq\lambda\leq 1$. Just do $f_1=\exp(\lambda\theta^\top f)$ and similarly for $f_2$ with $1-\lambda$. This proves that $A_y(\lambda) = \log z(\lambda,y)$ is convex for any~$y$  and the fact remains true by averaging $A_y(\lambda)$ over several $y$, therefore $-\psi(\lambda)$ is convex.

There is another, more insightful way to prove that $A(\lambda)$ is convex. Consider the variable $F=f(X)$ with $X\sim p_\lambda(x)$. It is easy to verify the following facts:
$$
\nabla A = \frac{\nabla z}{z} = E(F),
\qquad
\nabla\nabla^\top A 
= \frac{\nabla\nabla^\top z}{z} 
- \frac{\nabla z \nabla z^\top}{z^2}
= {\rm Var}(F),
$$
so the Hessian is a variance matrix and is therefore positive, which proves convexity. How about definite positiveness? To stand a chance, we need the number of features to be less than the number of labels otherwise the basis vectors are linearly dependent.

Let $m_F(y)$ and $V_F(y)$ respectively denote the mean and variance of $f(x,y)$ conditional on~$y$. Denoting by $\langle \rangle$ the empirical mean wrt $y$:
$$
\nabla \psi = \mu - \langle m_F \rangle
$$

$$
\nabla \nabla^\top \psi = - \langle V_F \rangle
$$

In general, we have more features than classes for good prediction performance so each matrix $V_F(y)$ is singular and tends to be smaller (in terms of spectral radius) as the number of features increases. The Hessian is a sample average of these matrices and will thus follow the same trend.

In practice, it looks like the problem is always nearly singular so we may need regularization!

\paragraph{Damping}

We can regularize the Maxent problem via a $L_2$ penalty, yielding:
$$
\psi_r(\lambda) = \psi(\lambda) - \alpha \|\lambda\|^2
$$

This implements a callback force towards the prior $\pi(x)$. Can be interpreted as some way of saying that our trust in the empirical information is not infinite. 


\paragraph{Utility}

Let us consider a reference target value $x_0$. By definition,
$$
p_\lambda(x_0|y) = \frac{\pi(x_0)}{z(\lambda,y)} e^{\lambda^\top f(x_0,y)}
$$

Therefore,
$$
\log z(\lambda,y) = \lambda^\top f(x_0,y) - \log \frac{p_\lambda(x_0|y)}{\pi(x_0)}
$$

We can use this fact to rewrite the dual function:
$$
\psi(\lambda) = \lambda^\top (\mu - \mu_0) + \int h(y) \log \frac{p_\lambda(x_0|y)}{\pi(x_0)} dy
$$
with: 
$$
\mu_0 = \int h(y) f(x_0, y) dy
$$

The first term $\lambda^\top (\mu-\mu_0)$ is essentially the same thing as the utility considered in my previous version of composite likelihood training (which was maximized on the simplex). Utility is label-dependent as it involves the moments. If the basis consists of feature-based log-likelihood functions, it measures the discrepancy vs the reference and pushes towards large coefficient values. Cross-entropy appears here as a regularized version of utility. The regularization term $E[\log p_\lambda(x_0|y)$ replaces the hard constraint that $\lambda$ lie in the simplex: it forces the reference class to be probable for any input, so it will tend to discard predictions that are peaked on a single class if it's not $x_0$. This term acts as a callback force.



\subsection{Minimally informative likelihood}

In this case, the constraint $p(y)=h(y)$ no longer applies. Consider:
$$
D(p,m) = \int p(x,y) \log \frac{p(x,y)}{\pi(x)m(y)} dx dy
$$
and note that the prediction objective is: $D(p) = \min_m D(p,m)$. This is the Blahut-Arimoto trick, which enables us to consider an alternate optimization strategy wrt $p$ and $m$, respectively. 

At fixed $p$, this is a maximum (joint) relative entropy problem. Lagrangian...
$$
{\cal L}(p,m,\lambda)
=
- \int p(x,y) \log \frac{p(x,y)}{\pi(x)m(y)} dxdy
+
\lambda^\top \left( 
\int p(x,y) f(x,y) dydy - \mu 
\right) \\
$$

The derivative is given by:
$$
\frac{\partial\cal L}{\partial p(x,y)}
= 
- \log \frac{p(x,y)}{\pi(x)m(y)} - 1
+ \lambda^\top f(x,y)
$$
hence the optimal model at fixed $m$ has the form:
$$
p_{\lambda,m}(x,y) = \frac{1}{z(\lambda,m)} \pi(x) m(y) e^{\lambda^\top f(x,y)} 
$$
with:
$$
z(\lambda,m) = \int \pi(x) m(y) e^{\lambda^\top f(x,y)} dx dy
$$

Note that the induced posterior distribution $p_{\lambda,m}(x|y)\propto \pi(x)p_{\lambda,m}(y|x)$ has the same form as in the conditional maxent case (if using the very same moment constraints).

Dual function...
$$
\psi(\lambda,m) 
= \lambda^\top \mu - \log z(\lambda, m)
$$

$$
\nabla z = \int \pi(x) m(y) f(x,y) e^{\lambda^\top f(x,y)} dx dy
$$

$$
\nabla\nabla^\top z = \int \pi(x)m(y) f(x,y) f(x,y)^\top e^{\lambda^\top f(x,y)} dxdy
$$

$$
\nabla \psi = \mu - \frac{\nabla z}{z} 
$$

$$
\nabla\nabla^\top \psi = 
- \left(
\frac{\nabla \nabla^\top z}{z} 
- \frac{\nabla z \nabla z^\top}{z^2}
\right) 
$$

Equivalently, for any distribution $h(x,y)$ that satisfies the moment constraint, we have:
$$
\psi(\lambda,m) 
=
\int h(x,y) \log \frac{p_{\lambda,m}(x,y)}{\pi(x)m(y)} dxdy
= D(h\|\pi_xm_y) - D(h\|p_{\lambda,m})
$$
showing that this is a generative learning problem!

The BA algorithm alternates between estimating $\lambda$ at fixed $m$ via the maximization of $\psi(\lambda,m)$ and estimating $m$ at fixed $\lambda$ by marginalization:
$$
m(y) = \int p_{\lambda,m}(x,y) dx
$$

The MIL seems intractable if we don't impose constraints on $m(y)$. It becomes feasible by restricting $m(y)$ to have a support limited to the training data, similarly to the conditional maxent implementation:
$$
m(y) = \sum_k w_k \delta(y-y_k),
$$
hence $m$ is determined by a finite set of weights $w_k$. In practice, the optimal weights look very sparse.

{\color{red} Big fat speculation: wouldn't MIL make more sense when passing generative mean-value constraints, i.e. empirical mean and variance per class? This could avoid the double learning stage! We don't do it with Maxent because we want feature-based generative models for interpretation purposes (if we did, it would be logistic regression). But using MIL, we get a full generative model for free... which is a game changer. It's no longer composite! The trouble is, this model is kinda degenerate (due to its ``empirical'' style, which is needed for computational reasons): the likelihood of any new~$y$ is almost surely zero. So not very good to do hypothesis testing. We have an interpolation problem, Houston. And that interpolation problem is the very reason why we trashed discriminative models from the start! So maybe not the way to go.}


\subsection{Data weighting}

The distribution $h(x)$ is the marginal of the labels, which is natural to approximate via their empirical distribution. We may then have a discrepancy between $h(x)$ and $\pi(x)$, which makes little sense if it is believed that $h(x)$ is a ``good encoder''. 

To correct for this, we may adjust the empirical distribution of labels and data so as to match the target prior, $\pi(x)$, leading to weight the data points as a preprocessing step:
$$
h(y) = \sum_i w_i \delta(y-y_i),
\qquad
w_i = \frac{\pi(x_i)}{h(x_i)}
$$

Are the two methods equivalent? No. The feature-based parameter estimates are obviously the same in both cases, but the search space for the predictive distribution is different, hence we can't get the same answer. Which one of the two methods is best? The one that allows you to choose your own prior makes more sense to me because the distribution of classes in your training dataset may be completely different from the one that you will deal with in practice. For instance, it be weird to say that someone with all the symptoms of a very rare disease does not have it because it is very rare. 



\section{Super composite likelihood}
\label{appendix:super}

When chosen for computational simplicity, features may not only convey limited information at individual level, their informativeness may also be very much class-dependent. Consider, for instance, diagnosing a disease from a routine medical checkup. Body temperature may point to a bacterial infection by comparison with normality, but would not help detecting a non-infectious cardiovasc ular disease -- and conversely for, say, blood pressure. This motivates a more general pooling than~(\ref{eq:log_pool}) setting where features can be weighted depending on the target variable:
$$
p_\lambda(x|y) \propto \pi(x) \prod_i \left[\frac{p(z_i|x)}{p_0(z_i)}\right]^{\lambda_i(x)}
$$

$p_0$ is an arbitrary reference (implicit if weights are class-independent as it is then effectless). Can be the feature distribution averaged over all classes (not recommended I would say) or the distribution of a particular class if that makes sense (e.g. healthy population). In this case, the super composite likelihood is equivalent to the standard one in the two-class case.

This is the super composite likelihood idea in a nutshell.

Again, we can train this model and see that it is the same as solving a maxent problem. Difference with above is that the constraints are not averaged over $x$ (basis functions are of the form $\chi(x-a)\rho_i(x,y)$) and that they involve arbitrary log-likelihood ratios vs some reference distribution. Make sense to have a reference as it gives a customizable baseline for detection.

If some classes are merged or split, then the other classes keep the same weights (assuming $h(y)$ is kept constant, so we need to make sure the prior is adapted accordingly)! So if a patient was found to have 10 times more probability to be MCI than healthy (and healthy is the reference class), and we later split AD population into, say, pure AD and mixed AD, the MCI probably ratio fact will remain.

Super composite likelihood is essentially about combining binary versions of composite likelihood. It is analogous to the ``one-versus-rest'' strategy in multiple logistic regression. 

%Link with PDF projection \cite{Baggenstoss-03}.

Advantages: richer model (so better predictions), probabilty ratios stable to class addition. Drawbacks: require a reference, no more consistency. Still interpretable, that's why it is worthwile mentioning.

%I was thinking about the unit sum weight version... The diagnosis example could be one in which clues are independent, in which case $\lambda_i(x)\equiv 1$ would be optimal, making any super composite trick irrelevant. Also see that with hypothesis-dependent weights, we may lose the estimation consistency. At the end of the day, the interest of the super composite idea is questionable.


\section{Data reduction inequality}
\label{appendix:reduction_inequality}

A fundamental property of the Kullback-Leibler divergence is that it decreases under feature extraction, {\em i.e.}, application of a deterministic transformation. This comes as a consequence of the logarithmic sum inequality \cite{Cover-06} (and also follows as a straightforward corollary of the PDF~projection theorem \cite{Minka-04,Baggenstoss-15}). 

The proof is elementary and can be sketched as follows. Let $Z=f(Y)$ some feature extracted from a variable~$Y\sim p(y)$. Given an arbitrary distribution~$\pi(y)$, let  $\tilde{p}(z)$ and $\tilde{\pi}(z)$ the distributions induced on~$Z$ by $p(y)$ and $\pi(y)$, respectively. For any potential value~$z$ of~$Z$, consider the level set: $\Gamma(z)=\{y, f(y)=z\}$. Under conditions of existence, we can partition the integral involved in the KL~divergence $D(p\|\pi)$ using these level sets: 
$$
\int p(y) \log \frac{p(y)}{\pi(y)} dy
=
\int \left( \int_{\Gamma(z)} p(y) \log \frac{p(y)}{\pi(y)} dy \right) dz\\
.
$$

Applying the logarithmic sum inequality \cite{Cover-06} to each integral over~$\Gamma(z)$, we then readily get:
$$
\int_{\Gamma(z)} p(y) \log \frac{p(y)}{\pi(y)} dy
\geq 
\tilde{p}(z) \log \frac{\tilde{p}(z)}{\tilde{\pi}(z)}
,
$$
owing to the fact that $\displaystyle \int_{\Gamma(z)} p(y) dy = \tilde{p}(z)$ by definition. Therefore,
$$
\int \tilde{p}(z) \log \frac{\tilde{p}(z)}{\tilde{\pi}(z)} dz
\leq 
\int p(y) \log \frac{p(y)}{\pi(y)} dy
,
$$
or, in a more compact way, $D(\tilde{p}\|\tilde{\pi})\leq D(p\|\pi)$. The case of equality occurs if, and only if, $p(y)/\pi(y)=\tilde{p}(z)/\tilde{\pi}(z)$, in other words, if~$z$ is a sufficient statistic for the ``alternative'' hypothesis~$H_1:y\sim p(y)$ versus the ``null'' hypothesis~$H_0:y\sim\pi(y)$. 

Since it is a well-known fact that $D(\tilde{p}\|\tilde{\pi})\geq 0$ as any KL~divergence, we conclude that:
\begin{equation}
\label{eq:reduction_inequality}
0 
\leq D(\tilde{p}\|\tilde{\pi}) 
\leq D(p\|\pi)
.
\end{equation}


\section{Basic asymptotic properties of composite likelihood}
\label{appendix:asymptotic}

It follows from the double inequality~(\ref{eq:reduction_inequality}) that, for any extracted feature~$z_i$ and for any hypothesis~$\theta$,
$$
0 \leq
E\left[
\log \frac{p(z_i|\theta_\star)}{p(z_i|\theta)}
\right]
\leq
E\left[
\log \frac{p(y|\theta_\star)}{p(y|\theta)}
\right],
$$
where the expectation is taken with respect to the true distribution $p(y|\theta_\star)$. Using a weighted sum of such inequalities, we can bracket the expected variations of the logarithm of any standard composite likelihood function (assuming unit sum positive weights):
\begin{equation}
\label{eq:variation_bound}
0 \leq
E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta, \mathbf{w})} \right]
\leq 
E\left[ \log \frac{L(\theta_\star)}{L(\theta)} \right]
.
\end{equation}

This implies two asymptotic properties of standard composite likelihood: 
\begin{itemize}
\item {\em Consistency.} The expected log-composite likelihood is maximized by~$\theta_\star$, the true value of~$\theta$.
\item {\em Conservativeness.} The expected log-composite likelihood function is ``flatter'' than the true expected log-likelihood. In other words, composite likelihood ratios $L_c(\theta, \mathbf{w})/L_c(\theta_\star, \mathbf{w})$ relative to~$\theta_\star$ tend to be larger than the corresponding true likelihood ratios. 
\end{itemize}

These properties do not necessarily extend to the more general case of SCL. We may state a weaker consistency property by considering the upper envelope of expected log-composite likelihood ratios:
$$
M(\theta) = \max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right],
$$
where ${\cal W}$ is the $n$-dimensional simplex. Clearly, the expected logarithm of the SCL is upper bounded by~$M(\theta)$:
$$
E[\log {\cal L}_c(\theta, \mathbf{W})] \leq M(\theta)
.
$$

Moreover, $M(\theta)$ is maximized by $\theta_\star$ since (\ref{eq:variation_bound}) implies that, for any $(\theta,\theta_0)$,
$$
E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
\leq
E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right],
$$
which remains true when taking the maximum over the weights in both sides: 
$$
M(\theta) = 
\max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
\leq
\max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
= M(\theta_\star)
.
$$

Therefore, maximizing SCL corresponds to maximizing a lower bound on~$M(\theta)$, which can be considered as an objective function since it is maximized by~$\theta_\star$ (like the expected log-likelihood). Lower bound maximization guarantees a certain objective value, however not the maximum, hence asymptotic consistency may not hold.

\section{Log-likelihood ratio}

$$
\log r = \log \frac{N(x;\mu_1,\sigma_1)}{N(x;\mu_0,\sigma_0)}
$$

$$
\log N(x;\mu,\sigma) = -\frac{1}{2}\left[
\log(2\pi\sigma^2) + \frac{(x-\mu)^2}{\sigma^2}
\right]
$$

$$
\log r = -\frac{1}{2}
\left[
\log \frac{\sigma_1^2}{\sigma_0^2}
+ \frac{(x-\mu_1)^2}{\sigma_1^2} - \frac{(x-\mu_0)^2}{\sigma_0^2}
\right]
$$

It follows:
\begin{eqnarray*}
\log r 
 & = & 
-\frac{1}{2}\left[
\log \frac{\sigma_1^2}{\sigma_0^2}
+ \frac{\sigma_0^2(x-\mu_1)^2 - \sigma_1^2(x-\mu_0)^2}{\sigma_0^2 \sigma_1^2}
\right] \\
 & = & 
\frac{1}{2}\left[
\log \rho^2
+ \frac{(\rho^2-1)x^2 - 2(\rho^2\mu_0-\mu_1)x+ (\rho^2\mu_0 ^2-\mu_1^2)}{\sigma_0^2}
\right]
\end{eqnarray*}
with $\rho=\sigma_1/\sigma_0$. In the case of same variance ($\rho=1$), this simplies to an affine function of $x$ with offset given by the center of $(\mu_0,\mu_1)$:
$$
\log r = \frac{\mu_1-\mu_0}{\sigma^2} \left(
x - \frac{\mu_0 + \mu_1}{2}
\right) 
$$

In any case,
$$
E[\log r | H_1] = 
\frac{1}{2}
\left[
- \log \frac{\sigma_1^2}{\sigma_0^2}
+ \frac{\sigma_1^2 + (\mu_1-\mu_0)^2}{\sigma_0^2}
- 1
\right]
$$

Let $\delta = (\mu_1-\mu_0)/\sigma_0$. We have:
$$
E[\log r | H_1] = 
\frac{1}{2}
\left[
\delta^2
- \log \rho^2
+ \rho^2  - 1
\right]
$$


\section{Feature relevance}

Let $x_0$ a reference class and consider the log-probability ratio:
$$
\alpha = \log \frac{p_\lambda(x|y)}{p_\lambda(x_0|y)}
$$

The larger $\alpha$ for a given $x_0$, the happier we are. Let us average $\alpha$ wrt $x$ and $y$ as well as $x_0\sim \pi(x_0)$, which in this context may be interpreted as the outcome of a random classifier. Ignoring offsets, we have:
$$
E[\alpha]
= \sum_i \lambda_i \int \pi(x_0)\pi(x) p(z_i|x) \log\frac{p(z_i|x)}{p(z_i|x_0)} dz_i dx dx_0
= \sum_i \lambda_i \bar{d}_i
$$
with:
$$
\bar{d}_i =
\int \int \pi(x_0) \pi(x) D(p_i^x\|p_i^{x_0}) dx dx_0
$$

If we have offsets, then there is an offset in this expresion, meaning that part of the log-probability ratio is accounted for by the offsets... this is not important.

Hence, the expected log-likelihood ratio is a weighted sum of positive feature-specific contributions $r_i=\lambda_i \bar{d}_i$. The larger the contribution, the more influential the feature on the average. The analysis can be refined by skipping the averaging over $x_0$, in which case feature influence becomes dependent on the comparison class $x_0$... so it's all more complicated and potentially confusing. It would make sense to do that in the super-composite framework, where the reference class is fixed.

The bottom line is that global feature relevance is determined by the CL weight(s) but also by the KL divergence matrix. 

Note that, by the information processing inequality, we have $D(p_i^x\|p_i^{x_0}) \leq d_{\rm lim}(x,x_0)\equiv D[p(y|x)\|p(y|x_0)]$, hence $\bar{d}_i\leq \bar{d}_{\rm lim}$ and:
$$
r_i \leq \lambda_i \bar{d}_{\rm lim}
$$
so if we normalize relevance according to unit sum weights, no single feature can be more relevant than the full data. 

How about {\em local} relevance? We simply replace $d_{k\ell i}$ by the absolute empirical log-likelihood ratio $|\log p_k(z_i)/p_\ell(z_i)|$. That's it.

\bibliographystyle{abbrv}
\bibliography{cvis,stat,alexis}

%%\input{draft.biblio}

\end{document}

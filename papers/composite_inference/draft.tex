\documentclass[english]{scrartcl}

\usepackage{fullpage}
\usepackage{amstext,amssymb,amsmath,amsthm}
\usepackage{graphicx,subfigure}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{color}

\graphicspath{{.}{pics/}}

\title{Composite Bayesian inference}
%\date{}
\author{Alexis Roche\thanks{\url{alexis.roche@centraliens.net}}}

\def\x{{\mathbf{x}}}
\def\y{{\mathbf{y}}}
\newcommand{\blambda}{{\boldsymbol{\lambda}}}
\newcommand{\Blambda}{{\boldsymbol{\Lambda}}}
\newcommand{\bell}{{\boldsymbol{\ell}}}
\newcommand{\E}{\mathbb{E}}



\begin{document}

\maketitle

\begin{abstract}
We revisit and generalize the concept of composite likelihood as a method to make a probabilistic inference by aggregation of multiple Bayesian agents. The resulting class of predictive models, which we call composite Bayesian models, is a middle way between generative and discriminative models. This perspective gives insight to choose the weights associated with composite likelihood, either {\em a priori} or via learning; in the latter case, they may be tuned so as to minimize prediction cross-entropy, yielding an easy-to-solve convex problem. We argue that composite Bayesian inference trades off between interpretability and prediction performance, both of which are crucial to many artificial intelligence tasks.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Textbook statistical inference (frequentist or Bayesian) rests upon the existence of a probabilistic data-generating model that is both empirically valid and computationally tractable. Because this double requirement may be very challenging for multidimensional data, other inference models have been developed in applied science: deliberately misspecified generative models, as in quasi-likelihood \cite{White-82,Walker-13} or na\"ive Bayes \cite{Ng-01} methods; data compression models as in minimum description length \cite{Grunwald-07}; and discriminative models\footnote{A {\em discriminative} model is a parametric family that describes the conditional distribution of the target variable given the data, while a {\em generative} model describes the joint distribution of the target and the data.}, which currently dominate the field of artificial intelligence (AI) and typically require supervised learning on large datasets -- these include many classical machine learning \cite{Ho-95,BergerA-96,Vapnik-00,Rasmussen-06} and deep learning \cite{Lecun-15,Goodfellow-16} techniques (with the exception of deep belief networks \cite{Hinton-06}).

In a well-defined universe of possibilities, discriminative models can map data to predictions as well as intelligent beings or even better, however they lack introspection in the sense that they cannot {\em test} their predictions. Consider as a straightforward example deciding whether an object is a sauce pan or a frying pan based on depth. A two-class discriminative model can learn a threshold that correctly classifies most pans in practice, but will confidently classify a blender as a sauce pan, while linear discriminant analysis\footnote{Despite the name, linear discriminant analysis is based on a generative model.}, for instance, could hint that a blender is a poor match to both pan categories. Intrinsic inability to ``confirm'' or ``justify'' predictions makes discriminative models difficult to interpret.

There is growing awareness that AI should be interpretable in life-impacting applications \cite{Molnar-18}, where it is (and perhaps should remain) confined to a supportive role in human-driven decision-making processes, if only because decisions need to be explained and justified to other humans, while determining the set of possible outcomes may also be part of the process. For instance, in medical diagnosis, automated disease predictions need to be corroborated by individualized findings to be taken into account. In other words, there is little clinical value in ``black boxes'', and certainly more in automated methods that can ``justify'' themselves just like human experts. One such method that has long been used by clinicians is the familiar reference range analysis, which stems from simple generative models.

%Imagine a doctor telling a patient: ``Sorry, sir, you have Alzheimer's disease because this fancy AI software said so''. Unless the said software is 100\% reliable, which is practically impossible, this would obviously be unacceptable. Acceptable would be to corroborate the diagnosis with some key quantitative empirical observations: low memory test scores, hippocampal atrophy, etc. In other words, inference has to come with some insight. An AI system is but one expert relying on hypotheses which may be invalid, as any expert, and is therefore prone to error. A typical error source is to train a classifier on a subset of classes from the real world. We cannot avoid inference errors but we can safeguard against them by being able to interrogate the system: why do you think what you think? To achieve that, the system should be capable of {\em introspection.

%Vapnik: ``one should solve the classification problem directly and never solve a more general problem as an intermediate step''. True if the goal is classification only. Wrong if introspection is needed because the problem is then more general in nature.

%Human inference does not work in a pre-determined universe of ``causes''. We confront theories with reality to evaluate how likely they are, while knowing that none of the theories considered so far may be ``right''. In fact, we are not looking for the truth, but for a conving enough theory. 

%Composite likelihood seen as a pool of experts comes with a potential introspection mechanism: first sort experts by decreasing influence on the predictive distribution, see how likely the different classes are to the most influential experts.

A related limitation of discriminative models is that they are not suitable for unsupervised learning or on-the-fly parameter estimation because they treat the data and the model parameters as {\em marginally independent} variables, meaning that the data conveys no information about the parameters unless the target variable is observed. This is illustrated in Figure~\ref{fig:graph_comparison} by the respective directed graphs representing generative and discriminative models. For the same basic reason, supervised learning in a discriminative model is statistically less efficient than in a generative model spanning the same family of posteriors, hence it requires more training data to achieve optimal performance \cite{Ng-01}. 

%Overall, pure discriminative models are of little use outside the context of big labeled data.

\begin{figure}[!ht]
\begin{center}
\subfigure[Generative model]{\includegraphics[width=.25\textwidth]{generative.pdf}\label{fig:generative}}
\hspace*{.2\textwidth}
\subfigure[Discriminative model]{\includegraphics[width=.25\textwidth]{discriminative.pdf}\label{fig:discriminative}}
\caption{Directed graphs representing a generative and discriminative models, where $X$, $Y$ and $\theta$ respectively denote the target variable, the data, and the model parameters. Note the marginal independence of data and parameters in the discriminative model.}
\label{fig:graph_comparison}
\end{center}
\end{figure}

This note advocates probabilistic opinion pooling \cite{Genest-86} as a natural way to merge discriminative and generative modeling approaches in order to make predictions that are both accurate and interpretable. The key idea is to combine multiple low-dimensional generative models corresponding to different pieces of information extracted from the input data. Each such model acts as an isolated ``agent'' that uses a single feature to express a testable opinion in the form of a likelihood function of the target variable. The agent opinions are then aggregated into a unique predictive probability distribution analogous to a Bayesian posterior. This idea may be understood as a probabilistic version of boosting, or as a ``divide and conquer'' approximation to the intractable Bayesian posterior. 

%Importantly, each agent opinion is justifiable as it is based on a generative model.

When choosing the aggregation method as log-linear pooling, the predictive distribution turns out to be proportional to a quantity known in computational statistics as composite likelihood (CL), see \cite{Varin-11} and the references therein. CL was developed as a surrogate of traditional likelihood for parameter estimation. While maximum CL does not inherit the general property of maximum likelihood to achieve asymptotical minimum variance, it is asymptotically consistent under mild conditions \cite{Xu-11} (see Appendix~\ref{app:frequentist} for a basic weak consistency proof), and may offer an excellent trade-off between computational and statistical efficiency in practice.

Following the opinion pooling interpretation, the weights assigned to the different features in~CL may be optimized for prediction performance in a typical supervised learning scenario. As we will see, this strategy amounts to training a maximum entropy classifier using the feature-based log-likelihoods as basis functions. The likelihood functions themselves may need to be pre-trained, therefore the composite Bayesian approach typically involves a two-stage training scheme: a generative training (to learn the feature-based likelihood parameters) followed by a discriminative training (to learn the feature weights in aggregation).

The opinion pooling framework also suggests a generalization of CL, which we call {\em super composite likelihood}, in which the features may be weighted depending on the target values, leading to a more flexible predictive model that can be trained similarly.

%Technical details are given in the remainder. 

The reader is warned that slightly abusive mathematical notation is sometimes used in the remainder. This is done deliberately for the sake of clarity in places where we think that notation can be lightened without ambiguity. 


\section{Composite likelihood as opinion pooling}
\label{sec:log_pool}

Let $\mathbf{Y}$ an observable multivariate random variable with sampling distribution $p(\y|x)$ conditional on some unobserved variable of interest, $X\in{\cal X}$, where ${\cal X}$ is assumed to be a finite set for simplicity. Given an experimental outcome $\y$, the likelihood is the sampling distribution evaluated at $\y$, seen as a function of $x$:
$$
L(x) = p(\y|x)
.
$$

This requires a plausible generative model which, for complex data, may be out of reach or involve too many nuisance parameters. A natural workaround known as data reduction is to extract some lower-dimensional representation $z(\y)\sim f(z|x)$ using a many-to-one mapping, and consider the potentially more convenient likelihood function:
$$
\ell(x) = f(z|x)
.
$$

Substituting $L(x)$ with $\ell(x)$ boils down to restricting the feature space, thereby  ``delegating'' statistical inference to an ``agent'' provided with partial information. While it is valid for such an agent observing~$z$ only to consider $\ell(x)$ as the likelihood function of the problem, the drawback is that $\ell(x)$ might yield too vague a prediction of~$X$ due to the information loss incurred by data reduction. To make the trick statistically more efficient, we may extract several features, $z_i(\y)$ for $i=1,2,\ldots,n$, and try to combine the likelihood functions $\ell_i(x) = f(z_i|x)$ that they elicit.

If we see the likelihoods as Bayesian posterior distributions corresponding to {\em uniform} priors, this is a problem of probabilistic opinion aggregation from possibly redundant sources, for which several methods exist in the literature \cite{Tarantola-82,Genest-86,Garg-04,Allard-12}. A common choice, owing to its simplicity and tendency to produce single peaked distributions, is log-linear pooling\footnote{When $\pi(x)$ is not uniform, it is also called {\em generalized} log-linear pooling, however this terminology could be confusing here since we will present another generalization.}:
\begin{equation}
\label{eq:log_pool}
p_\blambda(x|\y) \propto \pi(x) \prod_{i=1}^n f(z_i|x)^{\lambda_i},
\end{equation} 
where $\pi(x)$ is a reference distribution, and $\blambda=(\lambda_1,\ldots,\lambda_n)$ is a vector of weights. We shall asume for feasibility that the weights are intrinsic to the agents and are therefore independent from the data~$\y$. Also, negative weights should be ruled out to warrant a natural monotonicity property: if all agents agree that an outcome $x_1$ is less probable than an outcome $x_2$, this should be reflected by the consensus probabilities, {\em i.e.}, we should have $p_\blambda(x_1|\y)\leq p_\blambda(x_2|\y)$. 


\section{Composite Bayes rule}
\label{sec:bayes_rule}

Log-linear pooling~(\ref{eq:log_pool}) bears a striking similarity to Bayes rule as it yields the form: 
$$
p_\blambda(x|\y)\propto \pi(x) L^c_\blambda(x),
$$
where the distribution $\pi(x)$ plays the role of a prior, while the quantity:
\begin{equation}
\label{eq:comp_lik}
L^c_\blambda(x) \equiv \prod_{i=1}^n \ell_i (x)^{\lambda_i}
\end{equation} 
plays that of a likelihood function, and happens to be known in computational statistics as a {\em marginal composite likelihood} \cite{Varin-11}. The slightly more general {\em conditional composite likelihood} form can be derived in the same way as above by conditioning all probabilities on confounding variables, see Appendix~\ref{app:conditional}. 

The clear computational advantage of CL over genuine likelihood is that it is more efficient to evaluate the marginal distributions of each feature than the joint distribution of all features. CL shares a convenient factorized form with the likelihood derived under the assumption of mutual feature independence, often referred to as {\em na\"ive Bayes} method in the machine learning literature, which corresponds to the special case of unitary weights, $\blambda\equiv 1$. Since log-linear opinion pooling does not assume feature independence, it yields both an alternative interpretation and a generalization of na\"ive Bayes, whereby unitary feature weights may be used by default but do not guarantee optimal prediction performance.


\section{Composite likelihood calibration}
\label{sec:calibration}

CL involves two types of parameters:
\begin{enumerate}
    \item ``generative'' parameters, {\em i.e.}, the parameters of the feature generation models $f(z_i|x)$; 
    \item ``discriminative'' parameters, {\em i.e.},  the weights~$\blambda=(\lambda_1,\ldots,\lambda_n)$ assigned to the feature-based likelihood functions in~(\ref{eq:log_pool}).
\end{enumerate}

If training data is available, the generative parameters may be tuned in a first stage using standard estimation techniques. Otherwise, they may be considered as nuisance parameters and eliminated at prediction time by applying to CL one of the same techniques as for traditional likelihood \cite{Berger-99}. We focus in the sequel on tuning the composite weights.


\subsection{Agnostic weights}

In the absence of knowledge, uniform weights may be chosen under a rule of indifference. CL is then a scaled version of na\"ive Bayes likelihood, the unique weight value being irrelevant to the maximum CL estimator (MCLE). To get meaningful predictive probabilities, it may be tuned empirically so as to best adjust the pseudo posterior variance matrix to the asymptotic MCLE variance matrix \cite{Pauli-11}, or via a close-in-spirit curvature adjustment \cite{Ribatet-12}, as proposed in previous attempts at Bayesian inference from composite likelihood. 

Alternatively, a simple agnostic recommendation we believe to be useful for composite weights is to sum up to one. This is motivated by a characterization theorem for opinion pooling \cite{Genest-86,Genest-86b}: the only {\em externally Bayesian} pooling operator for which the consensus probability of an outcome only depends on the agent probabilities of the same outcome, is the log-linear opinion pool with unit sum weights. The external Bayesian property essentially means that the consensus should not change if an agent opinion is taken into account by the other agents as opposed to being stated independently. Log-linear pooling with unit sum weights also turns out to be an optimal {\em compromise} in the sense that it minimizes the average inclusive Kullback-Leibler (KL) divergence to the agent opinions \cite{Garg-04}. 

Unit sum weights, however, implicitly assume maximum redundancy between features, hence tend to produce conservative predictive probabilities (see Appendix~\ref{app:frequentist}). This means low prediction confidence (over-estimated credibility sets) when features are weakly correlated, as is desirable in general. Conversely, unitary weights ($\blambda\equiv 1$) as in Na\"ive Bayes assume independent features and may thus yield over-confident predictions. An ideal method to weight features is one that can capture feature redundancy and balance it with feature relevance \cite{Peng-05}. This suggests a learning approach whenever feasible.


\subsection{Learning the weights}
\label{sec:learning}

Assuming a training dataset ${\cal D}=\{(x_k,\y_k), k=1,\ldots,N\}$ (possibly the same as for generative pre-training), the most direct way to learn the composite weights is to maximize their likelihood under the composite predictive distribution or, equivalently, minimize prediction ``cross-entropy'':
\begin{equation}
\label{eq:train_likelihood}
\max_{\blambda\succeq 0} U(\blambda),
\qquad \text{with} \quad
U(\blambda) \equiv\sum_{k=1}^N \log p_\blambda(x_k|\y_k).
\end{equation}

From~(\ref{eq:log_pool}), we see that the set of conditional distributions $p_\blambda(x|\y)$ spanned by the weights~$\blambda$ is the exponential family with natural parameter~$\blambda$ and basis functions given by the feature-based log-likelihoods:
$$
p_\blambda(x|\y) = \pi(x) \exp[\blambda^\top \bell(x,\y) - a(\blambda,y)],
$$
with:
$$
\ell_i(x,\y) \equiv \log f(z_i|x),
\qquad
a(\blambda, \y) \equiv \log \sum_{x\in{\cal X}} \pi(x) e^{\blambda^\top \bell(x,\y)}.
$$

It follows that the utility function $U(\blambda)$ in~(\ref{eq:train_likelihood}) is concave as a general property of likelihood in exponential families, hence this learning can be implemented using a standard convex optimization algorithm such as limited-memory BFGS \cite{Byrd-95}. See Appendix~\ref{app:training} for some implementation details. 

Because some positive weight constraints may turn out inactive, optimization has a natural tendency to produce sparse weights. As illustrated in Figure~\ref{fig:disc_weight_plot} on the breast cancer diagnostic UCI dataset \cite{Wolberg-94}, there is no clear relation between optimal weights and feature-level discrimination power, showing that maximum likelihood learning differs radically from univariate feature selection as it implicitly takes feature redundancy into account via joint weight optimization.

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=.7\textwidth]{disc_weight_plot.pdf}
  \end{center}
\caption{Distribution of optimal composite weights vs.~individual feature discrimination power in a binary classification task (breast cancer UCI dataset). Discrimination is measured by the maximum KL divergence between class-conditional generative distributions.}
\label{fig:disc_weight_plot}
\end{figure}

As is customary and generally good practice, the training examples may be weighted in~(\ref{eq:train_likelihood}) so that the empirical distribution of classes matches the prior~$\pi(x)$, thus enforcing balanced classes if the prior is uniform. Also, some regularization may be needed for stability: we shall in practice maximize $U(\lambda)-\alpha \|\blambda\|^2$, where $\alpha>0$ is a small damping factor.

Other training variants that we mention here but do not particularly recommend for interpretability arise from adding basis functions to the exponential family. For instance, class-dependent offsets may be added so as to learn the ``prior'' together with the composite weights. It is also possible to perform unconstrained optimization to achieve higher training likelihood, but this means that some composite weights are then negative, in violation of the monotonicity property mentionned in Section~\ref{sec:log_pool}.


\paragraph{$I$-projection interpretation.}

Exponential family properties also imply that learning via likelihood maximization~(\ref{eq:train_likelihood}) is dual to an $I$-projection \cite{Csiszar-84}: 
\begin{equation}
\label{eq:i_proj}
\min_{p\in{\cal P}} D(p\|p_0),
\qquad \text{with} \quad
p_0(x,\y) \equiv \pi(x)h(\y),
\end{equation}
subject to the constraints $p(\y)=h(\y)$ and $\E_p[\bell] \succeq \E_h[\bell]$, where $\E_h$ denotes the expectation with respect to the joint {\em empirical} distribution of targets and inputs, $h(\y)$ is the corresponding empirical marginal distribution of inputs, and ${\cal P}$ is the set of joint probability distributions on ${\cal X}\times {\cal Y}$ with ${\cal Y}\equiv\{\y_1,\ldots,\y_N\}$. Note that the problem statement implicitly approximates the true (unknown) joint distribution of $(x,\y)$ by its empirical estimate, hence the need for regularization in practice.

The marginal constraint $p(\y)=h(\y)$ implies that only the conditional distribution $p(x|\y)$ is optimized, making the $I$-projection equivalent to maximum conditional entropy \cite{BergerA-96}. The mean log-likelihood constraints $\E_p[\bell] \succeq \E_h[\bell]$ encapsulate dependences between the target and the data, and consider a model admissible if it guarantees the same average log-likelihood levels as observed separately for each feature. This is weaker a constraint than assuming that the feature generative models $f(z_i|x)$ are true: it only assumes that the {\em description length} \cite{Grunwald-07} achieved by each feature model is a truthworthy upper bound. The composite likelihood weights may be interpreted as the Lagrange multipliers associated with these description length constraints. 

An insightful analysis given in \cite{Grunwald-04} shows that $I$-projection is, under broad conditions, a minimax strategy for prediction according to the log loss. Hence, any $I$-projection is in some sense a best possible predictive model given some arbitrary knowledge exctracted from the training data, which is represented by mean-value constraints and is {\em inexact} in practice due to the finite training set. Optimized composite likelihood only differs from other maximum entropy classifiers such as multinomial logistic regression by its underlying inexact knowledge, which in this case may be interpreted in terms of feature description length.


\section{Super composite likelihood}
\label{sec:super}

So far, we have assumed that each agent is given a single weight in the opinion aggregation~(\ref{eq:log_pool}). Let us now consider a more general pooling operator where agents can be weighted depending on the target variable:
\begin{equation}
\label{eq:super_pool}
p_\blambda(x|\y) \propto \pi(x) \prod_{i=1}^n \left[\frac{\ell_i(x)}{\ell_i(x_0)}\right]^{\lambda_i(x)},    
\end{equation}
where~$x_0$ is an arbitrary fixed target value, which we call the reference, and $\blambda: {\cal X}\to \mathbb{R}_+^n$ is a weighting function here. Note that the reference weights $\lambda_i(x_0)$ play no role and can therefore be conventionally assumed to cancel. As shown in Appendix~\ref{app:hetero}, the main reason for introducing the normalization by a reference is to make pooling externally Bayesian if the weighting function sums up to one uniformly,
$$
\forall x\in{\cal X}\setminus \{x_0\},\quad
\sum_{i=1}^n \lambda_i(x) = 1,
$$
hence providing a sensible default rule to tune the weights. Note that (\ref{eq:super_pool}) amounts to log-linear pooling if the weights are target-independent as the reference is then effectless. Using the same analogy with Bayes rule as in Section~\ref{sec:bayes_rule}, we see that the quantity:
\begin{equation}
\label{eq:super_comp_lik}
L^{sc}_\blambda(x) \equiv 
\prod_{i=1}^n \left[\frac{\ell_i(x)}{\ell_i(x_0)}\right]^{\lambda_i(x)}
\end{equation} 
acts as a likelihood function at prediction time. We call the form~(\ref{eq:super_comp_lik}) super composite likelihood (SCL) owing to the ``doubly composite'' aspect of combining distinctive feature-target pairs. SCL further generalizes CL since both forms are seen to be proportional for target-independent weights. 

Conversely, if the weighting function maps each target to a unit vector ({\em i.e.}, $\forall x\in{\cal X}\setminus \{x_0\}$, $\lambda_i(x)=1$ for a single feature~$i$, and zero otherwise), then SCL turns out identical to the class-specific likelihood surrogate derived in \cite{Baggenstoss-03} from a generative model selection argument; namely, the {\em PDF projection theorem}, which in fact boils down to an $I$-projection \cite{Minka-04}. While our derivation follows a {\em discriminative} model selection approach, it is interesting to see that it includes the class-specific method as a special case, and thus sheds new light on this method.

A compelling advantage of SCL over CL is that it can deal with missing likelihood values, which happen if the feature generative distributions $f(z_i|x)$ are unknown for some targets~$x$. SCL makes it possible to assign zero weights $\lambda_i(x)$ to every feature-target pair for which $\ell_i(x)$ is unknown, yet using nonzero weights for other pairs. Likewise, the features for which the reference likelihood $\ell_i(x_0)$ is unknown receive zero weight for all target values, implying that they have no influence whatsoever on the predictive distribution.

If training data is available, the SCL weights may be learned by maximum likelihood as described for CL in Section~\ref{sec:learning}, yielding a similar convex problem. Since SCL enables to explore a wider class of predictive models than CL, it has the potential to achieve better prediction performance as long as overfitting does not happen during training (although SCL may not be asymptotically consistent unlike CL, see Appendix~\ref{app:frequentist}). The $I$-projection interpretation still holds, yet with class-specific mean-value constraints:
$$
\forall a\in{\cal X}, \forall i=1,\ldots,n,
\quad
\E_p\left[\delta_{xa}\log \frac{f(z_i|x)}{f(z_i|x_0)}\right]
\geq \E_h\left[\delta_{xa}\log \frac{f(z_i|x)}{f(z_i|x_0)}\right],
$$
where $\delta$ denotes the Kronecker delta. 




\section{Discussion}
\label{sec:discussion}

The composite Bayesian framework unifies several seemingly unrelated concepts from computational statistics and machine learning: composite likelihood \cite{Varin-11}, PDF projection \cite{Baggenstoss-03}, na\"ive Bayes. It ultimately provides a rich class of trainable predictive models that work on pre-determined data features, and is therefore suitable for ``shallow'' learning tasks, where it may compete with classical discriminative models such as logistic regression, support vector machines or random forests. Potential use cases include transfer learning \cite{Goodfellow-16}, {\em i.e.}, when features are learned automatically in a related task via deep learning, for instance.

Unlike previous attempts at Bayesian inference from incomplete statistical models that relied on generative model selection \cite{Yuan-99b,Wang-14} or asymptotic theory \cite{Pauli-11,Ribatet-12}, our construction is based on a discriminative model selection approach. Composite Bayesian models are thus explicitly taylored to prediction and cannot simulate complete data but, contrary to conventional discriminative models, they encapsulate feature generation models.

Training a composite Bayesian model involves the sequential estimation of feature generative parameters and feature relevance weights. This two-stage training is reminiscent of restricted Boltzmann machines (RBMs) \cite{Hinton-06,Fischer-14}, which are fully generative models, with the important difference that it operates on {\em disjoint} sets of parameters in the case of composite Bayesian models, while RBM training modifies the same parameters twice. Also, the generative training stage of RBMs is typically unsupervised, and it is an open research issue whether composite Bayesian models are suitable for unsupervised learning.

\begin{figure}[!ht]
\begin{center}
\subfigure[Case study: clearly categorized class-3 wine]{
\includegraphics[width=.9\textwidth]{interp_case1.pdf}}
%\hspace*{.2\textwidth}
\subfigure[Case study: atypical class-2 wine]{
\includegraphics[width=.9\textwidth]{interp_case2.pdf}}
\end{center}
\caption{Graphical representation of feature influence on wine classification in a composite Bayesian model using heteroscedastic Gaussian feature generation models and uniform prior. Bars represent feature contributions to the difference of log-composite likelihoods between the most probable and second most probable class. Dots represent $\chi^2$ feature percentiles under the most probable class; green and red colors mark values respectively smaller and larger than 95\%.}
\label{fig:interp}
\end{figure}

Interpretability is from our point of view the key advantage of composite Bayesian models. It results from their semi-generative nature, which enables to test at prediction time how consistent the input data is with each feature distribution, thereby providing the user with crucial clues to understand and validate the prediction. Moreover, since feature log-likelihoods are combined additively, it is easy to determine which features contribute most to a particular comparison of hypotheses about the target, and summarize the rational for a prediction by highlighting a few decisive features. 

This is illustrated in Figure~\ref{fig:interp} for a composite Bayesian expert fully trained (including weight optimization as in Section~\ref{sec:learning}) on a famous UCI dataset \cite{Aeberhard-92} to classify wines into 3 categories from 13~features. The top panel shows how the expert (correctly) classifies a particular wine in category~3 with near $100\%$ probability; it can be seen that the prediction is mainly based on the flavanoids concentration and the color intensity. Since both have typical values for category~3 wines, this is a clear-cut decision. In another case depicted in the bottom panel, the wine is (again, correctly) classified in category~2 with $98.9\%$ probability, however two of the main features driving the prediction (alcanality of ash and magnesium concentration) have peculiar values for category~2, while proline concentration quite strongly points to category~1. Despite high prediction confidence, this case may require further investigation.

We believe that there are many applications of statistical inference in which interpretation is the real goal while prediction performance is only an intermediate requisite. A prediction method needs to prove reliable to be relevant; but, once proven reliable, it may need to be interpreted. Composite Bayesian inference may come into play when the {\em why} question is more important than the {\em what} question.


\appendix


\section{Basic frequentist properties of composite likelihood}
\label{app:frequentist}

Let a multivariate observable variable~$\y \sim p(\y|x_\star)$ distributed according to some target value~$x_\star$. For any hypothetical target value~$x$ and any extracted feature $z_i\sim f(z_i|x)$, the following double inequality holds:
$$
0 \leq
E\left[
\log \frac{f(z_i|x_\star)}{f(z_i|x)}
\right]
\leq
E\left[
\log \frac{p(\y|x_\star)}{p(\y|x)}
\right],
$$
where the expectation is taken with respect to the true distribution $p(\y|x_\star)$ at fixed~$x_\star$. This fact follows from two basic properties of KL~divergence: positivity and partition inequality.

Using a weighted sum of such inequalities, we can bracket the expected variations of the logarithm of any composite likelihood function~(\ref{eq:comp_lik}) using positive weights~$\blambda\succeq 0$:
\begin{equation}
\label{eq:variation_bound}
0 \leq
E\left[ \log \frac{L_c(x_\star, \blambda)}{L_c(x,\blambda)} \right]
\leq 
\|\blambda\|_1 E\left[ \log \frac{L(x_\star)}{L(x)} \right]
,
\end{equation}
where $\|\blambda\|_1 =\sum_i \lambda_i$ and $L(x)\equiv \log p(\y|x)$ is the true likelihood function. This trivially implies two general asymptotic properties of composite likelihood:
\begin{itemize}
\item {\em Weak consistency.} The expected composite log-likelihood is maximized by~$x_\star$.
\item {\em Conservativeness.} If the weights sum up to one or less ($\|\blambda\|_1\leq 1$), composite likelihood ratios of the true target~$x_\star$ vs.~other target values~$x$ tend to  underestimate true likelihood ratios.
\end{itemize}

Note that the derivation assumes that the weights~$\blambda$ are independent from the target~$x$, therefore these properties do not necessarily extend to SCL.


\section{Conditional composite likelihood}
\label{app:conditional}

As a straightforward  extension of marginal CL, each feature-based likelihood may be conditioned by an additional ``independent'' feature $\nu_i(\y)$ considered as a predictor of the ``dependent'' feature, $z_i(\y)$, yielding the more general form:
\begin{equation}
\label{eq:cond_feat_lik}
\ell_i(x) = f(z_i|x,\nu_i).
\end{equation}

Conditioning may be useful if it is believed that $\nu_i$ alone provides little or no information about~$x$, but is informative when considered jointly with~$z_i$, as in the case of regression covariates, for instance. Equation~(\ref{eq:comp_lik}) then amounts to conditional CL \cite{Varin-11}, a more general form of CL which also includes Besag's historical {\em pseudo-likelihood} \cite{Besag-74} developed for image segmentation.


\section{Composite likelihood training}
\label{app:training}

Using the same notation as in Section~\ref{sec:learning}, the likelihood function in~(\ref{eq:train_likelihood}) can be expanded as follows:
$$
U(\blambda) 
= \sum_{k=1}^N \left[
\log \pi(x_k) + \blambda^\top \bell(x_k, \y_k) - a(\blambda,\y_k)
\right]
$$

Maximizing $U(\blambda)$ is therefore equivalent to maximizing: 
$$
\psi(\blambda) \equiv \blambda^\top \bar{\bell} - \bar{a}(\blambda), 
$$
with:
$$
\bar{\bell} \equiv \frac{1}{N} \sum_k \bell(x_k,\y_k),
\qquad
\bar{a}(\blambda) \equiv \frac{1}{N} \sum_k a(\blambda,\y_k).
$$

Note that $\psi(\blambda)$ is nothing but the dual function associated with the $I$-projection problem~(\ref{eq:i_proj}). The derivatives of~$\psi(\blambda)$ are found to be:
\begin{eqnarray*}
\nabla\psi(\blambda)
 & = & \bar{\bell} - \frac{1}{N} \sum_k \nabla a(\blambda,\y_k), \\
\nabla\nabla^\top\psi(\blambda)
 & = & - \frac{1}{N} \sum_k \nabla \nabla^\top a(\blambda,\y_k),
\end{eqnarray*}
with:
$$
\nabla a(\blambda,\y) = \E_{\blambda}(\bell),
\qquad
\nabla \nabla^\top a(\blambda,\y) = {\rm Var}_{\blambda}(\bell),
$$
where $\E_{\blambda}$ and ${\rm Var}_{\blambda}$ respectively denote the expectation and variance with respect to $p_\blambda(x|\y)\propto \pi(x)\exp[\blambda^\top \bell(x,\y)]$ at fixed~$\y$. This shows that $a(\blambda,\y)$ is convex in $\blambda$ since the variance matrix is positive, which, in turn, proves the concavity of~$\psi$.



\section{Heterogeneous log-linear pooling}
\label{app:hetero}

Consider a sequence of probability distributions $p_i(x)$, $i=1,\ldots,n$, representing multiple opinions on a variable $x\in{\cal X}$. The general goal of opinion aggregation is to define an operator $F(p_1,\ldots,p_n)$ that takes opinions as inputs and outputs a single distribution $p(x)$ representing a consensus. Log-linear pooling \cite{Genest-86} is one particular such operator:
$$
p(x)\propto \pi(x) \prod_{i=1}^n p_i(x)^{\lambda_i},
$$
where $\pi(x)$ is an arbitrary distribution and $\lambda_i\in\mathbb{R}$ are arbitrary weights. It is easily seen that, if the weights sum up to one, log-linear pooling is externally Bayesian in the sense that, for any nonvanishing positive valued function $q:{\cal X}\to \mathbb{R}^\star_+$, opinion pooling and opinion updating commute:
$$
F[N(q p_1), \ldots, N(q p_n)]
=
N[q F(p_1,\ldots, p_n)],
$$
where~$N$ denotes the normalization endomorphism of $\mathbb{R}_+^{\cal X}$:
$$
N(f)(x) = \frac{f(x)}{\sum_{x'\in{\cal X}} f(x')}
.
$$

Conversely, \cite{Genest-86b} proved that any externally Bayesian operator for which $F(p)(x)$ is a function of $x,p_1(x),\ldots,p_n(x)$, is of the above form with unit sum weights and is therefore a log-linear opinion pool. 

A question that arises naturally is whether it is possible to weight opinions depending on targets, so that $\lambda_i(x)$ becomes a function of~$x$. This would make it possible, for instance, to consider an agent reliable when expressing an opinion about a certain hypothesis~$x_1$, but unreliable when it comes to another hypothesis~$x_2$. As a first try, let us consider a straightforward extension of log-linear pooling:
$$
p(x)\propto \pi(x) \prod_i p_i(x)^{\lambda_i(x)},
$$
where $\lambda_i : {\cal X}\to\mathbb{R}$ is some weighting function. Unfortunately, this operator is generally not externally Bayesian because $F[N(q p_1), \ldots, N(q p_n)](x)
\propto r(x) F(p_1,\ldots, p_n)(x)$ with:
$$
r(x) = q(x)^{\sum_i \lambda_i(x)} \prod_i z_i^{-\lambda_i(x)},
\qquad 
z_i = \sum_{x\in{\cal X}} q(x) p_i(x),
$$
and $r(x)$ cannot be made proportional to $q(x)$, except with constant weights, because the normalizing factors~$z_i$ may differ from one agent to the other.

Consider, however, the following alternative:
\begin{equation}
\label{eq:hetero_log_pool}
p(x) \propto \pi(x) \prod_i \left[\frac{p_i(x)}{p_i(x_0)}\right]^{\lambda_i(x)},    
\end{equation}
where $x_0$ is some arbitrary reference value. This does the trick because, for any positive function $q(x)$, the ratio $q(x)p_i(x)/q(x_0)p_i(x_0)$ is left unchanged by normalization of $q p_i$ since the normalizing factors are the same at both the numerator and the numerator, hence they cancel out, yielding:
$$
F[N(q p_1), \ldots, N(q p_n)](x)
\propto
\left[\frac{q(x)}{q(x_0)}\right]^{\sum_i \lambda_i(x)}
F(p_1, \ldots, p_n)(x),
$$
which shows that~(\ref{eq:hetero_log_pool}) is externally Bayesian whenever the weighting function verifies:
$$
\forall x \in {\cal X}\setminus \{x_0\},
\qquad
\sum_i \lambda_i(x) = 1.
$$

The operator~(\ref{eq:hetero_log_pool}) generalizes standard log-linear pooling and has the property that the consensus $F(p)(x)$ is a function of $x,p_1(x),\ldots,p_n(x)$ but also $p_1(x_0),\ldots,p_n(x_0)$. The ``trick'' to enable heterogeneous (target-dependent) weights is essentially to consider the odds relative to a fixed reference.

Caca.
$$
\log p(x) \equiv \mu(x) + \sum_i \lambda_i(x) \log p_i(x)
$$

If we look for a pool of the form:
$$
\log p(x) \equiv \mu(x) + \sum_i \lambda_i(x) \log p_i(x)
$$






%\bibliographystyle{ieeetr}
\bibliographystyle{abbrv}
\bibliography{cvis,stat,alexis}

%%\input{draft.biblio}

\end{document}

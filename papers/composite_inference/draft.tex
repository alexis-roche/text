\documentclass[english]{scrartcl}

\usepackage{fullpage}
\usepackage{amstext,amssymb,amsmath,amsthm}
\usepackage{graphicx,subfigure}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{color}

\graphicspath{{.}{pics/}}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{proposition}[theorem]{Proposition}


\title{Composite Bayesian inference}
\date{}
\author{Alexis Roche\thanks{\url{alexis.roche@centraliens.net}}}

%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}
%\newcommand{\matphi}{\boldsymbol{\Phi}} 
%\def\x{{\mathbf{x}}}
%\def\z{{\mathbf{z}}}
%\def\u{{\mathbf{u}}}
%\def\p{{\bar{\mathbf{p}}}}
%\def\q{{\bar{\mathbf{q}}}}



\begin{document}

\maketitle

\begin{abstract}
This note revisits the concept of composite likelihood from the perspective of probabilistic inference, and advocates a machine learning approach to tune the associated ``weights'', which stems naturally from a connection with the maximum entropy principle: the predictive distribution that maximizes entropy relative to a given prior and subject to multiple mean log-likelihood constraints is, up to a normalizing factor, the prior multiplied by a particular composite likelihood function, hence providing a ``composite'' extension of Bayes rule. We argue that composite Bayesian inference is a middle way between generative and discriminative approaches to statistical inference, and can be powerful in shallow learning problems, {\em e.g.}, in the context of transfer learning.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Frequentist and Bayesian inference are based on the concept of likelihood, which relies on the existence of a statistical data-generating model that is both experimentally plausible and computationally tractable. Because this is challenging for complex data, other paradigms are commonly used in applied science: deliberately misspecified generative models, as in quasi-likelihood methods \cite{White-82,Walker-13} or na\"ive Bayes \cite{Ng-01}, the minimum description length principle \cite{Grunwald-07} and, last but not least, discriminative models, which currently dominate the field of artificial intelligence and typically require supervised learning on large datasets -- these include a variety of deep learning \cite{Lecun-15,Goodfellow-16} and shallow learning techniques such as maximum entropy classifiers \cite{BergerA-96}, support vector machines \cite{Vapnik-00} and Gaussian processes \cite{Rasmussen-06}.

Composite likelihood (see \cite{Varin-11} and the references therein) is a semi-generative approach that extends the familiar notion of likelihood without requiring a full data-generating model. The key idea is to model an arbitrary set of low-dimensional features separately and then combine them, instead of modeling the data distribution as a whole. This may be viewed as a ``divide and conquer'' method to approximate the true but intractable likelihood. While maximum composite likelihood does not inherit the general property of maximum likelihood to yield asymptotically minimum-variance estimators, it may offer an excellent trade-off between computational and statistical efficiency.

In this note, composite likelihood is interpreted as a probabilistic opinion pool of ``agents'' making use of different pieces of information, or clues, extracted from the input data. Each agent acts as an isolated generative model-based statistician and expresses an opinion based on a specific clue in the form of a likelihood of the hidden variables. The opinion pool then compuptes a probability distribution on the hidden variables analogous to a Bayesian posterior, thereby extending Bayesian inference to problems where the complete likelihood is undefined.

We further show that a particular log-linear opinion pool yields the best possible predictive distribution in the sense of maximum entropy \cite{Grunwald-04}. This perspective entails a method to tune the weights of the different agents from training data as in a classical discriminative learning scenario.


\section{Composite likelihood as opinion pooling}
\label{sec:pool}

Let $Y$ an observable multivariate random variable with sampling distribution $p(y|x)$ conditional on some unobserved variable of interest, $X\in{\cal X}$, where ${\cal X}$ is a known set. Given an experimental outcome $y$, the likelihood is the sampling distribution evaluated at $y$, seen as a function of $x$:
$$
L(x) = p(y|x)
.
$$

This requires a plausible generative model which, for complex data, may be out of reach or involve too many nuisance parameters. A natural workaround known as {\em data reduction} is to extract some lower-dimensional representation $z=f(y)$, where $f$ is a many-to-one mapping, and consider the more convenient likelihood function:
$$
\ell(x) = p(z|x)
.
$$

Substituting $L(x)$ with $\ell(x)$ boils down to restricting the sample space, thereby  ``delegating'' statistical inference to an ``agent'' provided with partial information. While it is valid for such an agent observing~$z$ only to consider $\ell(x)$ as the likelihood function of the problem, the drawback is that $\ell(x)$ might yield too vague a prediction of~$X$ due to the information loss incurred by data reduction. To make the trick statistically more efficient, we may extract several features, $z_i=f_i(y)$ for $i=1,2,\ldots,n$, and try to combine the likelihood functions $\ell_i(x) = p(z_i|x)$ that they elicit.

If we regard the likelihoods as posterior distributions corresponding to uniform priors, this is a classical problem of probabilistic opinion aggregation from possibly redundant sources, for which several methods exist in the literature \cite{Tarantola-82,Genest-86,Garg-04,Allard-12}. We will show in Section~\ref{sec:maxent} that one which is particularly well-suited to our case is the {\em log-linear opinion pool}: 
\begin{equation}
\label{eq:log_pool}
p_\lambda(x|y) = \frac{1}{Z_\lambda(y)} \pi(x) \prod_{i=1}^n p(z_i|x)^{\lambda_i},
\end{equation} 
where $\pi(x)$ is some reference distribution or prior, and $\lambda=(\lambda_1,\ldots,\lambda_n)$ is a vector of real-valued weights so that the normalizing factor:
$$
Z_\lambda(y) = \int \pi(x) \prod_{i=1}^n p(z_i|x)^{\lambda_i} dx
$$
is finite\footnote{It is possible to have the weights depend on~$y$, however there is a strong rational for constant weights as we shall see in the sequel.}. 

The log-linear pool~(\ref{eq:log_pool}) bears a striking similarity to Bayes rule, yielding the form: $p_\lambda(x|y)\propto \pi(x) L^c_\lambda(x)$, where the quantity:
\begin{equation}
\label{eq:comp_lik}
L^c_\lambda(x) \equiv \prod_{i=1}^n \ell_i (x)^{\lambda_i}
\end{equation} 
plays the same role as a traditional likelihood function. We recognize in~(\ref{eq:comp_lik}) a general expression known as {\em marginal composite likelihood} \cite{Varin-11}. It shares a convenient factorized form with the likelihood derived under the assumption of mutual feature independence, sometimes called {\em na\"ive Bayes} likelihood in the literature \cite{Ng-01}, which corresponds to the special case of all unitary weights, $\lambda_1=\ldots=\lambda_n= 1$. 
The general computational advantage of composite likelihood over the true likelihood is that it only requires to evaluate the marginal feature distributions rather than the joint distribution of all features. 


\section{The problem of tuning composite likelihood weights}

There exist few general guidelines to choose the composite likelihood weights~$\lambda$ in~(\ref{eq:comp_lik}). Common practice is to impose nonnegative weights \cite{Varin-11} although this is not strictly required from the opinion aggregation viewpoint (the normalizing factor $Z_\lambda(y)$ in~(\ref{eq:log_pool}) only needs be finite for any~$y$ for the log-linear pool to yield a proper conditional distribution). 

In cases where the features can be considered to be {\em exchangeable}, it is natural to pick uniform weights. The composite likelihood function is then a scaled version of the {\em na\"ive Bayes} likelihood, the common weight value being irrelevant to the location of the composite likelihood maximizer. It can be tuned so as to best adjust the pseudo posterior variance matrix to the asymptotic variance matrix of the maximum composite likelihood estimator \cite{Pauli-11}, or via a close-in-spirit curvature adjustment \cite{Ribatet-12}, in attempts to reconcile the frequentist and Bayesian notions of uncertainty.

Another common recommandation for the composite likelihood weights is to sum up to one. 

Going back to the log-linear pool underlying composite likelihood, Genest {\em et al} \cite{Genest-86b} showed that the only pooling operator that does not explicitly depend on~$x$ and preserves a property known as {\em external Bayesianity} is the log-linear opinion pool~(\ref{eq:log_pool}) with the additional constraint that the weights  sum up to one. External Bayesianity essentially means that it should not matter whether a prior on~$X$ is incorporated before or after combining opinions, provided that all agents agree on the same prior. Garg {\em et al} \cite{Garg-04} showed that the log-linear pool with unit weight sum also minimizes the average Kullback-Leibler (KL) divergence to the agent opinions.
The MaxEnt argument was already given by \cite{Wang-14}.


%Instead, redundancy between agents is assumed by default, and is effectively encoded by the unit sum constraint on weights\footnote{Nevertheless, features which are {\em known} to be mutually independent can be merged into a single feature. This results in increasing their weights in the log-linear pool.}.

The log-linear pool has, in particular, the 0/1 forcing property, meaning that if any agent assigns zero likelihood to a value~$x$, then $x$ has zero probability in the pool. 

Unit sum weights = crap. 

\section{Maximum entropy derivation of composite likelihood}
\label{sec:maxent}

The coordinator discards the priors used by the agents. 

The MaxEnt argument was already given by \cite{Wang-14}. 

% Accounting here for Xi'An comment on xianblog.wordpress.com
% The sum of the powers is constrained to be equal to one, even though
% I do not understand why the dimensions of the projections play no
% role in this constraint. Simplicity is advanced as an argument,
% which sounds rather weak…
This simple constraint is implied by the external Bayesianity requirement: as it turns out, the only aggregation operator which is both externally Bayesian and independent from $x$ boils down to plugging a composite likelihood with unit sum weights into Bayes rule, hence extending the classical notion of likelihood in Bayesian analysis.


\section{Composite likelihood as message approximation}
\label{sec:message}

Composite likelihood may also be understood as a means to approximate the ``true'' likelihood function\footnote{By {\em true likelihood}, we mean the likelihood corresponding to a specified yet intractable generative model. In practice, such a model is obviously not required.} or, in the language of graphical models, the {\em message} that the data sends to the latent variable~$x$. Several ``clues'' are sent to~$x$ via different data features, and then integrated assuming {\em non-coalescent} emitting sources, meaning that statistical dependences between clues are treated as unknown, although not ignored. The whole idea is depicted by the factor graph\footnote{See \cite{Bishop-06} for a didactic introduction to factor graphs.} in Figure~\ref{fig:fgraph2}, where the latent variable is connected to multiple factors involving single clues (rather than a single factor involving multiple clues), thereby enabling efficient computations.

%A simplified message is computed by extracting arbitrary features from the data and treating them as {\em non-coalescent} clues in the sense that their statistical dependences are not modeled (although not ignored). 

\begin{figure}[!ht]
\begin{center}
\subfigure[Generative model]{\includegraphics[width=.35\textwidth]{fgraph1.pdf}\label{fig:fgraph1}}
\subfigure[Composite likelihood model]{\includegraphics[width=.6\textwidth]{fgraph2.pdf}\label{fig:fgraph2}}
\caption{Factor graph representations of a generative model (a) and its approximation using composite likelihood (b). In (a), the factor connecting the data~$y$ and the latent variable~$x$ is the true likelihood, $p(y|x)$. In (b), factors connecting~$y$ and the clues $z_1,z_2,\ldots$ represent feature extractions, $\alpha_i(y,z_i)=\delta[z_i-f_i(y)]$, while factors connecting the clues and~$x$ are scaled single-feature likelihoods, $\beta_i(z_i,x)=p(z_i|x)^{w_i}$. In both graphs, the factor shown in black is the prior $\pi(x)$.}
\label{fig:fgraph}
\end{center}
\end{figure}

This approximation scheme happens to be optimal in an information-theoretic sense. As noted in \cite{Garg-04}, the log-linear pool minimizes the average Kullback-Leibler (KL) divergence to the probabilistic opinions:
\begin{equation}
\label{eq:avg_kl}
p_\star = \arg\min_p \sum_{i=1}^n w_i D(p\|p_i),
\end{equation}
where $p_i(\theta) \propto \pi(\theta)\ell_i(\theta)$ is the posterior distribution of agent~$i$. Note that, even though the weights are arbitrary in~(\ref{eq:avg_kl}), the optimal solution normalizes them to unit sum. Composite likelihood thus arises as the best possible consensus among agents according to a natural criterion, in addition to satisfying the external Bayesianity axiom. 

If one interprets the average divergence from agent opinions~(\ref{eq:avg_kl}) as a proxy for the divergence from ``the truth'', $p(x|y)$, composite likelihood can be seen as a variational approximation to the true likelihood. This corresponds to the intuitive notion that a consensus among sufficiently many experts should yield a reasonable guess. An essential difference with usual approximate inference methods \cite{Bishop-06,Minka-05} is that the likelihood function does not need to be computable owing to the use of a proxy. However, as we shall see in Section~\ref{sec:weights}, knowledge of the feature sampling distributions can be further exploited to optimize the likelihood approximation with respect to the composite likelihood weights.



\section{Conditional composite likelihood}
\label{sec:conditional}

The SCL derivation rests upon the definition of feature-based likelihood as $\ell_i(x)=p(z_i|x)$. As a straightforward  extension, $\ell_i(x)$ may be conditioned by an additional ``independent'' feature $z^c_i = f^c_i(y)$ considered as a predictor of the ``dependent'' feature, $z_i=f_i(y)$, yielding the more general form:
\begin{equation}
\label{eq:cond_feat_lik}
\ell_i(x) = p(z_i|x,z^c_i).
\end{equation}

Conditioning may be useful if it is believed that $z^c_i$ alone is little or not informative about $x$, but can provide relevant information when considered jointly with $x$, as in the case of regression covariates, for instance. Standard composite likelihood (\ref{eq:comp_lik}) then amounts to {\em conditional composite likelihood} \cite{Varin-11}, a more general form of composite likelihood also including Besag's historical {\em pseudo-likelihood} \cite{Besag-74}, which was a major breakthrough in computer vision. 

Likewise, the above derivation remains valid when ``independent'' features are used, and we can thus define a conditional version of SCL by plugging likelihood functions of the form~(\ref{eq:cond_feat_lik}) into (\ref{eq:super}).




\section{Discussion}
\label{sec:discussion}

Composite likelihood is a relatively recent concept from computational statistics that has mainly been developed so far in a frequentist perspective as a surrogate for the maximum likelihood method. In this paper, we have shown (to our best knowledge, for the first time) a deep connection between composite likelihood and  probabilistic opinion pooling, thereby establishing composite likelihood as a class of {\em discriminative models}\footnote{We here define a discriminative model as a model of which the parameters describe the conditional distribution of an unobserved variable of interest given an observable variable, as opposed to a {\em generative} model, which involves parameters encoding for the conditional distribution of an observable given an unobserved variable.} for statistical inference and machine learning. This connection is possible under the mild restriction that composite likelihood weights are chosen to sum up to one.

\subsection{From na\"ive Bayes to super composite likelihood}

In the probabilistic opinion pooling perspective, composite likelihood is essentially a reinterpretation and a generalization of the {\em na\"ive Bayes} paradigm that relaxes the associated ``na\"ive'' mutual feature independence assumption. In particular, when all features are given equal weight, the composite likelihood is the na\"ive Bayes likelihood raised to the power~$1/n$, where~$n$ is the number of selected features, yielding for instance the same maximum a posteriori (MAP) estimator if a flat prior is used. However, beside providing a simple justification to the wide use of na\"ive Bayes MAP algorithms,  composite likelihood using unit sum weights also entails a conservative rescaling of credibility sets derived from na\"ive Bayes, which gets more drastic as the number of features increases. This comes as a consequence of relaxing feature independence.

We further argued that this rescaling may, to some extent, be unduly conservative in multiclass problems if features are weighted uniformly over the space of unobserved labels, leading us to propose the more general concept of super composite likelihood (SCL). SCL essentially approximates likelihood ratios relative to a fixed reference hypothesis using locally weighted composite likelihood functions. Owing to weight adaptability, SCL describes a more general class of discriminative models than standard composite likelihood. 

The idea can also be understood in terms of approximating the factor graph in Figure~\ref{fig:fgraph3}, which encodes the true but intractable posterior distribution, by another factor graph of the type depicted in Figure~\ref{fig:fgraph4}. This substitution results in breaking into pieces both the observed and unobserved variable spaces, and assembling a series of concise {\em messages} passing from the former to the latter. Note that this is a pretty unusual case of approximate inference in factor graphs where the factors to be approximated are unknown (or need not be known).

\subsection{Super composite likelihood training}

Any SCL~model is fully determined by the marginal generative distributions of some pre-specified features, which may rely on a moderate number of parameters if low-dimensional features are chosen. There are two approaches to deal with these parameters.

One is to estimate them beforehand by supervised learning, if an adequate training dataset is available. This could be compared with contrastive pre-training of RBMs \cite{Hinton-06,Fischer-14}, which also optimizes parameters for generation of observable features. An important difference, however, is that contrastive pre-training is unsupervised. On the other hand, SCL pre-training relies on weaker assumptions as it does not assume conditional feature independence unlike RBMs. Once pre-training is complete, the SCL~weights may be tuned by maximum~SCL, as shown in Section~\ref{sec:weights}: this second training stage is, again, supervised in essence, but does not require additional training examples since it is fully determined by the feature distributions learned in the pre-training step.

%{\color{red} Moreover, while RBM parameters are typically refined in a supervised discriminative learning step, disjoint set of parameters for SCL. Dunno how to say that.} 

%In such context, SCL competes with classical discriminative models (logistic regression, Gaussian processes \cite{Rasmussen-06}, maximum entropy models \cite{BergerA-96}, etc.), and may compare more or less favorably in practice depending on the amount of training data. For relatively small training datasets, we may hope for more accurate inference using~SCL than using traditional discriminative models, extrapolating from the results of \cite{Ng-01} regarding the comparison between logistic regression and na\"ive Bayes classifiers.
%{\color{red}Ici, il manque la comparaison avec les RBMs qui ne sont pas (forcement) des modeles discriminatifs.}

\begin{figure}[!ht]
\begin{center}
\subfigure[Generative model]{\includegraphics[width=.25\textwidth]{dgraph1.pdf}\label{fig:dgraph1}}
\hspace*{.05\textwidth}
\subfigure[Classical discriminative model]{\includegraphics[width=.25\textwidth]{dgraph2.pdf}\label{fig:dgraph2}}
\hspace*{.05\textwidth}
\subfigure[Composite likelihood model]{\includegraphics[width=.25\textwidth]{dgraph3.pdf}\label{fig:dgraph3}}
\caption{Belief networks representing, respectively: (a) a generative model, (b) a classical discriminative model, and (c) a composite likelihood model. Note the marginal independence between the data~$y$ and the nuisance parameter~$\psi$ in (b).}
\label{fig:dgraph}
\end{center}
\end{figure}

%%, hence alleviating the need for heavily supervised model training

The other approach to deal with feature distribution parameters is to consider them as nuisance parameters, as proposed in Section~\ref{sec:nuisance_params}, thereby avoiding pre-training. The method then becomes completely unsupervised. The weights can be tuned by maximizing the log-SCL integrated over the prior on nuisance parameters (see Section~\ref{sec:nuisance_weights}), which requires no training dataset whatsoever, and the nuisance parameters can be eliminated by substituting the~SCL function with a {\em super composite evidence} (see Section~\ref{sec:nuisance_integration}). In this version, SCL~essentially generalizes Bayesian integration. 

The potential of SCL for unsupervised learning is perhaps suprising considering that it is a discriminative model. This stems from the fact that, unlike classical discriminative models, SCL~exploits direct information conveyed by the data about both the parameters of interest and the nuisance parameters, reflecting the generative nature of the underlying feature distribution models. This essential difference is illustrated in Figure~\ref{fig:dgraph}: the factor graph model underlying composite likelihood in its parametric version (see Figure~\ref{fig:fgraph5}) is not reducible to a belief network of the type represented in Figure~\ref{fig:dgraph2}, in which the data and the nuisance parameter are marginally independent. In contrast, the data is informative about the nuisance parameter in a composite likelihood model, as shown in Figure~\ref{fig:dgraph3}.


\section{Conclusion}
\label{sec:conclusion}

In summary, (super) composite likelihood has the potential to yield weakly supervised or unsupervised Bayesian-like inference procedures depending on the particular task at hand. This property reflects the encoding of statistical relationships between the data and {\em all} unknown parameters. Composite likelihood thus appears as a trade-off between generative models, which are optimal for unsupervised learning but possibly intractable, and traditional discriminative models (logistic regression, Gaussian processes \cite{Rasmussen-06}, maximum entropy models \cite{BergerA-96}, etc.), which are inherently supervised. Composite likelihood models are discriminative models assembled from atomic generative models and, from this point of view, may be considered as {\em semi-generative} models.



%composite likelihood may be considered as a {\em semi-generative} model: a discriminative model assembled from partial generative models.

%As a note of caution, we shall stress that the pre-determined weights assigned to the different associations between observed and unobserved values represent prior knowledge regarding the informativeness of clues. A poor choice of weights will inevitably result in a poor approximation to the ``true'' Bayesian posterior -- the posterior that would be obtained from a realistic generative model if it was tractable. In future work, we will investigate feature selection strategies to mitigate this problem.

% Improve the discussion on following aspects:
% ** Why is it compatible with unsupervised learning? Give more insight.
% ** Stress the contribution: class-specific weighting.
% * Pivotality argument.
% * Bayes is a special case of composite Bayes.

%{\color{red} {\em Mardi 12 avril 2016.} Je laisse reposer la version actuelle 3-4 semaines avant mise \`a jour sur arxiv. La plupart des points qui restaient obscurs en f\'evrier ont, me semble-t-il, \'et\'e lev\'es: lien avec les neural networks/RBM, strat\'egies d'apprentissage, optimisation des poids, int\'er\^et du super composite (multi-class problems). Sur l'optimisation des poids, il reste une petite zone d'ombre: faut-il r\'egulariser d'apprentissage de mani\`ere \`a \'eviter une pond\'eration \'eparse? L'argument pour serait d'acc\'el\'erer la convergence stochastique de la SCL vers l'utilit\'e th\'eorique. On en parle d\'ej\`a un peu dans la version actuelle en disant que si 2 attributs sont aussi informatifs l'un que l'autre, alors le mieux est de leur donner le m\^eme poids. Plus g\'en\'eralement, on pourrait aborder cet aspect via: 1) des contraintes de r\'epartition ``d\'emocratique'' des poids pour \'eviter une situation ``winner-take-all'' (eg, avec une p\'enalisation quadratique des poids); ou alors, 2) des contraintes de variations impos\'ees aux colomnes de la matrice de poids, ce qui donnerait une esp\`ece de compromis entre CL classique et SCL... mais je ne suis pas certain de l'int\'er\^et.}


\appendix

\section{Data reduction inequality}
\label{sec:reduction_inequality}

A fundamental property of the Kullback-Leibler divergence is that it decreases under feature extraction, {\em i.e.}, application of a deterministic transformation. This comes as a consequence of the logarithmic sum inequality \cite{Cover-06} (and also follows as a straightforward corollary of the PDF~projection theorem \cite{Minka-04,Baggenstoss-15}). 

The proof is elementary and can be sketched as follows. Let $Z=f(Y)$ some feature extracted from a variable~$Y\sim p(y)$. Given an arbitrary distribution~$\pi(y)$, let  $\tilde{p}(z)$ and $\tilde{\pi}(z)$ the distributions induced on~$Z$ by $p(y)$ and $\pi(y)$, respectively. For any potential value~$z$ of~$Z$, consider the level set: $\Gamma(z)=\{y, f(y)=z\}$. Under conditions of existence, we can partition the integral involved in the KL~divergence $D(p\|\pi)$ using these level sets: 
$$
\int p(y) \log \frac{p(y)}{\pi(y)} dy
=
\int \left( \int_{\Gamma(z)} p(y) \log \frac{p(y)}{\pi(y)} dy \right) dz\\
.
$$

Applying the logarithmic sum inequality \cite{Cover-06} to each integral over~$\Gamma(z)$, we then readily get:
$$
\int_{\Gamma(z)} p(y) \log \frac{p(y)}{\pi(y)} dy
\geq 
\tilde{p}(z) \log \frac{\tilde{p}(z)}{\tilde{\pi}(z)}
,
$$
owing to the fact that $\displaystyle \int_{\Gamma(z)} p(y) dy = \tilde{p}(z)$ by definition. Therefore,
$$
\int \tilde{p}(z) \log \frac{\tilde{p}(z)}{\tilde{\pi}(z)} dz
\leq 
\int p(y) \log \frac{p(y)}{\pi(y)} dy
,
$$
or, in a more compact way, $D(\tilde{p}\|\tilde{\pi})\leq D(p\|\pi)$. The case of equality occurs if, and only if, $p(y)/\pi(y)=\tilde{p}(z)/\tilde{\pi}(z)$, in other words, if~$z$ is a sufficient statistic for the ``alternative'' hypothesis~$H_1:y\sim p(y)$ versus the ``null'' hypothesis~$H_0:y\sim\pi(y)$. 

Since it is a well-known fact that $D(\tilde{p}\|\tilde{\pi})\geq 0$ as any KL~divergence, we conclude that:
\begin{equation}
\label{eq:reduction_inequality}
0 
\leq D(\tilde{p}\|\tilde{\pi}) 
\leq D(p\|\pi)
.
\end{equation}


\section{Basic asymptotic properties of composite likelihood}
\label{sec:asymptotic}

It follows from the double inequality~(\ref{eq:reduction_inequality}) that, for any extracted feature~$z_i$ and for any hypothesis~$\theta$,
$$
0 \leq
E\left[
\log \frac{p(z_i|\theta_\star)}{p(z_i|\theta)}
\right]
\leq
E\left[
\log \frac{p(y|\theta_\star)}{p(y|\theta)}
\right],
$$
where the expectation is taken with respect to the true distribution $p(y|\theta_\star)$. Using a weighted sum of such inequalities, we can bracket the expected variations of the logarithm of any standard composite likelihood function (assuming unit sum positive weights):
\begin{equation}
\label{eq:variation_bound}
0 \leq
E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta, \mathbf{w})} \right]
\leq 
E\left[ \log \frac{L(\theta_\star)}{L(\theta)} \right]
.
\end{equation}

This implies two asymptotic properties of standard composite likelihood: 
\begin{itemize}
\item {\em Consistency.} The expected log-composite likelihood is maximized by~$\theta_\star$, the true value of~$\theta$.
\item {\em Conservativeness.} The expected log-composite likelihood function is ``flatter'' than the true expected log-likelihood. In other words, composite likelihood ratios $L_c(\theta, \mathbf{w})/L_c(\theta_\star, \mathbf{w})$ relative to~$\theta_\star$ tend to be larger than the corresponding true likelihood ratios. 
\end{itemize}

These properties do not necessarily extend to the more general case of SCL. We may state a weaker consistency property by considering the upper envelope of expected log-composite likelihood ratios:
$$
M(\theta) = \max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right],
$$
where ${\cal W}$ is the $n$-dimensional simplex. Clearly, the expected logarithm of the SCL is upper bounded by~$M(\theta)$:
$$
E[\log {\cal L}_c(\theta, \mathbf{W})] \leq M(\theta)
.
$$

Moreover, $M(\theta)$ is maximized by $\theta_\star$ since (\ref{eq:variation_bound}) implies that, for any $(\theta,\theta_0)$,
$$
E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
\leq
E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right],
$$
which remains true when taking the maximum over the weights in both sides: 
$$
M(\theta) = 
\max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
\leq
\max_{\mathbf{w}\in{\cal W}} E\left[ \log \frac{L_c(\theta_\star, \mathbf{w})}{L_c(\theta_0, \mathbf{w})} \right]
= M(\theta_\star)
.
$$

Therefore, maximizing SCL corresponds to maximizing a lower bound on~$M(\theta)$, which can be considered as an objective function since it is maximized by~$\theta_\star$ (like the expected log-likelihood). Lower bound maximization guarantees a certain objective value, however not the maximum, hence asymptotic consistency may not hold.


\bibliographystyle{abbrv}
\bibliography{cvis,stat,alexis}

%%\input{draft.biblio}

\end{document}

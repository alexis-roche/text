\documentclass[english]{scrartcl}

\usepackage{fullpage}
\usepackage{amstext,amssymb,amsmath,amsthm}
\usepackage{graphicx,subfigure}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{color}

\graphicspath{{.}{pics/}}

\title{Composite Bayesian inference in hindsight}
%\date{}
\author{Alexis Roche\thanks{\url{alexis.roche@centraliens.net}}}

\def\x{{\mathbf{x}}}
\def\y{{\mathbf{y}}}
\def\z{{\mathbf{z}}}
\newcommand{\blambda}{{\boldsymbol{\lambda}}}
\newcommand{\Blambda}{{\boldsymbol{\Lambda}}}
\newcommand{\bell}{{\boldsymbol{\ell}}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\isep}{\mathrel{{.}\,{.}}\nobreak}


\begin{document}

\maketitle

\section{Introduction}

In the years 2015--2019, I worked on an attempt to put forward a slightly new statistical inference paradigm, which I called composite Bayesian inference.

Composite Bayesian inference is a new perspective on the ``naive Bayes'' method that justifies it without relying on implausible independence assumptions. It also comes with a posterior distribution, which is a scaled version of the ``naive Bayes'' posterior, enabling the computation of conservative credibility sets.

I do believe at this point that Bayesian inference is the right way to look at naive Bayes. At the very least, the interpretation it provides is insightful.

However, the most general version of composite inference I presented goes well beyond the classical na\"ive Bayes scope, and is based on the concept of ``super composite likelihood'' function, where the weights assigned to the multiple atomic features are dependent on the variable of interest, {\em i.e.}, they are based on variable data representations. 

My hidden goal here was to justify image registration similarity measures in a neat statistical inference framework.

I didn't do a great job at this. The super composite likelihood could more simply be derived as a regular composite likelihood using variable features, without the need to introduce a more complex definition. The crux of the problem is to justify likelihood functions based on variable features, but I didn't do that.

\subsection{Variable feature likelihood}

The idea of variable feature (VF) likelihood was introduced by Baggenstoss in \cite{Baggenstoss-03} under the not very explicit name of ``class-specific method''. It is definitely a relevant idea, but this initial work was not fully successful for several reasons.

On the positive side, Baggenstoss gave a simple and general way, which he called ``PDF~projection theorem'', to construct a VF-based likelihood function. As later noted by \cite{Minka-04}, it is just an application of the maximum entropy or, more generally, the minimum KL~divergence principle.

However, there are three main flaws in Baggenstoss's presentation in my humble (or not so much) opinion:
\begin{enumerate}
\item {\em Bias}. It is easy to see how the PDF~projection method can lead to heavily biased likelihood functions if the variable features are chosen arbitrarily. The truth is that fixed features can be chosen arbitrarily -- at worst, they will lead to a flat likelihood, but can't bias inference. The same cannot be said for variable features. Baggenstoss may have seen that, but brushed the issue under the carpet: ``The choice of features remains an art requiring intuition''. Still, the assumptions under which a particular choice of features works or not should be made explicit.
\item {\em Arbitrary reference hypothesis}. The PDF~projection theorem involves a reference, or null hypothesis distribution that Baggenstoss gives no guideline whatsoever on how to choose. Since the likelihood function that arises from PDF~projection clearly depends on the reference distribution, this choice matters. He tried to work around this issue by making the reference distribution parametric, which led him to the third flaw:
\item {\em Inconsistent parameter elimination}. Baggenstoss treated the VF likelihood exactly as if it was a true likelihood function, and implicitely proposed to eliminate the nuisance parameters by maximization. The deep information-theoretic interpretation of PDF~projection shows that it is wrong. It is easy to see that Baggenstoss's approach doesn't work in the simplest instance of VF likelihood where the parameter space is a singleton -- the method should then be equivalent to the well-known maximum likelihood ratio method for binary hypothesis testing. This suggests that the nuisance parameters pertaining to the null hypothesis cannot be just mixed with those of the feature distribution; instead, they should be considered as competitive parameter sets.
\end{enumerate}


\subsection{Solving for the bias: the missing constraint}

The PDF~projection method simply needs an additional assumption to be unbiased. I have known the solution for quite some time but have struggled to fully contemplate it.

The least we can ask to the variable feature likelihood is to be weakly consistent, that is, to be maximal at the true parameter value in the limit of large samples. If this property does not hold, then the method is worthwhile nothing.

A sufficient condition to guarantee weak consistency is that the variable feature function $t_\theta$ statisfies:
$$
\forall (\theta, \theta'),
\quad
\int p(t_{\theta'}|\theta) \log \frac{p(t_{\theta'}|\theta)}{\pi(t_{\theta'})} dt_{\theta'}
  \leq
\int p(t_{\theta}|\theta) \log \frac{p(t_{\theta}|\theta)}{\pi(t_{\theta})} dt_{\theta}
.
$$

This condition may look complicated, but it only says that the variable feature should always be the most informative about the variable of interest, among all feature candidates. Specifically, $t_\theta$ should be, for any~$\theta$, the best statistic to test the hypothesis that~$\theta$ is the correct parameter vs the null hypothesis. In plain and simple words, the variable data representation should not discard any critical evidence.

Ideally, $t_\theta$ would be a sufficient statistic for each binary test $H_\theta$ vs $H_0$. Baggenstoss did note that, under sufficiency of $t_\theta$, the PDF~projection gives the true data distribution. But it is not a necessary condition for the method to work. And luckily so, otherwise the VF likelihood method would be pretty difficult to apply in practice.

Let's take the same example as in my initial paper on composite inference. Assume we want to infer a disease category using VF likelihood based on basic clinical measures, say, body temperature and blood pressure. The above condition tells us that we need to associate body temperature with infectious diseases, and blood pressure with the cardiovascular category. If we do it the other way round, the VF likelihood will at best be useless, at worst biased.

It feels totally obvious. Which doctor in his right mind would look at blood pressure rather than body temperature to diagnose an infectious disease? 


\subsection{Discarding evidence}

I had this constraint in mind ever since I started working on composite inference, but I didn't bother to incorporate it into the method. Why?

I believe the main reason is because I was (wrongly) looking for a justification of mutual information as a general similarity measure for image registration. I wanted consistency to follow from the formulation rather than imposing it by design.

I have known for a while that mutual information is not always consistent, contrary to a claim made by \cite{Zollei-09}. It's quite easy to find counter-examples suggesting that noisy images with nonlinearly related intensities may break consistency, meaning that mutual information may be maximal far away from the correct alignment transformation; not only conventional mutual information, but basically any similarity measure based on matched intensity pairs.

Also, I spent part of my PhD developing an alternative to mutual information for MR/US registration, arguing that conventional intensity-based similarity measures are not appropriate in this case due to the nature of US~images. This stresses the unsurprising fact that the choice of registration features is key. There is no such thing as a universal intensity-based similarity measure. They all have limited validity domains.

Why did I insist so much in the opposite direction? My thinking was that it was possible to derive image registration similarity measures from some extented likelihood principle. Consistency is a property of standard likelihood. So it was kind of natural to expect consistency to similarly follow from generalized likelihood.

What I overlooked is that the likelihood extension I was considering, basically VF likelihood, stems from a semi-generative statistical model. In other words, it's a partial model that doesn't fully describe images. Now, can consistency follow from partial modeling assumptions? It would be a nice property, but isn't it too much to ask?

It seems it is.

\subsection{Conservativeness of VF likelihood}

A key remark is that the expected logarithm ratio $\log p(t_\theta|\theta)/\pi(t_\theta)$ is lower than the true log-likelihood ratio $\log p(x|\theta)/\pi(x)$ by the general fact that the KL~divergence decreases under feature extraction. It is also a direct consequence from the fact that VF likelihood minimizes the KL~divergence among all distributions $p(x)$ compatible with $p(t_\theta|\theta)$. 

There is not much we can make from this result alone. But if, by any chance, the reference distribution is chosen so that there exists a $\theta_0$ for which $\pi(x)=p(x|\theta_0)$, then...

(the expectation being taken under the {\em true} unknown distribution)


From there, we can argue that VF likelihood is conservative if used in a Bayesian framework.

\subsection{Getting VF likelihood right}

At the end of the day, the VF likelihood method is just a restriction of Baggenstoss's 2003 proposal. We say: hey, this is a cool method but you can't choose variable features randomly. Yet there are two other aspects in which my approach significantly deviated from Baggenstoss's.

\paragraph{Applying MaxEnt in a broader context.} Instead of assuming the distributions $p(t_\theta|\theta)$ and $\pi(t_\theta)$, we may just assume some mean-value constraints and derive epistemic distributions via KL~minimization, which brings connections with minimally informative likelihood, minimum description length, game theory and that kind of cool publishable stuff.

My motivation for this was primarily to justify voxel independence assumptions. However, this may be one of the worst ideas I had. In this more general framework, the key weak consistency property is gone -- think about it, do we want a theory that justifies cross-correlation for multimodal images? Moreover, we don't even need that to justify voxel independence for feature registration models: de Finetti's representation theorem does the job, as I noticed a while back (in 2005 I think). No need to generalize Baggenstoss's approach here: one point for him.

\paragraph{The treatment of nuisance parameters.} Baggenstoss failed to see that the likelihood ratio involved in the PDF~projection fundamentally arises from a joint KL~minimization objective, hence it reflects a competition between two hypotheses. Parameter elimination should be consistent with that. This observation naturally leads to maximum likelihood ratio estimates rather than conventional likelihood parameter elimination. Nuisance parameters are not correctly handled in Baggenstoss's work. That point is for me. I think.



\bibliographystyle{abbrv}
\bibliography{cvis,stat,alexis}

\appendix

\section{Baggenstoss 2003 paper: reading notes}

He introduces the class-specific approach by a curse of dimensionality argument:
\begin{quote}
Because each feature set can be designed without regard to other classes (or states), it can be of much lower dimension than a common feature set that must account for all classes, effectively avoiding the curse of dimensionality.
\end{quote}

This is spot on: we want to select features to avoid the curse of dimensionality, however class-independent feature selection may not give us enough flexibility. He further comments on that:
\begin{quote}
[Feature selection] leads to a fundamental tradeoff: whether to discard features in an attempt to reduce the dimension to something manageable or to include them and suffer the problems associated with estimating a PDF at high dimension.
\end{quote}

He mentions that he did publish a previous work on the topic in 1999, which was based on sufficient statistics and used a fixed reference hypothesis.

His exposition of the PDF~projection theorem starts with the remark that, if $X$ is a random vector and $Z = f(X)$ is a one-to-one function of~$X$, then:
$$
p_x(\x)
=
|J_f(\x)| p_z(\z)
.
$$

In the more general case of a many-to-one function, there are multiple distributions~$p_x(\x)$ that are compatible with a given~$p_z(\z)$. To solve for this ambiguity, Baggenstoss postulates that a reference distribution is needed:
\begin{quote}
In order to apply the constraint, it is necessary to make use of a reference hypothesis $H_0$ for which we know the PDF of both $\x$ and $\z$.
\end{quote}

In his derivation of the PDF~projection theorem, he postulates the 



%%\input{draft.biblio}

\end{document}
